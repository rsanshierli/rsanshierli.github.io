<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>《Chinese Open Relation Extraction and Knowledge Base Establishment》阅读记录</title>
      <link href="/2020/03/04/article-5/"/>
      <url>/2020/03/04/article-5/</url>
      
        <content type="html"><![CDATA[<p>本文总结了中国语言学中的三种独特但普遍的现象，研究了无监督的基于语言学的中文开放关系提取（ORE），可以自动发现任意关系且无需任何人工标记的数据集。通过将实体关系映射到依存树并考虑独特的中文语言特性，提出一种基于依存语义范式（DSNF）的无监督中文ORE模型。该模型对实体和关系之间的相对位置没有任何限制，并且提取由动词或名词的介导关系并处理平行从句来实现结果。将此模型应用于网络文本，建立并发布了称为COER的大规模实体和关系知识库。</p><a id="more"></a><h1><a name="t1"></a><a name="t1"></a>2. Introduction</h1><p style="text-indent:33px;">中文关系抽取主要基于传统的监督式机器学习算法，与英文的差距主要有两个原因：<strong>一是</strong>缺乏使用神经网络或远程监督来训练模型的大规模标注中文知识语料库；<strong>二是</strong>在<strong>形态、句法和语法</strong>方面，中文比英文更复杂，这使得公开提取更加困难。本文提出一种新颖的无监督中文开放关系提取模型依存语义范式（DSNFs），该模型在充分考虑中文独特语言特性的基础上，可以实现自动从中文文本中提取实体和关系，而无需任何人工干预；并建立一个称为COER的大型实体和关系知识库，可以为深度学习模型提供丰富的训练集，并为远程监督方法提供高质量的数据库。</p><p style="text-indent:33px;"><strong>在相关实体对的上下文中存在规则的句法构造</strong>。例如，如果实体分别是句子的主语和宾语，则关系词将包含谓词动词。依存关系分析可以反映句子组成部分之间的语义修饰，因此<strong>依存关系分析和实体关系之间的映射可用于构造依存语义范式</strong>。</p><p style="text-align:center;"><img alt="" class="has" height="298" src="https://img-blog.csdnimg.cn/2019122618253229.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwOTMxMTgx,size_16,color_FFFFFF,t_70" width="850"></p><p style="text-indent:33px;">由上图可知，“德国”将“高克”修饰为其属性(att)从而获取三元组(德国，总统，高克)，由SBV、VOB中的指向关系获得三元组(高克，访问，中国)，并且根据并列依存COO(访问，发表)，获得三元组(高克，发表，演讲)。依存关系由LTP提供。</p><p style="text-indent:0;"><strong>主要贡献：</strong></p><ol><li>概括了三种独特但普遍存在的中国语言学现象。</li><li>提出一种基于依存语义范式（DSNFs）的无监督ORE模型，不限制实体和关系词的相对位置，通过提取动词或名词的介导关系并捕获共现关系三元组来获得结果&nbsp;。</li><li>建立大规模的中国实体和关系知识库。<a href="https://github.com/TJUNLP/COER" target="_blank" rel="noopener">语料获取</a></li></ol><h1><a name="t2"></a><a name="t2"></a>3. Unique Chinese Linguistic Phenomena</h1><p style="text-indent:33px;">语言学具有普遍性、稳定性和国籍性的独特特征，这将影响提取策略的制定。汉语语言学的复杂性使其难以操作，并且与英语的方法不兼容。汉英语言学的差异主要体现在<strong>形态</strong>和<strong>句法</strong>上。</p><p style="text-indent:0;">形态上的差异如下：</p><ol><li>每个英文单词用空格隔开，中文单词之间没有明显的分界线。</li><li>英文单词的时态有所不同，但中文单词却没有变化。相同的中文表达可能会充当不同的语义函数，可能会产生歧义，应根据具体情况对此进行判断。</li><li>英语句子中只有一个谓词头，从句之间有很多连词，中文句子中谓词头可能不是唯一的，多个从属子句可以无需任何连接词进行连接。</li></ol><p style="text-indent:0;">因此，许多通常在英语语料库中表现良好的使用形态特征的方法在中文中可能会取得较差的效果，考虑更深的语言特性并与关系提取相结合以获得适合汉语的模型是本文的核心研究主题之一。</p><p style="text-indent:33px;">定义三种主要的中文语言现象，与英语完全不同，如下：</p><p style="text-indent:33px;"><strong>1. Nominal Modification-Center(NMC) Phenomenon</strong></p><p style="text-indent:33px;">名词修饰中心由修饰语和主词组成，主词是一个常用名词，在句子中通常起主语或宾语的作用。主词通常是短语的主要组成部分，而不是直接作为主语或宾语的实体，当NMC包含一个命名实体作为修饰符时通常会干扰提取，例如“<em>奥巴马</em>总统”、“<em>华盛顿</em>警方”。而英语喜欢直接使用实体作为主词或借助介词，例如&nbsp;“president <em>Obama</em>” “the Police of <em>Washington</em>”。因此将主词定义为伪实体，以便在提取过程中进行相应的转换。例如“奥巴马总统访问中国”，很容易得到伪实体“总统”和实体“中国”之间的“访问”关系，然后可以转换并输出关系元组(奥巴马，访问，中国)。</p><p style="text-indent:33px;"><strong>2. Chinese Light Verb Construction(CLVC) Phenomenon</strong></p><p style="text-indent:33px;">CLVC是指<strong>轻动词</strong>必须与名词共现，动词的宾语以介词宾语的形式出现，介词的位置灵活多变。在语言学中，轻动词几乎没有语义内容，通常以名词构成谓词。在英语中位于轻动词和介词之间的名词短语通常被视为轻动词结构(LVC)。例如短语“is the president of...”和“establish diplomatic relations with...”，其中“is”和“establish”是轻动词。许多文章已经证明，对LVC的不合理处理可能会导致明显的无信息提取。如果仅提取轻动词作为关系词，那么得到的三元组可能是错误的，例如从句子“巴拿马在2017年与中国建立外交关系”中提取(巴拿马，建立，中国)。英语提取器REVERB通过语法约束解决了这个问题：轻言语短语必须是“verb-noun-preposition”的连续单词序列。</p><p style="text-indent:33px;">英语中LVC的单词序列与中文完全不同，因此REVERB中的句法约束不能直接传递给中文提取。例如在“Faust made a deal with the devil”中，“made a deal with”是一个LVC，谓词也是如此，提取结果是(Faust, made a deal with, the devil)。将句子翻译成中文如下图所示，“verb-noun-preposition”的序列将不再保留，因为介词的位置在CLVC中更加灵活。</p><p style="text-align:center;"><img alt="" class="has" height="403" src="https://img-blog.csdnimg.cn/2019122621391461.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwOTMxMTgx,size_16,color_FFFFFF,t_70" width="800"></p><p style="text-indent:33px;">论文采用依存结构来解决CLVC在关系提取过程中的问题，发现依存分析可以很好地表示CLVC的定律。无论介词在句子中的位置如何，介词和实体都会组合成状语短语并出现在依存句法分析中。介词取决于ADV的动词，并且有一个实体对象取决于POB的介词，同时名词以VOB作为直接宾语紧跟轻动词以补充动词的语义内容。 如果上述结构在依存树中匹配，那么可以准确地提取CLVC短语作为关系。为了消除不连贯或无意义的提取，将动词及其直接宾语的组合标识为关系表达式（介词除外）。</p><p style="text-indent:33px;">此外，及物性概念动词可以单独用作谓语，也可以与名词（直接宾语）结合使用，并具有介词短语作为修饰动词的副词修饰语。尽管概念动词在语义上是独立的，但是只有谓词和名词的组合才能表达完整的文本语义，将此现象定义为扩展CLVC。</p><p style="text-indent:0;">&nbsp;</p><p style="text-indent:33px;"><strong>3. Intransitive Verb（IV）Phenomenon</strong></p><p style="text-indent:33px;">IV表示不及物动词必须通过介词链接到其修饰词，介词可以在动词的左侧或右侧。根据动词是否可以直接与宾语搭配，将动词分为两类：及物动词和不及物动词，宾语不能直接跟随不及物动词。在英语中，必须在不及物动词和宾语之间添加介词，例如“born in”，“work at”和“apologize to”。REVERB通过语法约束来解决该问题：关系短语必须是句子中单词的连续序列，以动词开头，以介词结尾。例子，“Hudson born in Hampstead, which is a suburb of London”，可以提取关系短语“born in ”。翻译成中文：</p><p style="text-align:center;"><img alt="" class="has" height="390" src="https://img-blog.csdnimg.cn/20191227154242982.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwOTMxMTgx,size_16,color_FFFFFF,t_70" width="850"></p><p style="text-indent:33px;">与处理CLVC的原则一致，依存分析有助于探索IV，而无需担心介词位置的影响。当介词位于动词的左侧时，介词取决于ADV的动词，并且有一个实体作为宾语取决于POB的介词。 否则通过位于动词的右侧，介词取决于CMP的动词，而实体取决于POB的介词。如果上述结构在依存树中匹配，则可准确提取IV短语作为关系。IV不同于CLVC，它们具有不同的语言意义，尽管它们都需要介词来构造句子，并且在实际提取过程中可以类似地对待，但是具有显式语义的不及物动词与轻度动词是有区别的。</p><h1><a name="t3"></a><a name="t3"></a>4. DEPENDENCY SEMANTIC NORMAL FORMS&nbsp;</h1><p style="text-indent:33px;">依存树对于关系提取非常有用，比面向表面的表示更紧凑地捕获关系短语的句法和语义属性。由于实体对必须以依存短语的形式出现在依存树中，因此实体之间的依存路径将反映成对的对应关系。包含用于关系提取的表征语法类型很多，例如主语-谓语，谓语-宾语，介词-宾语，协同，修饰语和其他结构。通过将它们映射到依存 树中得到依存语义范式(DSNF)。DSNF是关系的句法和语义抽象，可以概括为单词、POS标签、依存路径和路径上依存标签的组合。定义了两种显示DSNF的方法：一是逻辑表达式，二是图形表达式。将句法结构分四类：<strong>修饰结构(MOD)、动词结构(VERB)、并列结构(COOR)和公式化结构(FORM)</strong>，可以在每种类型中获得一个或多个DSNF。</p><p style="text-align:center;"><img alt="" class="has" height="1047" src="https://img-blog.csdnimg.cn/20191227161253712.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwOTMxMTgx,size_16,color_FFFFFF,t_70" width="776"></p><h2><a name="t4"></a><a name="t4"></a><strong>4.1 Modified Construction</strong></h2><p style="text-indent:33px;"><strong>修饰结构(MOD)：</strong>关系词是实体对上下文中的修饰语成分，其中主词是实体，修饰语称为修饰定语。中文中修饰语结构属于定语结构。修饰语的成分类型非常广泛，包括<strong>专有名词(实体)</strong>、<strong>普通名词</strong>、<strong>含“的”短语</strong>以及<strong>数字</strong>等。定语和主语之间的标签通常是<strong>ATT</strong>或<strong>RAD</strong>。例子如(德国，总统，高克)。另外修饰语可能是多层次的，<strong>一个主词可以具有多个定语</strong>。在实际提取中一个关系词主要考虑存在两个或三个定语并且几个修饰语组合的情况。根据MOD定义的DSNF1如图5所示。</p><h2><a name="t5"></a><a name="t5"></a><strong>4.2&nbsp;Verbal Construction</strong></h2><p style="text-indent:33px;"><strong>动词结构(VERB)：</strong>动词充当谓语短语并且可能是关系词。对于一个实体对，一是谓词主语另一在宾语的位置，它可以直接与动词(Verb-Object)关联，也可以与介词(Preposition-Object)间接关联。根据动词的多样化，关系可以分为以下几类：</p><p style="text-indent:33px;"><strong>Transitive Verbal Relations(及物动词关系)&nbsp;&nbsp;</strong>两个实体(作为句子的主语或宾语)都依赖于SBV或VOB标签的开头谓词，例如(高克，访问，中国)。有时及物动词后面没有动词宾语，则介词宾语短语会修饰动词，并且该短语通常在谓词的前面。</p><p style="text-indent:33px;"><strong>CLVC or Extended CLVC Relations(轻动词关系)&nbsp;&nbsp;</strong>例如在句子“主席对埃及进行国事访问”中“对...进行国事访问”是汉语轻动词的结构。主语“主席”直接取决于SBV的轻动词“进行”；“埃及”间接依赖于POB的轻动词作为介词宾语，则可提取三元组(主席，进行国事访问，埃及)。可以使用DSNF3处理CLVC，在特殊情况下，如果介词呈现被动语态(由或者被)，则应交换实体的位置以确保三元组的逻辑语义。</p><p style="text-indent:33px;"><strong>IVC Relations&nbsp;&nbsp;</strong>当介词位于不及物动词的左侧时，利用DSNF3提取三元组，但<strong>仅将谓语</strong>用作关系词。当介词位于动词右边时，定义DSNF4以获得关系。</p><h2><a name="t6"></a><a name="t6"></a><strong>4.3&nbsp;Coordination Construction</strong></h2><p style="text-indent:33px;"><strong>并列结构(COOR)：</strong>关系宾语或并列动作可能构成COOR，在提取过程中主要考虑因素是那些并列实体或动词。对于并列实体E1和E2，所有涉及E1的三元组都适用于E2。即E2可以代替三元组中的E1以获取新的三元组。<strong>逗号或连词</strong>通常用于连接并列实体。描述几种不同动作的并列动词是由同一实体执行的，这些动词通常分布在句子的并列从句中。例如例子1中的“访问”和“发表”。另外COOR必须包含在其他语法类别中，比如通过动词结构提取三元组。DSNF5、DSNF6和DSNF7仅处理COOR依赖于<strong>及物动词结构</strong>的情况。此外并列名词或并列动词可能在同一句子中同时出现并且不会相互排斥。</p><h2><a name="t7"></a><a name="t7"></a><strong>4.4 Formulaic Construction 公式化结构(FORM)</strong></h2><h2><a name="t8"></a><a name="t8"></a><strong>4.5&nbsp;Constraints for DSNFs</strong></h2><p style="text-indent:33px;">为了尽可能避免因依存关系分析错误而导致的级联效应，对DSNF约束如下：</p><ol><li>通常为了保持语义完整性，提取过程以句子完成。文本使用标点符号细分为句子，包括句号、感叹号、分号、分号和问号。当句子的长度大于阈值M时，该句子将进一步用逗号分解，并对子句进行提取。</li><li>保持句子的单词顺序，实体对之间的其他实体数不能超过X1，实体对之间的其他任何单词数都必须小于X2。</li><li>为了避免在DSNFs匹配期间主语错误地依赖谓词，如果E1依赖SBV的谓词，则E1和谓词（代词除外）之间不应有其他词同时依赖SBV的谓词 。</li><li>对于DSNF1，句子中E1和E2之间的距离不应超过4。</li><li>对于DSNF3，当单词是取决于VOB谓词的直接宾语时，该单词必须是名词。</li><li>对于DSNF3，当谓词前面有多个副词短语时，DSNF3只能匹配最接近谓词的短语。状语短语由介词和实体组成，它出现在依存关系分析中。介词取决于ADV的动词，而实体宾语取决于POB的介词。</li><li>通过特定于语言的调整，名词可以分为两种类型：一是与职业有关的名词，例如“主席、教授、警察”；二是朴素名词。由于与职业有关的名词是一个封闭的集合，可通过从词典中进行选择来检测名词的类型。如果实体是NMC结构的一部分，并且单词的名称是朴素名词，则通过连接朴素名词来扩展实体，例如“纽约警察”。</li></ol><h1><a name="t9"></a><a name="t9"></a>5.&nbsp;COER: CHINESE OPEN ENTITY AND RELATION KNOWLEDGE BASE</h1><p style="text-indent:33px;">COER是一个可扩展的实体和关系语料库，包含大约一百万个关系三元组，其中关系是开放和任意的。当前大约有556,012个命名实体和282,347个开放关系短语，包含军事，体育，娱乐，经济学和其他领域，可确保此数据库的开放性。提取的三元组存储在一系列XML文件中，关系项目由原始文本、实体对、关系短语和最短依赖路径组成。每个Entity_pair单元均包含两个参数条目，每个Relation_phrase单元均包含多个提及条目。同时条目包含丰富的属性，内容组织结构用树表示：</p><p style="text-align:center;"><img alt="" class="has" height="523" src="https://img-blog.csdnimg.cn/20191227195519940.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwOTMxMTgx,size_16,color_FFFFFF,t_70" width="700"></p><h1><a name="t10"></a><a name="t10"></a>6. EXPERIMENTS</h1><h2><a name="t11"></a><a name="t11"></a>6.1 Experimental Setup</h2><ol><li><strong>预处理输入文本&nbsp;&nbsp;</strong>首先使用阈值M = 40将文本拆分为句子，借助LTP的NLP工具对每个目标句子进行预处理，包括分词、POS标记和依赖项解析。</li><li><strong>选择候选实体&nbsp;&nbsp;</strong>根据LTP和迭代启发式算法通过命名实体识别方法检测所有候选实体，后者将名词与相邻名词合并来组合最大名词短语。这些名词只能具有{ni，nh，ns，nz，j}列表中的相同POS，其分别表示组织名称、人员名称、地理名称、其他专有名词和缩写含义。启发式是对前者的补充。</li><li><strong>匹配DSNF&nbsp;</strong> 分别阈值X1和X2设置为3和12。对于同一句子中的每对候选实体，可以自动确定最短路径中共享的句法结构是否正确匹配到某个DSNF。</li><li><strong>根据匹配结果输出三元组</strong></li></ol><p style="text-indent:33px;">结果验证：</p><p style="text-align:center;"><img alt="" class="has" height="188" src="https://img-blog.csdnimg.cn/20191227202451331.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwOTMxMTgx,size_16,color_FFFFFF,t_70" width="800"></p><h2><a name="t12"></a><a name="t12"></a>6.2 Experimental Results</h2><p style="text-align:center;"><img alt="" class="has" height="145" src="https://img-blog.csdnimg.cn/20191227202758135.png" width="653"></p><p style="text-align:center;"><img alt="" class="has" height="335" src="https://img-blog.csdnimg.cn/20191227203249763.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwOTMxMTgx,size_16,color_FFFFFF,t_70" width="500"></p><h2><a name="t13"></a><a name="t13"></a>6.3 Error Analysis</h2><p style="text-indent:33px;">中文有许多解释性和复杂的关系表达式，例如“宣布终止经济和财务制裁”。模型暂不能处理<strong>代词</strong>的干扰。</p><p style="text-align:center;"><img alt="" class="has" height="159" src="https://img-blog.csdnimg.cn/20191227203630383.png" width="500"></p><p style="text-align:center;"><img alt="" class="has" height="194" src="https://img-blog.csdnimg.cn/20191227203654482.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwOTMxMTgx,size_16,color_FFFFFF,t_70" width="500"></p>]]></content>
      
      
      <categories>
          
          <category> 笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文记录 </tag>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《Extracting Relational Facts by an End-to-End Neural Model with Copy Mechanism》记录</title>
      <link href="/2020/03/04/article-4/"/>
      <url>/2020/03/04/article-4/</url>
      
        <content type="html"><![CDATA[<p>句子中的关系事实通常很复杂，不同的关系三元组在句子中存有实体重叠。根据三元组重叠度将句子分为三种类型，包括Normal，EntityPairOverlap 和 SingleEntiyOverlap。 现有方法主要集中在Normal类上，无法准确地提取关系三元组。 本文提出了一种基于具有复制机制的序列到序列学习的端到端模型，该模型可以从任何这些类的句子中联合提取相关事实。 在解码过程中，采用两种不同的策略：仅使用一个联合解码器或应用多个分离解码器。</p><a id="more"></a><h1 style="text-indent:0px;"><a name="t1"></a><a name="t1"></a>Introduction</h1><p style="text-indent:33px;">句子中的关系事实常常很复杂，不同的关系三元组在句子中可能有重叠。如果一个句子的三元组都没有重叠的实体，则该句子属于Normal类；如果一个句子的某些三元组中的某些实体对重叠，则该句子属于EntityPairOverlap（EPO）类；如果一个句子的某些三元组中有一个重叠的实体，而这些三元组中没有重叠的实体对，则该句子属于SingleEntityOverlap（SEO）类。</p><p style="text-align:center;"><img alt="" class="has" height="462" src="https://img-blog.csdnimg.cn/20191223154146830.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwOTMxMTgx,size_16,color_FFFFFF,t_70" width="550"></p><p style="text-indent:33px;">为了处理三元组重叠的问题，必须允许一个实体自由地参与多个三元组，对此论文提出一种基于<strong>复制机制</strong>的序列到序列学习的端到端模型，该模型可以从包含这些类别的句子中联合提取相关事实。此模型的主要组件包括两部分：<strong>编码器和解码器</strong>。编码器将自然语言句子（源句子）转换为固定长度的语义向量，然后解码器读入此向量并直接生成三元组。为了生成一个三元组，首先解码器生成关系；其次通过采用复制机制，解码器从源句子复制第一个实体（头实体）；最后解码器从源句子中复制第二个实体（尾实体），以此来提取多个三元组。解码过程中采用两种不同的策略：仅使用一个统一解码器（OneDecoder）生成所有三元组，应用多个分离解码器（MultiDecoder）其每个解码器生成一个三元组。</p><h1 style="text-indent:0px;"><a name="t2"></a><a name="t2"></a>Model</h1><h2><a name="t3"></a><a name="t3"></a>OneDecoder Model</h2><p style="text-align:center;"><img alt="" class="has" height="846" src="https://img-blog.csdnimg.cn/20191223162423231.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwOTMxMTgx,size_16,color_FFFFFF,t_70" width="1100"></p><blockquote><p style="text-indent:0;">Figure 2: The overall structure of OneDecoder model. A bi-directional RNN is used to encode the source sentence and then a decoder is used to generate triples directly. The relation is predicted and the entity is copied from source sentence.</p></blockquote><p style="text-indent:0;"><strong>Encoder&nbsp; &nbsp;&nbsp;</strong>首先将源句子转换为矩阵，矩阵向量表示各个词嵌入，依序将矩阵传入Bi-RNN生成时间步 t 时刻的结果和隐藏状态。输出结果为&nbsp;<img alt="O^{E}= \left [ o_{1}^{E},\cdots ,o_{n}^{E} \right ],o_{t}^{E}=\left [ \overrightarrow{o_{t}^{E}},\overleftarrow{o_{n-t+1}^{E}} \right ]" class="mathcode" src="https://private.codecogs.com/gif.latex?O%5E%7BE%7D%3D%20%5Cleft%20%5B%20o_%7B1%7D%5E%7BE%7D%2C%5Ccdots%20%2Co_%7Bn%7D%5E%7BE%7D%20%5Cright%20%5D%2Co_%7Bt%7D%5E%7BE%7D%3D%5Cleft%20%5B%20%5Coverrightarrow%7Bo_%7Bt%7D%5E%7BE%7D%7D%2C%5Coverleftarrow%7Bo_%7Bn-t&amp;plus;1%7D%5E%7BE%7D%7D%20%5Cright%20%5D">，隐藏状态的表示类似。</p><p style="text-indent:0;"><strong>Decoder&nbsp;</strong>&nbsp; &nbsp;首先解码器生成三元组的关系，其次解码器从源句子中复制一个实体作为三元组的第一个实体，最后解码器从源句子中复制第二个实体。重复此过程，解码器可以生成多个三元组。当生成所有有效的三元组，解码器将生成NA三元组，即意味着“停止”。NA三元组由NA关系和NA实体对组成。</p><p style="text-align:center;"><img alt="" class="has" height="373" src="https://img-blog.csdnimg.cn/20191223165415874.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwOTMxMTgx,size_16,color_FFFFFF,t_70" width="901"></p><blockquote><p style="text-indent:0;">Figure 3: The inputs and outputs of the decoder(s) of OneDecoder model and MultiDecoder model. (a) is the decoder of OneDecoder model. As we can see, only one decoder (the green rectangle with shadows) is used and this encoder is initialized with the sentence representation s. (b) is the decoders of MultiDecoder model. There are two decoders (the green rectangle and blue rectangle with shadows). The ﬁrst decoder is initialized with s;Other decoder(s) are initialized with s and previous decoder’s state.</p></blockquote><p style="text-indent:0;">解码计算：<img alt="o_{t}^{D},h_{t}^{D} = g\left ( u_{t}, h_{t-1}^{D} \right )" class="mathcode" src="https://private.codecogs.com/gif.latex?o_%7Bt%7D%5E%7BD%7D%2Ch_%7Bt%7D%5E%7BD%7D%20%3D%20g%5Cleft%20%28%20u_%7Bt%7D%2C%20h_%7Bt-1%7D%5E%7BD%7D%20%5Cright%20%29">，其中&nbsp;<img alt="h_{0}^{D}" class="mathcode" src="https://private.codecogs.com/gif.latex?h_%7B0%7D%5E%7BD%7D">&nbsp;为源句子的初始化表示，<img alt="u_{t}=\left [ v_{t};c_{t} \right ]\cdot W^{u}" class="mathcode" src="https://private.codecogs.com/gif.latex?u_%7Bt%7D%3D%5Cleft%20%5B%20v_%7Bt%7D%3Bc_%7Bt%7D%20%5Cright%20%5D%5Ccdot%20W%5E%7Bu%7D">，<img alt="c_{t}" class="mathcode" src="https://private.codecogs.com/gif.latex?c_%7Bt%7D">&nbsp;是注意力向量，<img alt="v_{t}" class="mathcode" src="https://private.codecogs.com/gif.latex?v_%7Bt%7D">&nbsp;是复制实体或者前一时刻预测关系的嵌入。</p><p style="text-align:center;"><img alt="" class="has" height="259" src="https://img-blog.csdnimg.cn/20191223170330403.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwOTMxMTgx,size_16,color_FFFFFF,t_70" width="400"></p><p style="text-indent:33px;">在获得时间步 t 时得解码结果&nbsp;<img alt="o_{t}^{D}" class="mathcode" src="https://private.codecogs.com/gif.latex?o_%7Bt%7D%5E%7BD%7D">&nbsp;后，如果 t 能除以3余1则进行一次关系预测，如果余2则从源句子中复制第一个实体，整除则复制第二个实体。</p><p style="text-indent:0;"><strong>Predict Relation</strong>&nbsp; &nbsp; 假设有m个有效关系，使用一个全连接层来计算所有有效关系的置信向量，<img alt="q^{r}=selu\left ( o_{t}^{D}\cdot W^{t}+b^{r} \right )" class="mathcode" src="https://private.codecogs.com/gif.latex?q%5E%7Br%7D%3Dselu%5Cleft%20%28%20o_%7Bt%7D%5E%7BD%7D%5Ccdot%20W%5E%7Bt%7D&amp;plus;b%5E%7Br%7D%20%5Cright%20%29">。在生成NA三元组时，NA关系的置信值计算为：<img alt="q^{NA}=selu\left ( o_{t}^{D}\cdot W^{NA}+b^{NA} \right )" class="mathcode" src="https://private.codecogs.com/gif.latex?q%5E%7BNA%7D%3Dselu%5Cleft%20%28%20o_%7Bt%7D%5E%7BD%7D%5Ccdot%20W%5E%7BNA%7D&amp;plus;b%5E%7BNA%7D%20%5Cright%20%29">。然后获得概率分布表示：<img alt="p^{r}=softmax\left ( \left [ q^{r};q^{NA} \right ] \right )" class="mathcode" src="https://private.codecogs.com/gif.latex?p%5E%7Br%7D%3Dsoftmax%5Cleft%20%28%20%5Cleft%20%5B%20q%5E%7Br%7D%3Bq%5E%7BNA%7D%20%5Cright%20%5D%20%5Cright%20%29">，选择最高概率的关系并将其嵌入作为下一步的输入。</p><p style="text-indent:0;"><strong>Copy the First Entity</strong>&nbsp; &nbsp; 实体的选择跟关系预测的计算相似，选择概率分布中的最高概率作为预测实体，同样使用其嵌入作为下一步的输入。概率分布表示为：<img alt="p^{e}=softmax\left ( \left [ q^{e};q^{NA} \right ] \right )" class="mathcode" src="https://private.codecogs.com/gif.latex?p%5E%7Be%7D%3Dsoftmax%5Cleft%20%28%20%5Cleft%20%5B%20q%5E%7Be%7D%3Bq%5E%7BNA%7D%20%5Cright%20%5D%20%5Cright%20%29">；置信向量为：<img alt="q_{i}^{e}=selu\left ( \left [ o_{t}^{D};o_{i}^{E} \right ] \cdot W^{e} \right )" class="mathcode" src="https://private.codecogs.com/gif.latex?q_%7Bi%7D%5E%7Be%7D%3Dselu%5Cleft%20%28%20%5Cleft%20%5B%20o_%7Bt%7D%5E%7BD%7D%3Bo_%7Bi%7D%5E%7BE%7D%20%5Cright%20%5D%20%5Ccdot%20W%5E%7Be%7D%20%5Cright%20%29">。</p><p style="text-indent:0;"><strong>Copy the Second Entity</strong>&nbsp; &nbsp; 第二个实体的复制必须避开已复制的第一个实体，对此，假设第一个实体为k-th，引入一个长度为源句子长度n的掩码向量M，<img alt="M_{i}=1,i\neq k,M_{i}=0,i=k" class="mathcode" src="https://private.codecogs.com/gif.latex?M_%7Bi%7D%3D1%2Ci%5Cneq%20k%2CM_%7Bi%7D%3D0%2Ci%3Dk">。然后计算概率分布，<img alt="p^{e}=softmax\left ( \left [ M\otimes q^{e};q^{NA} \right ] \right )" class="mathcode" src="https://private.codecogs.com/gif.latex?p%5E%7Be%7D%3Dsoftmax%5Cleft%20%28%20%5Cleft%20%5B%20M%5Cotimes%20q%5E%7Be%7D%3Bq%5E%7BNA%7D%20%5Cright%20%5D%20%5Cright%20%29">，⊗是逐个元素相乘。</p><h2 style="text-indent:0px;"><a name="t4"></a><a name="t4"></a>MultiDecoder Model</h2><p style="text-indent:33px;">MultiDecoder模型是所提出的OneDecoder模型的扩展。主要区别在于解码三元组时，MultiDecoder模型使用几个分离的解码器进行解码。图3（b）显示了MultiDecoder模型的解码器的输入和输出，有两个解码器（带阴影的绿色和蓝色矩形），解码器按顺序工作：第一个解码器生成第一个三元组，然后第二个解码器生成第二个三元组。</p><p style="text-align:center;"><img alt="" class="has" height="107" src="https://img-blog.csdnimg.cn/20191224103136129.png" width="500"></p><p style="text-indent:0;"><img alt="D_{i}" class="mathcode" src="https://private.codecogs.com/gif.latex?D_%7Bi%7D">&nbsp;表示第 i 个解码器，ut 计算跟之前一样，初始化隐藏状态计算如下：</p><p style="text-align:center;"><img alt="" class="has" height="110" src="https://img-blog.csdnimg.cn/20191224103253460.png" width="400"></p><h2 style="text-indent:0px;"><a name="t5"></a><a name="t5"></a>&nbsp;</h2><p>&nbsp;</p>]]></content>
      
      
      <categories>
          
          <category> 笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文记录 </tag>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《Effective Modeling of Encoder-Decoder Architecturefor Joint Entity and Relation Extraction》</title>
      <link href="/2020/03/04/article-3/"/>
      <url>/2020/03/04/article-3/</url>
      
        <content type="html"><![CDATA[<p>关系元组由两个实体以及它们之间的关系组成，并且经常在非结构化文本中找到这样的元组。文本中可能存在多个关系元组，并且它们之间可能共享一个或两个实体。从句子中提取这样的关系元组是一项艰巨的任务，并且在元组之间共享实体或重叠实体会使其更具挑战性。本文中提出了两种使用编码器-解码器体系结构共同提取实体和关系的方法。提出了一种用于关系元组的表示方案，该方案使解码器能够像机器翻译模型一样一次生成一个单词，并且仍然可以找到句子中存在的所有元组，它们具有不同长度的完整实体名称并且具有重叠的实体。提出一种基于指针网络的解码方法，其中在每个时间步生成一个完整的元组。</p><a id="more"></a><h1 id="Introduction" style="text-indent:0px;"><a name="t1"></a><a name="t1"></a>Introduction</h1><p style="text-indent:33px;">传统使用流水线方法，使用命名实体识别来识别句子中的实体，然后使用分类器查找它们之间的关系（或没有关系）。但是由于实体检测和关系分类的完全分离，这些模型错过了句子中存在的多个关系元组之间的交互作用。</p><p style="text-indent:33px;">本文三个主要挑战：（i）该模型能够将实体和关系提取在一起。 （ii）能够提取具有重叠实体的多个元组。 （iii）能够准确地提取一个具有全名的元组实体。为了解决这些挑战，提出了两种使用编码器-解码器体系结构的新颖方法。首先提出一种用于关系元组的新表示方案（表1），以便它可以用简单的方式表示具有重叠实体和不同长度实体的多个元组。采用编码器-解码器模型，其中解码器像机器翻译模型一样一次提取一个单词。在序列生成的最后，由于元组的独特表示，可以从单词序列中提取元组。尽管此模型执行得很好，但是一次生成一个单词对于此任务来说有点不自然。每个元组恰好具有两个实体和一个关系，并且每个实体在句子中显示为连续的文本范围。识别它们的最有效方法是在句子中找到它们的开始和结束位置。然后，<strong>使用五个元素来表示每个关系元组：两个实体的开始和结束位置以及它们之间的关系</strong>（请参见表1）。考虑到这一点，<strong>提出了一个基于指针网络的解码框架</strong>。该解码器由两个指针网络和一个识别它们之间关系的分类网络组成，该指针网络可以找到句子中两个实体的开始和结束位置。在解码的每个时间步，此解码器都提取整个关系元组，而不仅仅是单词。</p><p style="text-indent:33px;">本文的贡献如下：<strong>（1）提出了一种新的关系元组表示方法，使得一个编码器-解码器模型在每一步提取一个单词时，仍然可以从句子中找到多个实体重叠的元组和多个标记实体的元组，还提出了一种基于掩蔽的复制机制，只从源语句中提取实体。（2）提出在解码框架中使用指针网络进行修改，以使编码器-解码器模型更适合此任务。在每个时间步，此解码器都会提取整个关系元组，而不仅仅是单词。这个新的解码框架有助于加快训练过程，并使用更少的资源（GPU内存）。当从句子级元组提取转向文档级提取时，这将是一个重要因素。（3）对NYT数据集进行的实验表明，该方法明显优于所有以前的最新模型。</strong></p><h1 id="Task%20Description" style="text-indent:0px;"><a name="t2"></a><a name="t2"></a>Task Description</h1><p style="text-indent:33px;">关系元组由两个实体和一个关系组成。 这样的元组可以在句子中找到，其中实体是句子中的文本范围，并且关系来自预定义的集合R。这些元组可以在其中共享一个或两个实体。</p><ol><li style="text-indent:0px;">无实体重叠（NEO）：此类中的一个句子包含一个或多个元组，但它们不共享任何实体。</li><li style="text-indent:0px;">实体对重叠（EPO）：此类中的一个句子有多个元组，并且至少两个元组以相同或相反的顺序共享两个实体。</li><li style="text-indent:0px;">单实体重叠（SEO）：此类中的一个句子包含一个以上的元组，并且至少两个元组正好共享一个实体。</li></ol><p style="text-indent:0px;">一个句子可以同时属于EPO和SEO类，任务是提取句子中存在的所有关系元组。</p><h1 style="text-indent:0px;"><a name="t3"></a><a name="t3"></a>Encoder-Decoder Architecture</h1><p style="text-indent:33px;">此任务中输入是单词序列，输出是一组关系元组。第一种方法中，表示每个元组的ententity1；实体2;关系，使用“；”作为分隔符来分隔元组各部分，多行元组使用“ |”分隔。使用这些特殊标记，可以用一种简单的方式表示具有重叠实体和不同长度实体的多个关系元组。在推理过程中，序列生成结束后，可以使用这些特殊标记轻松提取关系元组。由于采用了这种统一的表示方案，对实体，关系和特殊标记的处理类似，因此编码器和解码器之间使用了包含所有这些标记的共享词汇。输入句子包含<strong>每个关系的线索词</strong>，可以帮助生成<strong>关系标记</strong>。其次使用两个特殊标记，以便模型可以区分关系元组的开头和元组组件的开头。为了使用编码器-解码器模型从句子中提取关系元组，该模型必须生成<strong>实体标记</strong>，找到关系线索词并将其映射到关系标记，并<strong>在适当的时间生成特殊标记</strong>。</p><p style="text-align:center;"><img alt="" class="has" height="213" src="https://img-blog.csdnimg.cn/20191216195622187.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwOTMxMTgx,size_16,color_FFFFFF,t_70" width="1200"></p><h1 style="text-indent:0px;"><a name="t4"></a><a name="t4"></a>Embedding Layer &amp; Encoder</h1><p style="text-indent:33px;">创建一个单一词汇表V，该词表由源句标记、关系集R中的关系名称、特殊分隔符、目标序列开始标记（SOS）、目标序列结束标记（EOS）以及未知标记（UNK）。词级嵌入由两个部分组成：预训练词向量和基于字符嵌入的特征向量。使用单词嵌入层<img alt="E_{\omega }\in \mathbb{R}^{\left | V \right |\times d_{\omega }}" class="mathcode" src="https://private.codecogs.com/gif.latex?E_%7B%5Comega%20%7D%5Cin%20%5Cmathbb%7BR%7D%5E%7B%5Cleft%20%7C%20V%20%5Cright%20%7C%5Ctimes%20d_%7B%5Comega%20%7D%7D">和字符嵌入层<img alt="E_{c}\in \mathbb{R}^{\left | V \right |\times d_{c}}" class="mathcode" src="https://private.codecogs.com/gif.latex?E_%7Bc%7D%5Cin%20%5Cmathbb%7BR%7D%5E%7B%5Cleft%20%7C%20V%20%5Cright%20%7C%5Ctimes%20d_%7Bc%7D%7D">，其中dw是单词向量的维数，A是输入句子标记的字符字母，而dc是字符嵌入向量的维数。使用具有最大池化的卷积神经网络为每个单词提取维度为df的特征向量。串联词嵌入和基于字符嵌入的特征向量以获得输入标记的表示形式。源语句S由其标记x1; x2; ::::; xn的向量表示，其中<img alt="x_{i}\in \mathbb{R}^{d_{w}+d_{f}}" class="mathcode" src="https://private.codecogs.com/gif.latex?x_%7Bi%7D%5Cin%20%5Cmathbb%7BR%7D%5E%7Bd_%7Bw%7D&amp;plus;d_%7Bf%7D%7D">是第i个单词的向量表示形式，n是S的长度。这些向量xi被传递双向LSTM（Bi-LSTM）以获得隐藏的表示<img alt="h_{i}^{E}" class="mathcode" src="https://private.codecogs.com/gif.latex?h_%7Bi%7D%5E%7BE%7D">。将Bi-LSTM的向前和向后LSTM的隐藏维设置为dh/2，以获得<img alt="h_{i}^{E}\in \mathbb{R}^{d_{h}}" class="mathcode" src="https://private.codecogs.com/gif.latex?h_%7Bi%7D%5E%7BE%7D%5Cin%20%5Cmathbb%7BR%7D%5E%7Bd_%7Bh%7D%7D">，其中dh是后面描述的解码器的序列生成器LSTM的隐藏维度。</p><h1 style="text-indent:0px;"><a name="t5"></a><a name="t5"></a>Word-level Decoder &amp; Copy Mechanism</h1><p style="text-indent:33px;">目标序列T仅由标记y0; y1; ::::; ym的词嵌入向量表示，其中<img alt="y_{i}\in \mathbb{R}^{d_{w}}" class="mathcode" src="https://private.codecogs.com/gif.latex?y_%7Bi%7D%5Cin%20%5Cmathbb%7BR%7D%5E%7Bd_%7Bw%7D%7D">是第i个标记的嵌入向量，m是目标序列的长度。 y0和ym分别代表SOS和EOS标记的嵌入向量。解码器一次生成一个标记，并在生成EOS时停止。使用LSTM作为解码器，在时间步t，解码器将源语句编码（<img alt="e_{t}\in \mathbb{R}^{d_{h}}" class="mathcode" src="https://private.codecogs.com/gif.latex?e_%7Bt%7D%5Cin%20%5Cmathbb%7BR%7D%5E%7Bd_%7Bh%7D%7D">）和先前的目标词嵌入（<img alt="y_{t-1}" class="mathcode" src="https://private.codecogs.com/gif.latex?y_%7Bt-1%7D">）作为输入，并生成当前标记的隐藏表示（<img alt="h_{t}^{D}\in \mathbb{R}^{d_{h}}" class="mathcode" src="https://private.codecogs.com/gif.latex?h_%7Bt%7D%5E%7BD%7D%5Cin%20%5Cmathbb%7BR%7D%5E%7Bd_%7Bh%7D%7D">）。使用注意力机制获得句子编码向量<img alt="e_{t}" class="mathcode" src="https://private.codecogs.com/gif.latex?e_%7Bt%7D">，使用具有权重矩阵<img alt="W_{v}" class="mathcode" src="https://private.codecogs.com/gif.latex?W_%7Bv%7D">和偏差矢量<img alt="b_{v}" class="mathcode" src="https://private.codecogs.com/gif.latex?b_%7Bv%7D">的线性层将<img alt="h_{t}^{D}" class="mathcode" src="https://private.codecogs.com/gif.latex?h_%7Bt%7D%5E%7BD%7D">投影到词汇表V。</p><p style="text-align:center;"><img alt="" class="has" height="96" src="https://img-blog.csdnimg.cn/2019121620442334.png" width="450"></p><p style="text-indent:0;"><img alt="\hat{o}_{t}" class="mathcode" src="https://private.codecogs.com/gif.latex?%5Chat%7Bo%7D_%7Bt%7D">&nbsp;表示嵌入词汇表中所有单词在时间步t上的标准化分数，<img alt="h_{t-1}^{D}" class="mathcode" src="https://private.codecogs.com/gif.latex?h_%7Bt-1%7D%5E%7BD%7D">&nbsp;是LSTM的前隐藏状态。</p><p style="text-indent:33px;">解码器的投影层将解码器输出映射到整个词汇表。然而在推断期间，解码器可以从词汇表中预测在当前句子或关系集或特殊标记中不存在的标记。为了防止这种情况，在投影层上应用softmax时进行mask，屏蔽词汇表的所有单词，但不包括当前源句标记、关系标记、分隔符UNK和EOS标记。要从softmax中屏蔽（排除）某些单词，将<img alt="\hat{o}_{t}" class="mathcode" src="https://private.codecogs.com/gif.latex?%5Chat%7Bo%7D_%7Bt%7D">设置相应的值（负无穷），并且相应的softmax分数将为零，确保仅从源句子复制实体。在softmax操作中包含UNK标记，以确保模型在推理期间生成新实体。如果解码器预测到UNK标记，将其替换为具有最高关注分数的相应源单词。在推论过程中，解码完成后将基于特殊标记提取所有元组，从关系集中删除重复的元组和两个实体相同的元组或不包含关系标记的元组。此模型后称为WordDecoding（WDec）。</p><h1 style="text-indent:0px;"><a name="t6"></a><a name="t6"></a>Pointer Network-Based Decoder</h1><p style="text-indent:33px;">第二种方法使用开始和结束位置来识别句子中的实体。从单词词汇表中删除特殊标记和关系名称，并且单词嵌入仅在编码器侧与字符嵌入一起使用。在模型的解码器端使用附加的关系嵌入矩阵<img alt="E_{r}\in \mathbb{R}^{\left | R \right |\times d_{r}}" class="mathcode" src="https://private.codecogs.com/gif.latex?E_%7Br%7D%5Cin%20%5Cmathbb%7BR%7D%5E%7B%5Cleft%20%7C%20R%20%5Cright%20%7C%5Ctimes%20d_%7Br%7D%7D">，其中R是关系集，dr是关系向量的维数。关系集R包括指示序列结束的。关系元组表示为序列T = y0; y1; ::: ;; ym，其中yt是一个元组，由源语句中的四个索引组成，指示两个实体的开始和结束位置以及它们之间的关系（请参见表1）。 y0是一个虚拟元组，表示序列的起始元组，而ym充当序列的结束元组，该序列具有EOS作为关系（该元组将忽略实体）。解码器由一个LSTM（具有隐藏维dh来生成元组序列）、两个指针网络（用于查找两个实体）以及分类网络（用于查找元组的关系）组成。在时间步骤t，解码器将源语句编码和所有先前生成的元组的表示&nbsp;<img alt="y_{prev}" class="mathcode" src="https://private.codecogs.com/gif.latex?y_%7Bprev%7D">&nbsp;作为输入，并生成当前元组的隐藏表示。句子编码向量集使用注意力机制获得。关系元组是一个集合，为了防止解码器再次生成相同的元组，在解码的每个时间步传递有关所有先前生成的元组的信息。yj是在时间步长j &lt;t预测的元组的向量表示，使用零向量（<img alt="y_{0}= \overrightarrow{0}" class="mathcode" src="https://private.codecogs.com/gif.latex?y_%7B0%7D%3D%20%5Coverrightarrow%7B0%7D">）表示伪元组y0，<img alt="h_{t-1}^{D}" class="mathcode" src="https://private.codecogs.com/gif.latex?h_%7Bt-1%7D%5E%7BD%7D">是时间t-1处LSTM的隐藏状态。</p><p style="text-align:center;"><img alt="" class="has" height="92" src="https://img-blog.csdnimg.cn/20191217111201422.png" width="500"></p><h1 style="text-indent:0px;"><a name="t7"></a><a name="t7"></a>Relation Tuple Extraction</h1><p style="text-indent:33px;">在获得当前元组&nbsp;<img alt="h_{t}^{D}" class="mathcode" src="https://private.codecogs.com/gif.latex?h_%7Bt%7D%5E%7BD%7D">的隐藏表示之后，首先在源语句中找到两个实体的开始和结束指针。将向量<img alt="h_{t}^{D}" class="mathcode" src="https://private.codecogs.com/gif.latex?h_%7Bt%7D%5E%7BD%7D">与编码器的隐藏向量<img alt="h_{i}^{E}" class="mathcode" src="https://private.codecogs.com/gif.latex?h_%7Bi%7D%5E%7BE%7D">连接起来，然后将它们传递给具有dp隐藏维度的Bi-LSTM层，以实现向前和向后LSTM。 该Bi-LSTM层的隐藏向量<img alt="h_{i}^{k}\in \mathbb{R}^{2d_{p}}" class="mathcode" src="https://private.codecogs.com/gif.latex?h_%7Bi%7D%5E%7Bk%7D%5Cin%20%5Cmathbb%7BR%7D%5E%7B2d_%7Bp%7D%7D">被传递到具有softmax的两个前馈网络（FFN），以将每个隐藏向量转换为介于0和1之间的两个标量值，Softmax操作应用于输入句子中的所有单词。这两个标量值表示相应的源句子标记成为第一个实体的开始和结束位置的概率。带有两个前向层的Bi LSTM层是标识当前关系元组的第一个实体的第一个指针网络。</p><p style="text-align:center;"><img alt="" class="has" height="100" src="https://img-blog.csdnimg.cn/20191217113902143.png" width="500"></p><p style="text-indent:0;"><img alt="s_{i}^{1},e_{i}^{1}" class="mathcode" src="https://private.codecogs.com/gif.latex?s_%7Bi%7D%5E%7B1%7D%2Ce_%7Bi%7D%5E%7B1%7D">&nbsp;表示第i个源字词是预测元组第一个实体的开始和结束标记的标准化概率。然后使用另一个的指针网络提取元组的第二个实体，将隐向量<img alt="h_{i}^{k}" class="mathcode" src="https://private.codecogs.com/gif.latex?h_%7Bi%7D%5E%7Bk%7D">和<img alt="h_{i}^{D}" class="mathcode" src="https://private.codecogs.com/gif.latex?h_%7Bi%7D%5E%7BD%7D">、<img alt="h_{i}^{E}" class="mathcode" src="https://private.codecogs.com/gif.latex?h_%7Bi%7D%5E%7BE%7D">结合起来送第二个指针网络来获得<img alt="s_{i}^{2}，e_{i}^{2}" class="mathcode" src="https://private.codecogs.com/gif.latex?s_%7Bi%7D%5E%7B2%7D%25uFF0Ce_%7Bi%7D%5E%7B2%7D">，<img alt="e_{i}^{2}" class="mathcode" src="https://private.codecogs.com/gif.latex?e_%7Bi%7D%5E%7B2%7D">。标准化概率用来找寻两实体的向量表示。</p><p style="text-align:center;"><img alt="" class="has" height="86" src="https://img-blog.csdnimg.cn/20191217152116912.png" width="500"></p><p style="text-indent:0;">然后将<img alt="a_{t}^{1},a_{t}^{2}" class="mathcode" src="https://private.codecogs.com/gif.latex?a_%7Bt%7D%5E%7B1%7D%2Ca_%7Bt%7D%5E%7B2%7D">跟<img alt="h_{t}^{D}" class="mathcode" src="https://private.codecogs.com/gif.latex?h_%7Bt%7D%5E%7BD%7D">结合送入带有sofamax的FFN来预测关系。</p><p style="text-align:center;"><img alt="" class="has" height="105" src="https://img-blog.csdnimg.cn/20191217154552776.png" width="500"></p><p style="text-indent:0;"><img alt="r_{t}" class="mathcode" src="https://private.codecogs.com/gif.latex?r_%7Bt%7D">&nbsp;表示时间步 t 时的预测关系的标准化概率，关系嵌入向量&nbsp;<img alt="z_{t}" class="mathcode" src="https://private.codecogs.com/gif.latex?z_%7Bt%7D">&nbsp;通过对 <img alt="r_{t}" class="mathcode" src="https://private.codecogs.com/gif.latex?r_%7Bt%7D">使用argmax函数&nbsp;和 <img alt="E_{r}" class="mathcode" src="https://private.codecogs.com/gif.latex?E_%7Br%7D">&nbsp;获得，<img alt="y_{t}" class="mathcode" src="https://private.codecogs.com/gif.latex?y_%7Bt%7D">&nbsp;是时间步 t 预测元组的的向量表示。当预测关系为EOS时解码器停止序列生成过程。在推理过程中，选择两个实体的开始和结束位置以使四个指针概率的乘积最大化，同时保持两个实体不相互重叠且1 &lt;b &lt;e &lt;n的约束条件，其中b和e是相应实体的开始和结束位置。首先根据相应的起点和终点指针概率的最大积选择实体1的起点和终点，然后以类似的方式找到实体2，排除实体1的跨度以避免重叠。重复相同的过程，但是这次首先找到实体2然后是实体1。选择该对实体，这对实体在这两个选择之间给出了四个指针概率的较高乘积。此模型此后称为PtrNetDecoding（PNDec）。</p><h1 style="text-indent:0px;"><a name="t8"></a><a name="t8"></a>Attention Modeling</h1><p style="text-indent:33px;">针对词级解码模型使用三种不同的注意力机制，以获得源上下文向量<img alt="e_{t}" class="mathcode" src="https://private.codecogs.com/gif.latex?e_%7Bt%7D">&nbsp;：</p><ol><li style="text-indent:0px;">Avg：上下文向量是通过平均编码的隐藏向量来获得的。</li><li style="text-indent:0px;"><p style="text-align:center;"><img alt="" class="has" height="309" src="https://img-blog.csdnimg.cn/20191217162901831.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwOTMxMTgx,size_16,color_FFFFFF,t_70" width="500"></p></li><li style="text-indent:0px;"><p style="text-align:center;"><img alt="" class="has" height="338" src="https://img-blog.csdnimg.cn/20191217162924798.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwOTMxMTgx,size_16,color_FFFFFF,t_70" width="500"></p>对于基于指针网络的解码模型使用single注意模型的三个变体。一使用<img alt="h_{t-1}^{D}" class="mathcode" src="https://private.codecogs.com/gif.latex?h_%7Bt-1%7D%5E%7BD%7D">在注意力机制中计算<img alt="q_{t}^{i}" class="mathcode" src="https://private.codecogs.com/gif.latex?q_%7Bt%7D%5E%7Bi%7D">，二使用<img alt="y_{prev}" class="mathcode" src="https://private.codecogs.com/gif.latex?y_%7Bprev%7D">计算<img alt="q_{t}^{i}" class="mathcode" src="https://private.codecogs.com/gif.latex?q_%7Bt%7D%5E%7Bi%7D">&nbsp;，三使用<img alt="h_{t-1}^{D}" class="mathcode" src="https://private.codecogs.com/gif.latex?h_%7Bt-1%7D%5E%7BD%7D">和<img alt="y_{prev}" class="mathcode" src="https://private.codecogs.com/gif.latex?y_%7Bprev%7D">获得的两个注意向量进行级联来获得注意上下文向量。通过基于指针网络的解码模型，可以提供最佳性能。&nbsp;</li></ol><h1 style="text-indent:0px;"><a name="t9"></a><a name="t9"></a>Loss Funtion</h1><p style="text-align:center;"><img alt="" class="has" height="157" src="https://img-blog.csdnimg.cn/20191217164256868.png" width="500"></p><p style="text-indent:0px;"><img alt="v_{t}^{b}" class="mathcode" src="https://private.codecogs.com/gif.latex?v_%7Bt%7D%5E%7Bb%7D">&nbsp;是单词级解码中时间步t处目标单词的softmax得分，r/s/e 是实体相应的真实关系标签、真实start和end指针位置的softmax得分，b/t/r 是第b个训练实例、解码时间步t和一个元组的两个实体，B/T分别是batch size和解码器的最大时间步。</p><p style="text-indent:0px;">&nbsp;</p><p style="text-indent:0;">&nbsp;</p><p style="text-indent:0;">&nbsp;</p>]]></content>
      
      
      <categories>
          
          <category> 笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文记录 </tag>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《Attention Guided Graph Convolutional Networks for Relation Extraction》阅读记录</title>
      <link href="/2020/03/04/article/"/>
      <url>/2020/03/04/article/</url>
      
        <content type="html"><![CDATA[<p>依存树传达了丰富的结构信息，事实证明，这些信息对于提取文本中实体之间的关系很有用。然而，如何有效地利用相关信息，同时又从依存树中忽略无关信息仍然是一个具有挑战性的研究问题。现有的采用基于规则的硬修剪策略来选择相关的部分依赖结构的方法不总是产生最佳结果。对此，本文提出 Attention Guided GCN（AGGCN），这是一个直接将完整依存树作为输入的新颖模型。该模型可以理解为一种软修剪方法，可以自动学习如何有选择地关注对关系提取任务有用的相关子结构。在包括交叉句子n元关系提取和大规模句子级关系提取在内的各种任务上的大量结果表明，AGGCN模型能够更好地利用完整依存树的结构信息，比以前的方法具有明显更好的结果。</p><a id="more"></a><h1><a name="t2"></a><a name="t2"></a>1&nbsp; Introduction</h1><p style="text-indent:33px;">关系提取旨在检测文本中实体之间的关系，在各种自然语言处理应用程序中都有应用，包括生物医学知识发现、知识库填充以及问题解答。图1显示了一个用两个句子表达三个实体L858E，EGFR和 gefitinib 之间的关联敏感度的示例。大多数现有的关系提取模型可以分为两类：基于序列的和基于依赖的。基于序列的模型仅对单词序列起作用，而基于依赖的模型将依存树合并到模型中。与基于序列的模型相比，<strong>基于依存树的模型能够捕获表面看不到的非局部句法关系</strong>，并存在各种修剪策略来提取依赖信息，以进一步提升表现。Xu等人仅在完整树实体之间的最短依赖路径上应用神经网络，Miwa 和 Bansal 将整个树缩减为实体的最低公共祖先（LCA）以下的子树。 Zhang等人在修剪的树上应用图卷积网络（GCN）模型，该树包含与LCA子树中依赖关系路径上相距为K的token。</p><p style="text-indent:33px;">但是，基于规则的修剪策略可能会消除完整树中的一些重要信息。图1显示了跨语句n元关系提取中的一个示例，该示例中如果模型仅考虑修剪的树，则将排除关键token <em>“partial response”</em>。理想情况下，模型应该能够学习如何在完整树中平衡地选择和排除信息。本文提出了Attention Guided GCN（AGGCN），该网络可直接在完整树上运行。直观上看是一种“软修剪”策略，<span style="color:#f33b45;"><strong>将原始依存树转换为完全连接的边加权图</strong></span>。这些权重可以看作是节点之间相关性的强度，<strong>可以通过使用自我注意机制以端到端的方式进行学习</strong>。为了编码一个<strong>更大</strong>的完全连接图，则进行GCN模型的<strong>密集连接</strong>。对于GCN，L层用以捕获距离 L hops的邻居信息。浅层GCN模型可能无法捕获大图中非局部的相互关系。虽然更深的GCN可以捕获图形的更丰富的邻域信息，但从经验上看使用2层模型可以实现最佳性能。借助密集连接能够以较大的深度训练AGGCN模型，从而可以捕获丰富的本地和非本地依赖信息。实验表明，AGGCN模型能够针对各种任务实现更好的性能，对于跨句关系提取任务（多类三元和二元关系提取）该模型在准确性方面比当前的最新模型分别高8％和6％，对于大规模句子级提取任务（TACREDdataset）也优于其他模型，这表明该模型在大型训练集上的有效性。</p><p style="text-indent:0;"><strong>贡献：</strong><br>• 提出了AGGCN，它可以<strong>以端到端的方式学习“软修剪”策略</strong>，从而学习如何选择和丢弃信息。结合密集连接能够学习更好的图表示。<br>• 与以前的GCN相比，AGGCN模型无需任何额外的计算开销就能获得最优的结果。与树结构模型不同，它可以有效地<strong>并行</strong>应用于依赖树。</p><p style="text-align:center;"><img alt="" class="has" height="488" src="https://img-blog.csdnimg.cn/20191105212132141.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwOTMxMTgx,size_16,color_FFFFFF,t_70" width="1200"></p><blockquote><p style="text-indent:0;">图1：两个句子的依存树示例，表示三个实体之间的关系。这些实体之间的最短依赖路径以粗体（边和标记）突出显示。实体LCA子树的根节点是 <em>present，</em>虚线边表示距离子树 K=1 的token，注意 <em>partial response。</em></p></blockquote><h1><a name="t3"></a><a name="t3"></a>2&nbsp; Attention Guided GCNs</h1><h2><a name="t4"></a><a name="t4"></a>2.2&nbsp; Attention Guided Layers （暂记为注意引导层）</h2><p style="text-indent:33px;">AGGCN模型<strong>由M个相同的块组成</strong>，如图2所示。<strong>每个块由三种类型的层组成：注意引导层，密集连接层和线性组合层</strong>。已知大多数现有的修剪策略都是<strong>预先定义</strong>的，将整棵树修剪成一个子树，在该子树的基础上构造邻接矩阵。实际上这样的策略也可以看作是一种hard attention 的形式，其中连接不在结果子树上的节点的边将被直接分配零权重，这样的策略可能会从原始的依赖树中消除相关信息。本文在注意力引导层中使用“软修剪”策略，该策略<strong>将权重分配给所有边，权重可以由模型以端到端的方式学习</strong>。在注意力引导层中，<span style="color:#f33b45;"><strong>通过构造注意力引导邻接矩阵A将原始依存树转换为完全连接的边加权图</strong></span>。每个A对应于某个完全连接的图，每个&nbsp;<img alt="A_{ij}" class="mathcode" src="https://private.codecogs.com/gif.latex?A_%7Bij%7D">&nbsp;是从节点i到节点j的边权重。如图2所示，A（1）表示完全连接的图G（1）。通过使用<strong>自我注意力机制</strong>来构建A，自我注意力机制是一种捕获单个序列中两个任意位置之间的相互关系的注意力机制。一旦获得A就可以将其用作计算后面的图卷积层的输入。A的大小与原始邻接矩阵相同（n×n），不涉及额外的计算开销。注意引导层的关键思想是使用注意力来引导节点之间的关系，尤其是对于那些通过间接多跳路径直接连接的节点。这些软关系可以通过模型中的可区分函数来捕获。这里使用<strong>多头注意力</strong>来计算，使模型可以共同关注来自不同表示子空间的信息，该计算涉及一个查询和一组键值对，将输出计算为值的加权总和，其中权重是通过具有相应键的查询函数来计算的。</p><p style="text-align:center;"><img alt="" class="has" height="83" src="https://img-blog.csdnimg.cn/2019110615523579.png" width="500"></p><p style="text-indent:0;">其中Q和K都等于AGGCN模型的第l-1层的集合表示h（l-1），A（t）是与第t个头部相对应的第t个注意引导的邻接矩阵，最多可以构造N个矩阵，其中N是一个超参数。 图2显示了一个示例，该示例将原始邻接矩阵转换为多个注意引导的邻接矩阵，输入依赖树被转换为多个完全连接的边加权图。 在实践中将原始邻接矩阵视为初始化，以便可以在节点表示形式中捕获依赖关系信息，以便以后进行注意力计算。 从第二个块开始包含注意引导层。</p><p style="text-align:center;"><img alt="" class="has" height="602" src="https://img-blog.csdnimg.cn/20191106160205674.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwOTMxMTgx,size_16,color_FFFFFF,t_70" width="1200"></p><blockquote><p style="text-indent:0;">图2：AGGCN模型，示例语句及其依赖树。由M个相同的块组成，每个块具有三种类型的层，如右图所示<strong>。每个块都将节点嵌入和表示图的邻接矩阵作为输入</strong>。如左下图所示，<strong>使用多头注意力机制构建N个注意引导的邻接矩阵</strong>，原始的依赖树被转换为N个不同的完全连接的边加权图（为简化起见省略自循环），边附近的数字表示矩阵中的权重。结果矩阵被送到N个独立的紧密连接层中，从而生成新的表示形式。左上方显示了一个紧密连接层的示例，其中子层的数量L为3（L is a hyper-parameter），<strong>每个子层将所有先前的输出连接为输入</strong>。应用线性组合将N个紧密连接的层的输出组合成隐藏的表示形式。</p></blockquote><h2><a name="t5"></a><a name="t5"></a>2.3&nbsp; Densely Connected Layer</h2><p style="text-indent:33px;">与先前的修剪策略不同（生成的结构比原始结构小），<strong>注意力引导层会输出较大的完全连接图</strong>。在AGGCN模型中引入密集连接，以便在大型图上<strong>捕获更多结构信息</strong>，训练<strong>更深的模型</strong>，从而可以<strong>捕获丰富的局部和非局部信息</strong>，以学习<strong>更好的图表示</strong>形式。密集连接如图2所示，直连接是从任意层引入到其所有先前的层。将 <img alt="g_{j}^{\left ( l \right )}" class="mathcode" src="https://private.codecogs.com/gif.latex?g_%7Bj%7D%5E%7B%5Cleft%20%28%20l%20%5Cright%20%29%7D">&nbsp;定义为初始节点表示和1到 l-1层中产生的节点表示的级联：</p><p style="text-align:center;"><img alt="" class="has" height="56" src="https://img-blog.csdnimg.cn/20191106172141308.png" width="400"></p><p style="text-indent:0;">实际上每个密集连接的层都有L个子层，这些子层&nbsp;<img alt="d_{hidden}" class="mathcode" src="https://private.codecogs.com/gif.latex?d_%7Bhidden%7D">&nbsp;的维度由 L 和输入特征维度 d 决定。在AGGCN中<strong>&nbsp;<img alt="d_{hidden}=d/L" class="mathcode" src="https://private.codecogs.com/gif.latex?d_%7Bhidden%7D%3Dd/L"></strong>。例如，如果密集连接层具有3个子层，并且输入维度为300，则每个子层的 hidden 维度将变为&nbsp;<img alt="d_{hidden}" class="mathcode" src="https://private.codecogs.com/gif.latex?d_%7Bhidden%7D">&nbsp;= d / L = 300/3 =100。然后将每个子层的输出连接在一起以形成新的表示形式，最终输出尺寸为300（3×100）。与GCN模型的 hidden 维度大于或等于输入维度不同，<strong>AGGCN模型会随着层数的增加而缩小 hidden 维度</strong>，以类似于DenseNets一样提高参数效率。由于有N个不同的注意力引导邻接矩阵，因此需要N个独立的密集连接层。因此修改每个层的计算如下（对于第t个矩阵&nbsp;<img alt="\widetilde{A}^{\left ( t \right )}" class="mathcode" src="https://private.codecogs.com/gif.latex?%5Cwidetilde%7BA%7D%5E%7B%5Cleft%20%28%20t%20%5Cright%20%29%7D">）：</p><p style="text-align:center;"><img alt="" class="has" height="74" src="https://img-blog.csdnimg.cn/20191106200938418.png" width="350"></p><p style="text-indent:0;">其中t = 1，...，N 并选择与注意引导邻接矩阵&nbsp;<img alt="\widetilde{A}^{\left ( t \right )}" class="mathcode" src="https://private.codecogs.com/gif.latex?%5Cwidetilde%7BA%7D%5E%7B%5Cleft%20%28%20t%20%5Cright%20%29%7D">&nbsp;相关的权重矩阵和偏差项。权重矩阵的列维度每子层增加&nbsp;<img alt="d_{hidden}" class="mathcode" src="https://private.codecogs.com/gif.latex?d_%7Bhidden%7D">&nbsp;，即<img alt="W_{t}^{\left ( l \right )}\in \mathbb{R}^{d_{hidden}\times d^{\left ( l \right )}}" class="mathcode" src="https://private.codecogs.com/gif.latex?W_%7Bt%7D%5E%7B%5Cleft%20%28%20l%20%5Cright%20%29%7D%5Cin%20%5Cmathbb%7BR%7D%5E%7Bd_%7Bhidden%7D%5Ctimes%20d%5E%7B%5Cleft%20%28%20l%20%5Cright%20%29%7D%7D">，其中&nbsp;<img alt="d^{\left ( l \right )}=d+d_{hidden}\times \left ( l-1 \right )" class="mathcode" src="https://private.codecogs.com/gif.latex?d%5E%7B%5Cleft%20%28%20l%20%5Cright%20%29%7D%3Dd&amp;plus;d_%7Bhidden%7D%5Ctimes%20%5Cleft%20%28%20l-1%20%5Cright%20%29">。</p><h2><a name="t6"></a><a name="t6"></a>2.4&nbsp; Linear Combination Layer</h2><p style="text-indent:33px;">AGGCN模型包括一个线性组合层，以整合N个不同的密集连接层的表示。线性组合层的输出定义为：</p><p style="text-align:center;"><img alt="" class="has" height="63" src="https://img-blog.csdnimg.cn/20191106202157299.png" width="400"></p><p style="text-indent:0;">其中&nbsp;<img alt="h_{out}" class="mathcode" src="https://private.codecogs.com/gif.latex?h_%7Bout%7D">&nbsp;是通过合并来自N个独立密集连接层的输出而得到的输出，即 <img alt="h_{out}=\left [ h^{\left ( 1 \right )};\cdots ;h^{\left ( N \right )} \right ]\in \mathbb{R}^{d\times N}" class="mathcode" src="https://private.codecogs.com/gif.latex?h_%7Bout%7D%3D%5Cleft%20%5B%20h%5E%7B%5Cleft%20%28%201%20%5Cright%20%29%7D%3B%5Ccdots%20%3Bh%5E%7B%5Cleft%20%28%20N%20%5Cright%20%29%7D%20%5Cright%20%5D%5Cin%20%5Cmathbb%7BR%7D%5E%7Bd%5Ctimes%20N%7D">。 <img alt="W_{comb}\in \mathbb{R}^{\left ( d\times N \right )}\times d" class="mathcode" src="https://private.codecogs.com/gif.latex?W_%7Bcomb%7D%5Cin%20%5Cmathbb%7BR%7D%5E%7B%5Cleft%20%28%20d%5Ctimes%20N%20%5Cright%20%29%7D%5Ctimes%20d">是权重矩阵，<img alt="h_{comb}" class="mathcode" src="https://private.codecogs.com/gif.latex?h_%7Bcomb%7D">&nbsp;是线性变换的偏置向量。</p><h2><a name="t7"></a><a name="t7"></a>2.5&nbsp; AGGCNS for Relation Extraction</h2><p style="text-indent:33px;">在依赖树上应用AGGCN模型获得了所有token的隐藏表示，给定这些表示形式，预测实体之间的关系。将句子表示和实体表示连接起来以获得最终的分类表示。首先需要获取句子表示形式，<img alt="h_{sent}=f\left ( h_{mask} \right )=f\left ( AGGCN\left ( x \right ) \right )\left ( 6 \right )" class="mathcode" src="https://private.codecogs.com/gif.latex?h_%7Bsent%7D%3Df%5Cleft%20%28%20h_%7Bmask%7D%20%5Cright%20%29%3Df%5Cleft%20%28%20AGGCN%5Cleft%20%28%20x%20%5Cright%20%29%20%5Cright%20%29%5Cleft%20%28%206%20%5Cright%20%29">，其中&nbsp;<img alt="h_{mask}" class="mathcode" src="https://private.codecogs.com/gif.latex?h_%7Bmask%7D">&nbsp;表示掩码集合的隐藏表示，<strong>掩码旨仅选择不是句子中实体的token的表示形式</strong>。<img alt="f:\mathbb{R}^{d\times n}\rightarrow \mathbb{R}^{d\times 1}" class="mathcode" src="https://private.codecogs.com/gif.latex?f%3A%5Cmathbb%7BR%7D%5E%7Bd%5Ctimes%20n%7D%5Crightarrow%20%5Cmathbb%7BR%7D%5E%7Bd%5Ctimes%201%7D">&nbsp;是最大池化函数，可将n个输出向量映射为1个句子向量。同样可以得到实体表示，对于第 i 个实体，其表示为：<img alt="h_{e_{i}}=f\left ( h_{e_{i}} \right )\left ( 7 \right )" class="mathcode" src="https://private.codecogs.com/gif.latex?h_%7Be_%7Bi%7D%7D%3Df%5Cleft%20%28%20h_%7Be_%7Bi%7D%7D%20%5Cright%20%29%5Cleft%20%28%207%20%5Cright%20%29">，其中&nbsp;<img alt="h_{e_{i}}" class="mathcode" src="https://private.codecogs.com/gif.latex?h_%7Be_%7Bi%7D%7D">&nbsp;表示对应于第i个实体的隐藏表示。实体表示与句子表示合并以形成新的表示。将前馈神经网络应用于级联表示，<img alt="h_{final}" class="mathcode" src="https://private.codecogs.com/gif.latex?h_%7Bfinal%7D">用作逻辑回归分类器的输入以进行预测。</p><p style="text-align:center;"><img alt="" class="has" height="71" src="https://img-blog.csdnimg.cn/20191106205511860.png" width="400"></p><h1><a name="t8"></a><a name="t8"></a>3&nbsp; Experiments</h1><h2><a name="t9"></a><a name="t9"></a>3.1&nbsp; Data</h2><p style="text-indent:33px;">评估模型在两个任务上的性能，即跨句子n元关系提取和句子级关系提取。对于跨语句n元关系提取任务，引入的数据集，其中包含6987个三元关系实例和6087个从PubMed中提取的二元关系实例。大多数实例包含多个句子，并且每个实例都分配有五个标签之一，包括：&nbsp;“resistance or nonresponse”, “sensitivity”, “response”, “resistance”and“None”。考虑两个特定的评估任务，即二分类n元关系提取和多类n元关系提取。对于二分类n元关系提取，通过将四个关系类分组为“Yes”并将“None”视为“No”来对多类标签进行二值化。对于句子级关系提取任务，按照Zhang等人在TACRED数据集和Semeval-10 Task 8的实验设置，TACRED数据集拥有超过106K实例，引入了41种关系类型和特殊的“no relation”类型来描述实例中提及对之间的关系。主题分为个人和组织，而对象分为16种细粒度类型，包括日期，位置等。Semeval-10 Tasl 8是一个公共数据集，其中包含10,717个实例（具有9个关系）和特殊的“other” 类。</p><h2><a name="t10"></a><a name="t10"></a>3.2&nbsp; Results &nbsp;</h2><p><strong>Cross-Sentence n-ary Relation Extraction</strong></p><p style="text-indent:33px;">这些结果表明，与之前的基于完整树的方法（例如GS GLSTM）相比，AGGCN能够从基础图结构中提取更多信息，以通过图卷积学习更富表现力的表示形式。尽管可以通过修剪树木提高其性能，但AGGCN的性能也优于GCN。作者认为这是由于紧密连接层和注意引导层的结合所致，密集的连接可以促进大型图形中的信息传播，从而使AGGCN可以有效地从长距离依赖中学习而无需修剪技术。同时，注意力引导层可以进一步提取相关信息，并从密集连接层学习到的表示中滤除噪声。对于多类分类任务，这种细粒度的分类任务比粗粒度的分类任务困难得多，结果所有模型的性能都会大大降低。但是对于三元和二元关系，AGGCN模型仍然比GS GLSTM模型分别获得8.0和5.7分。与所有GCN模型相比，AGGCN具有更好的测试准确性，这进一步证明了其从完整树中学习更好表示的能力。</p><p style="text-align:center;"><img alt="" class="has" height="545" src="https://img-blog.csdnimg.cn/20191107104833593.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwOTMxMTgx,size_16,color_FFFFFF,t_70" width="900"></p><p style="text-indent:0;"><strong>Sentence-level Relation Extraction</strong></p><p style="text-align:center;"><img alt="" class="has" height="414" src="https://img-blog.csdnimg.cn/20191107110519765.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwOTMxMTgx,size_16,color_FFFFFF,t_70" width="800"></p><h2><a name="t11"></a><a name="t11"></a>3.3&nbsp; Analysis and Discussion</h2><p style="text-indent:0;"><strong>Ablation Study&nbsp;</strong>&nbsp;使用TACRED数据集上性能最佳的C-AGGCN模型来检查两个主要组件（即密集连接的层和注意引导层）的贡献。表4示出了结果。 可以看到添加注意引导层或密集连接的层可以提高模型的性能，这表明两个层都可以帮助GCN学习更好的信息聚合，从而为图形提供更好的表示，在注意引导层似乎起着更重要的作用。前馈层在我们的模型中是有效的，如果没有前馈层，结果将降至F1得分67.8。</p><p style="text-indent:0;"><strong>Performance with Purned Trees</strong>&nbsp;&nbsp;表5显示了带有修剪树的C-AGGCN模型的性能，其中K表示修剪树包含的token与LCA子树中的依赖路径的距离最大为K。所有具有K变量值的C-AGGCN模型都能够胜过最新的C-GCN模型（表2中报告）。 具体来说在与K = 1相同的设置下，C-AGGCN的得分比C-GCN高出1.5分。这表明在密集连接层和注意引导层的组合下，对于下游任务C-AGGCN比C-GCN可以学习更好的表示。 此外，具有完整树的C-AGGCN的性能优于具有修剪树的所有C-AGGCN，这些结果进一步表明，在利用完整树信息方面，“软修剪”策略优于硬修剪策略。</p><p style="text-indent:0;"><strong>Performance against Sentence Length</strong>&nbsp;&nbsp;图4显示了三个模型在不同句子长度下的F1分数。将句子的长度分为五个类别（&lt;20, [20、30], [30、40), [40、50), ≥50)。通常具有完整树的C-AGGCN在各种句子长度方面的表现要优于具有修剪树的C-AGGCN和C-GCN。在大多数情况下，具有修剪树的C-AGGCN的效果要优于C-GCN。当句子长度增加时，C-AGGCN对修剪的树的改进效果会下降，可以通过使用完整树来避免这种性能下降，因为完整树会提供有关隐含图结构的更多信息。随着句子长度的增加，依赖关系图随着包含更多节点而变得更大，表明C-AGGCN可以从较大的图（完整树）中受益更多。</p><p style="text-align:center;"><img alt="" class="has" height="417" src="https://img-blog.csdnimg.cn/20191107151215670.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwOTMxMTgx,size_16,color_FFFFFF,t_70" width="500"></p><p style="text-indent:0;"><strong>Performance against Training Data Size&nbsp;&nbsp;</strong>图3显示了C-AGGCN和CGCN在不同训练设置下的性能。考虑五个训练设置（训练数据的20％，40％，60％，80％，100％)。在相同数量的训练数据下，C-AGGCN在性能上都优于C-GCN。当训练数据的大小增加时，可以观察到性能差距变得更加明显。特别是使用80％的训练数据，C-AGGCN模型能够获得66.5的F1 Score，高于整个数据集上训练的C-GCN。</p><p style="text-align:center;"><img alt="" class="has" height="453" src="https://img-blog.csdnimg.cn/20191107151245569.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwOTMxMTgx,size_16,color_FFFFFF,t_70" width="1100"></p><h1><a name="t12"></a><a name="t12"></a>5&nbsp; Clonclusion</h1><p style="text-indent:0;">&nbsp;</p><p style="text-indent:0;">&nbsp;</p>]]></content>
      
      
      <categories>
          
          <category> 笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文记录 </tag>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《Graph Neural Networks with Generated Parameters for Relation Extraction》阅读笔记</title>
      <link href="/2020/03/04/article-2/"/>
      <url>/2020/03/04/article-2/</url>
      
        <content type="html"><![CDATA[<p>近年来，在机器学习领域，关系推理的改进取得了进展。在现有模型中，图神经网络是多跳关系推理的最有效方法之一。事实上，多跳关系推理在许多自然语言处理任务中是不可缺少的，例如关系抽取。本文通过自然语言语句提出带有生成参数的图神经网络，使图神经网络能够处理非结构化文本输入的关系推理。在一个人工标注数据和两个远程监督数据集上的实验结果表明，与baseline相比，GP-GNN模型取得了显著的提升。Case Study 可看到模型可以通过多跳关系推理发现更准确的关系。</p><a id="more"></a><h1><a name="t2"></a><a name="t2"></a>1 Introduction</h1><p style="text-indent:33px;">近年来，图神经网络已被应用于机器学习的各个领域，包括节点分类、关系分类、分子性质预测、小样本学习，并在这些任务上取得了优秀的成果，证明了GNNs处理图关系推理的强大能力。<strong>关系推理旨在抽象地推理实体/对象及其关系——给定一个具有m个实体的文本序列，对文本和实体进行推理并预测实体或实体对的类别</strong>。除了图之外，关系推理在许多自然语言处理任务中也很重要，如问答、关系抽取、摘要等。考虑图1所示的例子，现有的关系抽取模型可以很容易地抽取出Luc Besson导演的电影《Leon：The Professional》和这部电影是English的事实，但是如果没有多跳关系推理，就无法推断出Luc Besson和English之间的关系。通过对推理模式的研究，可以发现Luc Besson会说英语，遵循<em>“Luc Besson执导的《Leon：The Professional》这部电影是用English拍摄的，表明Luc Besson会说English” </em>这一推理逻辑。然而，大多数现有的GNNs只能处理<strong>预定义图上</strong>的多跳关系推理，不能直接应用于自然语言关系推理。在自然语言中实现多跳关系推理仍然是一个未解决的问题。</p><p style="text-align:center;"><img alt="" class="has" height="381" src="https://img-blog.csdnimg.cn/20191031201957110.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwOTMxMTgx,size_16,color_FFFFFF,t_70" width="950"></p><blockquote><p style="text-indent:0;">给定一个带有多个实体的句子，<strong>通过生成图神经网络的权重来对这些实体之间的关系进行建模。</strong>对“L'eon”和“英语”以及“Luc Besson”之间关系的建模有助于发现“Luc Besson”和“English”之间的关系。</p></blockquote><p style="text-indent:33px;">为了解决这个问题，本文提出了<strong>带有生成参数（GP-GNN）的图神经网络</strong>，以适应GNN来解决自然语言关系推理任务。 GP-GNN首先<strong>用文本序列中的实体构造一个全连接图</strong>，&nbsp;之后使用三个模块来处理关系推理：（1）<strong>使边能够对自然语言中的丰富信息进行编码的编码模块</strong>；（2）<strong>在各个节点之间传播关系信息的传播模块</strong>；（3）<strong>使用节点表示进行预测的分类模块</strong>。 与传统的GNN相比，<strong>GP-GNN可以从自然语言中学习边的参数，将其范围从仅对非关系图或边类型数量有限的图进行推断扩展到文本等非结构化输入。</strong>实验中将GP-GNNs应用于一个经典的自然语言关系推理任务：从文本中提取关系。在维基百科语料库中进行了与维基数据库知识库相结合的实验，建立了一个人工标注测试集和两个具有不同密集度的远距标注测试集。实验结果表明，通过考虑多跳关系推理，该模型在关系抽取任务上优于其他模型。定性分析结果表明，与基模型相比，该模型通过推理可以发现更多的关系。</p><p style="text-indent:33px;"><strong>主要贡献：</strong></p><p style="text-indent:33px;">1）提出了一个新的带有生成参数的图神经网络模型，实现了带有丰富文本信息的关系消息传递，可以应用于处理非结构化输入(如自然语言)的关系推理。</p><p style="text-indent:33px;">2）在文本关系抽取任务中验证了GP-GNNs与抽取关系模型的对比，证明了其多跳关系推理能力。</p><h1 style="text-indent:0px;"><a name="t3"></a><a name="t3"></a>2&nbsp; Related Work</h1><h1><a name="t4"></a><a name="t4"></a>3&nbsp; GP-GNNs</h1><p style="text-indent:33px;">GP-GNN首先<strong>建立完全连通图</strong><img alt="G=\left ( \nu,\varepsilon \right )" class="mathcode" src="https://private.codecogs.com/gif.latex?G%3D%5Cleft%20%28%20%5Cnu%2C%5Cvarepsilon%20%5Cright%20%29">，其中V是实体的集合，每个边<img alt="\left ( \nu _{i},\nu _{j} \right )\in \varepsilon" class="mathcode" src="https://private.codecogs.com/gif.latex?%5Cleft%20%28%20%5Cnu%20_%7Bi%7D%2C%5Cnu%20_%7Bj%7D%20%5Cright%20%29%5Cin%20%5Cvarepsilon">，<img alt="\nu _{i},\nu _{j}\in \nu" class="mathcode" src="https://private.codecogs.com/gif.latex?%5Cnu%20_%7Bi%7D%2C%5Cnu%20_%7Bj%7D%5Cin%20%5Cnu">对应于从文本中提取的序列<img alt="s= x_{0}^{i,j},x_{1}^{i,j},\cdots ,x_{l-1}^{i,j}" class="mathcode" src="https://private.codecogs.com/gif.latex?s%3D%20x_%7B0%7D%5E%7Bi%2Cj%7D%2Cx_%7B1%7D%5E%7Bi%2Cj%7D%2C%5Ccdots%20%2Cx_%7Bl-1%7D%5E%7Bi%2Cj%7D">。之后GP-GNN采用三个模块进行关系推理，包括<strong>编码模块、传播模块和分类模块</strong>，如图2所示。</p><p style="text-align:center;"><img alt="" class="has" height="382" src="https://img-blog.csdnimg.cn/20191101111536455.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwOTMxMTgx,size_16,color_FFFFFF,t_70" width="950"></p><p style="text-indent:0;">总体架构：编码模块将一系列向量表示作为输入，并输出一个转移矩阵； 传播模块利用生成的转移矩阵将隐藏状态从节点传播到其邻居。分类模块根据节点表示提供与任务相关的预测。</p><h2 style="text-indent:0px;"><a name="t5"></a><a name="t5"></a>3.1&nbsp; Encoding Module</h2><p style="text-indent:33px;"><strong>编码模块将序列转换为边相关的转移矩阵</strong>，即传播模块的参数，通过以下公式。其中<img alt="f\left ( \cdot \right )" class="mathcode" src="https://private.codecogs.com/gif.latex?f%5Cleft%20%28%20%5Ccdot%20%5Cright%20%29">&nbsp;是可以对序列数据进行编码的任何模型，例如LSTM、GRU、CNN，<img alt="E\left ( \cdot \right )" class="mathcode" src="https://private.codecogs.com/gif.latex?E%5Cleft%20%28%20%5Ccdot%20%5Cright%20%29">&nbsp;表示嵌入函数，<img alt="\theta _{e}^{n}" class="mathcode" src="https://private.codecogs.com/gif.latex?%5Ctheta%20_%7Be%7D%5E%7Bn%7D">&nbsp;表示第n层编码模块的参数。</p><p style="text-align:center;"><img alt="" class="has" height="79" src="https://img-blog.csdnimg.cn/20191101160432734.png" width="500"></p><h2 style="text-indent:0px;"><a name="t6"></a><a name="t6"></a>3.2&nbsp; Propagation Module</h2><p style="text-indent:33px;"><strong>传播模块逐层学习节点的表示形式</strong>，节点的初始嵌入（即第0层的表示）与任务相关，可以是对节点特征进行编码的嵌入，也可以是one-hot编码嵌入。给定层n的表示形式，<strong>n +1 层</strong>的表示形式通过下面公式计算，其中&nbsp;<img alt="N\left ( \nu _{i} \right )" class="mathcode" src="https://private.codecogs.com/gif.latex?N%5Cleft%20%28%20%5Cnu%20_%7Bi%7D%20%5Cright%20%29">&nbsp;表示图G中节点v的邻域，<img alt="\sigma \left ( \cdot \right )" class="mathcode" src="https://private.codecogs.com/gif.latex?%5Csigma%20%5Cleft%20%28%20%5Ccdot%20%5Cright%20%29">&nbsp;表示非线性激活函数。</p><p style="text-align:center;"><img alt="" class="has" height="74" src="https://img-blog.csdnimg.cn/20191102163404369.png" width="400"></p><h2 style="text-indent:0px;"><a name="t7"></a><a name="t7"></a>3.3&nbsp; Classification Module</h2><p style="text-indent:33px;">分类模块将节点表示作为输入和输出预测，GP-GNN的<strong>损失公式</strong>则为（3），其中&nbsp;<img alt="\theta _{c}" class="mathcode" src="https://private.codecogs.com/gif.latex?%5Ctheta%20_%7Bc%7D">&nbsp;表示分类模块的参数，K表示传播模块的层数，Y表示真实标签，通过梯度下降方法训练。</p><p style="text-align:center;"><img alt="" class="has" height="79" src="https://img-blog.csdnimg.cn/20191102163607480.png" width="500"></p><h1 style="text-indent:0px;"><a name="t8"></a><a name="t8"></a>4&nbsp; Relation Extraction with GP-GNNs</h1><p style="text-indent:33px;">从文本中提取关系是经典的自然语言关系推理任务。 给定句子<img alt="s=\left ( x_{0},x_{1},\cdots ,x_{l-1} \right )" class="mathcode" src="https://private.codecogs.com/gif.latex?s%3D%5Cleft%20%28%20x_%7B0%7D%2Cx_%7B1%7D%2C%5Ccdots%20%2Cx_%7Bl-1%7D%20%5Cright%20%29">，此句子中的一组关系R和一组实体<img alt="\nu _{s}" class="mathcode" src="https://private.codecogs.com/gif.latex?%5Cnu%20_%7Bs%7D">，其中每个<img alt="\nu _{i}" class="mathcode" src="https://private.codecogs.com/gif.latex?%5Cnu%20_%7Bi%7D">&nbsp;由一个或一系列token组成，从文本中提取关系是为了识别每个实体对&nbsp;<img alt="\left ( v_{i},v_{j} \right )" class="mathcode" src="https://private.codecogs.com/gif.latex?%5Cleft%20%28%20v_%7Bi%7D%2Cv_%7Bj%7D%20%5Cright%20%29">&nbsp;之间的成对关系<img alt="r_{v_{i},v_{j}}\in R" class="mathcode" src="https://private.codecogs.com/gif.latex?r_%7Bv_%7Bi%7D%2Cv_%7Bj%7D%7D%5Cin%20R">。</p><h2 style="text-indent:0px;"><a name="t9"></a><a name="t9"></a>4.1&nbsp; Encoding Module</h2><p style="text-indent:33px;">为了对实体对（或图中的边）的上下文进行编码，首先将<strong>位置嵌入与句子中的单词嵌入连接起来</strong>：<img alt="E\left ( x_{t}^{i,j} \right )= \left [ x_{t};p_{t}^{i,j} \right ]" class="mathcode" src="https://private.codecogs.com/gif.latex?E%5Cleft%20%28%20x_%7Bt%7D%5E%7Bi%2Cj%7D%20%5Cright%20%29%3D%20%5Cleft%20%5B%20x_%7Bt%7D%3Bp_%7Bt%7D%5E%7Bi%2Cj%7D%20%5Cright%20%5D">；其中&nbsp;<img alt="x_{t}" class="mathcode" src="https://private.codecogs.com/gif.latex?x_%7Bt%7D">&nbsp;表示单词&nbsp;<img alt="x_{t}" class="mathcode" src="https://private.codecogs.com/gif.latex?x_%7Bt%7D">&nbsp;的词嵌入，<img alt="p_{t}^{i,j}" class="mathcode" src="https://private.codecogs.com/gif.latex?p_%7Bt%7D%5E%7Bi%2Cj%7D">&nbsp;表示单词位置 t 相对于实体对的位置 i,j 的位置嵌入。之后<strong>将实体对的表示送入编码器</strong>&nbsp;<img alt="f\left ( \cdot \right )" class="mathcode" src="https://private.codecogs.com/gif.latex?f%5Cleft%20%28%20%5Ccdot%20%5Cright%20%29">，该编码器包含双向LSTM和多层感知器，其中n表示层索引（添加索引意味着网络模型各层参数不同），<img alt="\left [ \cdot \right ]" class="mathcode" src="https://private.codecogs.com/gif.latex?%5Cleft%20%5B%20%5Ccdot%20%5Cright%20%5D">&nbsp;表示将向量转换为矩阵，BiLSTM通过将<strong>前向LSTM的尾部隐藏状态和后向LSTM的头部隐藏状态</strong>串联在一起来编码序列，MLP表示具有非线性激活的多层感知器。</p><p style="text-align:center;"><img alt="" class="has" height="77" src="https://img-blog.csdnimg.cn/20191102172028925.png" width="600"></p><p style="text-indent:0px;"><strong>Word Representations&nbsp; </strong>首先使用词嵌入矩阵<img alt="W_{e}\in \mathbb{R}^{\left | V \right |\times d_{w}}" class="mathcode" src="https://private.codecogs.com/gif.latex?W_%7Be%7D%5Cin%20%5Cmathbb%7BR%7D%5E%7B%5Cleft%20%7C%20V%20%5Cright%20%7C%5Ctimes%20d_%7Bw%7D%7D">，将句子 {x0，x1，...，xl-1} 的每个标记 <img alt="x_{t}" class="mathcode" src="https://private.codecogs.com/gif.latex?x_%7Bt%7D">&nbsp;映射到 k 维嵌入向量<img alt="x_{t}" class="mathcode" src="https://private.codecogs.com/gif.latex?x_%7Bt%7D">，其中| V |是词汇量的大小。 本文中使用60亿大小的语料库进行50维的GloVe嵌入预训练。</p><p><strong>Position Embedding</strong>&nbsp; 将句子中的每个token标记为属于第一实体 <img alt="\nu _{i}" class="mathcode" src="https://private.codecogs.com/gif.latex?%5Cnu%20_%7Bi%7D">，第二实体 <img alt="\nu _{j}" class="mathcode" src="https://private.codecogs.com/gif.latex?%5Cnu%20_%7Bj%7D">&nbsp;或都不属于这两个实体，每个位置标记还通过位置嵌入矩阵<img alt="P\in \mathbb{R}^{3\times d_{p}}" class="mathcode" src="https://private.codecogs.com/gif.latex?P%5Cin%20%5Cmathbb%7BR%7D%5E%7B3%5Ctimes%20d_%7Bp%7D%7D">&nbsp;映射到 <img alt="d_{p}" class="mathcode" src="https://private.codecogs.com/gif.latex?d_%7Bp%7D">&nbsp;维向量，&nbsp;<img alt="p_{t}^{i,j}" class="mathcode" src="https://private.codecogs.com/gif.latex?p_%7Bt%7D%5E%7Bi%2Cj%7D">&nbsp;表示&nbsp;<img alt="x_{t}" class="mathcode" src="https://private.codecogs.com/gif.latex?x_%7Bt%7D">&nbsp;对应于实体对&nbsp;<img alt="\left ( v_{i},v_{j} \right )" class="mathcode" src="https://private.codecogs.com/gif.latex?%5Cleft%20%28%20v_%7Bi%7D%2Cv_%7Bj%7D%20%5Cright%20%29">&nbsp;的位置嵌入。</p><h2 style="text-indent:0px;"><a name="t10"></a><a name="t10"></a>4.2&nbsp; Propagation Module</h2><p style="text-indent:0px;"><strong>The Initial Embeddings of Nodes</strong>&nbsp; 提取实体 <img alt="\nu _{i}" class="mathcode" src="https://private.codecogs.com/gif.latex?%5Cnu%20_%7Bi%7D">&nbsp;和实体 <img alt="\nu _{j}" class="mathcode" src="https://private.codecogs.com/gif.latex?%5Cnu%20_%7Bj%7D">&nbsp;之间的关系，它们的初始嵌入记为&nbsp;<img alt="h_{\nu _{i}}^{\left ( 0 \right )}= a_{subject},h_{\nu j}^{\left ( 0 \right )}= a_{object}" class="mathcode" src="https://private.codecogs.com/gif.latex?h_%7B%5Cnu%20_%7Bi%7D%7D%5E%7B%5Cleft%20%28%200%20%5Cright%20%29%7D%3D%20a_%7Bsubject%7D%2Ch_%7B%5Cnu%20j%7D%5E%7B%5Cleft%20%28%200%20%5Cright%20%29%7D%3D%20a_%7Bobject%7D">，而其他实体的初始嵌入全部设置为零。为头、尾实体的初始嵌入设置特殊值作为一种“flag”消息，并通过传播模块来传递这些消息。 <img alt="a_{subject},a_{object}" class="mathcode" src="https://private.codecogs.com/gif.latex?a_%7Bsubject%7D%2Ca_%7Bobject%7D">&nbsp;也可以携带有关subject和object实体的先验知识，实验中设置&nbsp;<img alt="a_{subject}= \left [ 1;0 \right ]^{T},a_{object}=\left [ 0;1 \right ]^{T}" class="mathcode" src="https://private.codecogs.com/gif.latex?a_%7Bsubject%7D%3D%20%5Cleft%20%5B%201%3B0%20%5Cright%20%5D%5E%7BT%7D%2Ca_%7Bobject%7D%3D%5Cleft%20%5B%200%3B1%20%5Cright%20%5D%5E%7BT%7D">。（门控神经网络思想）</p><p style="text-indent:0;"><strong>Numbers of Layers</strong>&nbsp;&nbsp;在一般图中，层数K选择为图直径（<em>怎么判断图直径</em>）的数量级，以便所有节点都从整个图中获取信息。 但是由于本文中图紧密连接，层数可理解为赋予模型更多的表达能力。</p><h2 style="text-indent:0px;"><a name="t11"></a><a name="t11"></a>4.3&nbsp; Classification Module</h2><p style="text-indent:33px;">输出模块将目标实体对&nbsp;<img alt="\left ( v_{i},v_{j} \right )" class="mathcode" src="https://private.codecogs.com/gif.latex?%5Cleft%20%28%20v_%7Bi%7D%2Cv_%7Bj%7D%20%5Cright%20%29">&nbsp;的嵌入作为输入，这些嵌入首先由（6）转换，其中&nbsp;<img alt="\odot" class="mathcode" src="https://private.codecogs.com/gif.latex?%5Codot">&nbsp;表示逐元素相乘。公式（7）用于分类，其中<img alt="r_{\nu _{i,\upsilon _{j}}}\in R" class="mathcode" src="https://private.codecogs.com/gif.latex?r_%7B%5Cnu%20_%7Bi%2C%5Cupsilon%20_%7Bj%7D%7D%7D%5Cin%20R">，MLP表示多层感知器模块，使用交叉熵（8）作为分类损失，其中<img alt="r_{\nu _{i,\upsilon _{j}}}" class="mathcode" src="https://private.codecogs.com/gif.latex?r_%7B%5Cnu%20_%7Bi%2C%5Cupsilon%20_%7Bj%7D%7D%7D">&nbsp;表示实体对&nbsp;<img alt="\left ( v_{i},v_{j} \right )" class="mathcode" src="https://private.codecogs.com/gif.latex?%5Cleft%20%28%20v_%7Bi%7D%2Cv_%7Bj%7D%20%5Cright%20%29">&nbsp;的关系标签，S表示整个语料库。 实验中将每个目标实体对的嵌入表示堆叠在一起，以推断每对实体之间的潜在关系。</p><p style="text-align:center;"><img alt="" class="has" height="58" src="https://img-blog.csdnimg.cn/2019110316372496.png" width="500"></p><p style="text-align:center;"><img alt="" class="has" height="55" src="https://img-blog.csdnimg.cn/20191103163736856.png" width="500"></p><p style="text-align:center;"><img alt="" class="has" height="72" src="https://img-blog.csdnimg.cn/20191103163746275.png" width="450"></p><h1 style="text-indent:0px;"><a name="t12"></a><a name="t12"></a>5&nbsp; Experiments</h1><p style="text-indent:33px;">实验主要目的是：（1）证明本文的最佳模型可以在各种设置下提高关系提取的表现；（2）说明层数如何影响模型的性能；（3）进行定性研究以突出本文模型与基准模型之间的差异。 在第（1）部分和第（2）部分中，进行了三个子实验：（i）将首先证明本文模型可以提高人工标注测试集上的实例级关系提取，（ii）然后证明所提模型可以帮助提高在远距离标记的测试集上的袋级关系提取的性能，并且（iii）还拆分了远距离标记的测试集的子集，其中实体和边的数量很大。</p><h2 style="text-indent:0px;"><a name="t13"></a><a name="t13"></a>5.1&nbsp; Experiments</h2><h3><a name="t14"></a><a name="t14"></a>5.1.1&nbsp; Datasets</h3><p><strong>Distantly labeled set&nbsp;&nbsp;</strong>Sorokin和Gurevych用Wikipedia语料库提出了一个数据集，本文任务与其任务之间有一个小区别：本文任务是<strong>提取句子中每对实体之间的关系</strong>，而他们的任务是提取给定实体对与上下文实体对之间的关系。 因此需要修改其数据集：1）如果给定三元组中缺少<span style="color:#f33b45;">反向边？？</span>，例如如果句子中存在三元组（Earth, partof, SolarSystem），则向其添加一个反向标签（Solar System, partof, Earth）<span style="color:#f33b45;">如何添加</span>；2）对于所有没有关系的实体对，在它们之间添加了“NA”标签，对所有实验都使用相同的训练集。</p><p><strong>Human annotated test set&nbsp;</strong> 根据Sorokin and Gurevych提供的测试集，需要5个annotator标记数据集，被用来决定是否对每一对实体都使用远程监督，只有所有5个annotator都接受的实例才合并到人工标注的测试集中，最终测试集中有350个句子和1,230个三元组。<br><strong>Dense distantly labeled test set</strong>&nbsp; 进一步从远距离标记的测试集中拆分出一个密集的测试集，标准是：实体数量应严格大于2；句子的真实标签中必须至少有一个圆（至少有三个实体）（圆的每个边都有一个非“NA”标签）。该测试集可用于测试论文方法在实体之间具有复杂相互作用的句子上的表现，该测试集中有1,350个句子、超过17,915个三元组和7,906个关系。</p><h3><a name="t15"></a><a name="t15"></a>5.1.2&nbsp; Models for Comparison</h3><h3><a name="t16"></a><a name="t16"></a>5.1.3&nbsp; Hyper-parameters</h3><p style="text-align:center;"><img alt="" class="has" height="298" src="https://img-blog.csdnimg.cn/20191104095705561.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwOTMxMTgx,size_16,color_FFFFFF,t_70" width="450"></p><h2><a name="t17"></a><a name="t17"></a>5.2&nbsp; Evaluation Details</h2><h2><a name="t18"></a><a name="t18"></a>5.3&nbsp; Effectiveness of Reasoning Mechanism</h2><p style="text-indent:33px;">从表2和表3中可以看出，本文最佳模型在所有三个测试集上的性能均明显优于所有基模型。这些结果表明GP-GNN模型可以使用自然语言生成的参数对完全连接的图成功进行推理。这些结果还表明本模型不仅在句子级关系提取上表现良好，而且在袋级关系提取上也有所改进。请注意，Context-Aware&nbsp;RE还结合了上下文信息来预测目标实体对的关系，但是Contect-Aware&nbsp;RE仅对各种关系的共现建模，而忽略了上下文关系是否参与了目标实体对的关系提取的推理的过程。Context-Aware RE可能会引入更多的噪音，因为它可能会错误地增加与具有上下文关系的相似主题的关系的可能性。另一个发现是，在这三个数据集中，GP-GNN＃layers = 1版本优于CNN和PCNN，一个可能的原因是，维基百科语料库中的句子很复杂，对于CNN和PCNN而言可能很难建模。 Zhang和Wang（2015）也得出了类似的结论。</p><p style="text-align:center;"><img alt="" class="has" height="271" src="https://img-blog.csdnimg.cn/20191104110908402.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwOTMxMTgx,size_16,color_FFFFFF,t_70" width="500"></p><p style="text-align:center;"><img alt="" class="has" height="273" src="https://img-blog.csdnimg.cn/20191104110939624.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwOTMxMTgx,size_16,color_FFFFFF,t_70" width="1000"></p><h2 style="text-indent:0px;"><a name="t19"></a><a name="t19"></a>5.4&nbsp; The Effectiveness of the Number of Layers</h2><p style="text-indent:33px;">层数表示模型的推理能力，K层模型具有推断K跳关系的能力。为了证明层数的影响，比较了具有不同层数的模型。从表2和表3中可以看到，在所有三个数据集上，三层模型均达到最佳。从图3中还可以看到，随着层数的增加，曲线变得越来越精确，这表明在推理中考虑更多hops会导致更好的表现。但是在整个远程监督测试集上，第三层的提升要比在密集子集上的提升小得多。这种观测表明推理机制可以帮助识别关系，尤其是在存在更多实体的句子上。还可以看到，在带有人工标注的测试集上，3层模型比2层模型相比2层模型比1层模型有更大的提升，可能是由于袋级关系提取更加容易的原因。在实际应用中，可以为不同类型的句子选择不同的变量，或也可以将来自不同模型的预测整合在一起。</p><p style="text-align:center;"><img alt="" class="has" height="783" src="https://img-blog.csdnimg.cn/20191104112450548.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwOTMxMTgx,size_16,color_FFFFFF,t_70" width="500"></p><h2 style="text-indent:0px;"><a name="t20"></a><a name="t20"></a>5.5&nbsp; Qualitative Results：Case Study</h2><p style="text-align:center;"><img alt="" class="has" height="744" src="https://img-blog.csdnimg.cn/20191104152810726.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwOTMxMTgx,size_16,color_FFFFFF,t_70" width="1200"></p><h1 style="text-indent:0px;"><a name="t21"></a><a name="t21"></a>6&nbsp; Conclusion and Future Work</h1><p style="text-indent:33px;">解决了利用GNN与自然语言进行关系推理的问题，提出的模型GP-GNNs<strong>通过将自然语言编码为参数并执行层与层之间的传播来解决关系消息传递任务</strong>。 新模型也可以被认为是解决非文本输入（例如文本，图像，视频，音频）的图生成问题的通用框架。在这项工作中，证明了其在预测自然语言和袋级实体之间的关系方面的有效性，并表明通过在推理中考虑更多跃点，关系提取的效果可以得到显着改善。</p><p style="text-indent:33px;">&nbsp;</p><p style="text-indent:33px;">&nbsp;</p>]]></content>
      
      
      <categories>
          
          <category> 笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文记录 </tag>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《GraphRel：Modeling Text as Relational Graphs for Joint Entity and Relation Extraction》阅读记录</title>
      <link href="/2020/03/04/article-1/"/>
      <url>/2020/03/04/article-1/</url>
      
        <content type="html"><![CDATA[<p>本文提出了一种端到端的关系抽取模型GraphRel，它使用GCN来共同学习命名实体和关系。与之前的基线相比，我们通过关系加权GCN来考虑命名实体和关系之间的交互，以更好地提取关系。线性结构和依赖结构都用于提取文本的序列和区域特征，并且完整的单词图还用于提取文本中所有单词对的隐含特征。使用基于图的方法，重叠关系的预测比先前的序列方法有了很大的改进。我们在两个公共数据集上评估GraphRel。结果表明，GraphRel在保持高精度的同时，显著提高了查全率。另外，GraphRel的性能比以前的工作高出3.2%和5.8%(F1 Score)，实现了关系抽取的新SOTA。</p><a id="more"></a><h1 id="1%20Introduction"><a name="t2"></a><a name="t2"></a>1 Introduction</h1><p style="text-indent:33px;">提取具有语义关系的实体对，即三元组（如BarackObama，PresidentOf，UnitedStates）是信息提取的中心任务，需要从非结构化文本自动构建知识。 三个关键方面仍需要在统一框架中进行全面处理：</p><ul><li>实体识别和关系提取的端到端联合建模；</li><li>对重叠关系的预测，即共同提及的关系；</li><li>考虑关系之间的相互作用，尤其是重叠关系。</li></ul><p style="text-indent:33px;">传统上，①pipeline使用命名实体识别器提取实体，然后预测每对提取的实体之间的关系；②建立了联合实体识别和关系抽取模型，以利用这两个任务之间的密切交互。联合建模这些复杂的方法是基于特征的结构化学习系统，严重依赖于特征工程。</p><p style="text-indent:33px;">随着深度神经网络的成功，基于神经网络的自动特征学习方法已被应用到关系提取中。 这些方法在两个提及实体之间的单词序列上 ①使用CNN，LSTM或Tree-LSTM，②两个实体之间的最短依赖路径或 ③跨越两个实体的minimal constituency sub-tree 来编码每对实体的相关信息 。 但这些方法不是实体和关系的端到端联合建模。假设给出了实体，并且在实际使用中需要命名实体识别器时，预期其性能会显着下降。</p><p style="text-indent:33px;">关系提取的另一个挑战是如何考虑关系之间的相互作用，这对于重叠关系——即共享共同实体的关系特别重要。例如，①EntityPairOverlap，从（BarackObama, Governance, UnitedStates）推断出（BarackObama, PresidentOf, UnitedStates）。另一个案例是，从（BarackObama, LiveIn, WhiteHouse）和（WhiteHouse, PresidentialPalace, UnitedStates）推断出刚才的三元组，其中后两个三元组被认为具有②SingleEntityOverlap。尽管这种交互作用在知识库完成中很常见，但无论是通过直接推理还是通过间接证明进行这种交互，对于输入中不存在实体的联合实体识别和关系提取模型而言尤其困难。尽管Zheng等人提出了一种基于LSTM序列标签器的强大的实体与关系的神经端到端联合模型，但他们必须完全放弃重叠关系。</p><p style="text-indent:33px;">在本文中，我们提出了GraphRel，这是一种用于实体识别和关系提取的神经端到端联合模型，它是第一个处理关系提取中所有三个关键部分的模型。 GraphRel通过堆叠Bi-LSTM句子编码器和GCN依赖树编码器来学习自动提取每个单词的隐藏特征。 然后GraphRel标记实体提及单词并预测连接提及的关系三元组，这是第一阶段预测。</p><p style="text-indent:33px;">为了在考虑到三元组之间的相互作用的情况下进行预测，我们在GraphRel第二阶段添加了一个新颖的关系加权GCN。 第一阶段GraphRel接收到实体损失和关系损失，沿着依赖关系链接提取节点隐藏特征，同时建立具有关系加权边的新全连接图。 然后，通过对<strong>中间图</strong>进行操作，第二阶段GCN在最终分类每个边之前有效地考虑实体之间的相互作用以及（可能重叠的）关系。</p><p><strong>GraphRel：</strong></p><ul><li>考虑了线性和依赖结构，以及文本中所有单词对之间的隐含特征；</li><li>对实体和关系进行端到端的联合建模，同时考虑所有单词对进行预测；</li><li>仔细考虑实体与关系之间的相互作用。</li></ul><p style="text-indent:33px;">在两个公共关系提取数据集上评估该方法：NYT和WebNLG。 实验结果表明，GraphRel与以前的工作相比大大改善了重叠关系，并且在两个数据集上都达到了SOTA。</p><h1 id="2%20Related%20Work"><a name="t3"></a><a name="t3"></a>2 Related Work</h1><p style="text-indent:33px;">模型的BiLSTM-GCN编码器部分类似于Miwa和Bansal（2016）提出的BiLSTM-TreeLSTM模型，因为它们还在序列顶部堆叠了依赖树，以共同对实体和关系进行建模。BiLSTM-TreeLSTM模型在每个句子上使用Bi-LSTM进行自动特征学习，并且所提取的隐藏特征由顺序实体tagger和最短依赖路径关系classifier共享。 但是，在引入共享参数以进行联合实体识别和关系提取时，它们仍必须通过pipeline传递tagger预测的实体提及，以形成关系分类的实体对。</p><p style="text-indent:33px;">Zheng等人（2018）没有像以前的工作那样尝试对每个提及对进行分类，而是将关系提取构造为与实体识别一样的序列标记问题。 这可以在Bi-LSTM编码器的顶部通过LSTM解码器对关系提取进行建模。 但是，尽管在NYT数据集上显示出令人鼓舞的结果，但它们的优势却来自于关注孤立的关系并完全放弃了重叠关系。 相比之下，<strong>GraphRel在端到端并共同建模实体识别的同时，可以处理所有类型的关系</strong>。</p><p style="text-indent:33px;">Zeng等人随后提出了一种用于关系提取的端对端序列到序列模型。 它们通过Bi-LSTM编码每个句子，并使用最后的编码器隐藏状态初始化一个（OneDecoder）或多个（MultiDecoder）LSTM，以动态解码关系三元组。 解码时，通过选择一个关系并从句子中复制两个单词来生成三元组。seq2seq设置部分处理三元组之间的交互。然而，在生成新三元组时，通过考虑先前生成的具有强制线性顺序的三元组，只能单向捕获关系之间的相互作用。相反，在本文中，<strong>通过在LSTM-GCN编码器上应用第二阶段GCN，提出在具有自动学习链接的字图上传播实体和关系信息。</strong></p><p style="text-indent:33px;">最近，在许多自然语言处理（NLP）任务中都使用了GCN的依赖关系结构。 Marcheggiani 和 Titov 将GCN应用于单词序列以进行语义角色标记。 Liu等人通过GCN对长文档进行编码以执行文本匹配。 Cetoli等人结合RNN和GCN来识别命名实体。 关于考虑单词序列的依存关系进行关系提取的工作也有一些工作。 在提出的GrpahRel中，<strong>不仅堆叠Bi-LSTM和GCN来考虑线性结构和依赖结构，而且采用第二阶段关系加权GCN来进一步建模实体与关系之间的相互作用。</strong></p><h1 id="3%20Review%20of%20GCN"><a name="t4"></a><a name="t4"></a>3 Review of GCN</h1><p style="text-indent:33px;">作为卷积神经网络（CNN），图卷积网络（GCN）卷积相邻节点的特征，并将节点的信息传播到其最近的邻居。如图1所示，通过堆叠GCN层，GCN可以提取每个节点的区域特征。</p><p style="text-align:center;"><img alt="" class="has" height="226" src="https://img-blog.csdnimg.cn/20191022211619599.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwOTMxMTgx,size_16,color_FFFFFF,t_70" width="400"></p><p>GCN层通过使用以下等式考虑相邻节点的特征来检索新的节点特征：</p><p style="text-align:center;"><img alt="" class="has" height="120" src="https://img-blog.csdnimg.cn/2019102221192439.png" width="450"></p><p>其中&nbsp;<img alt="\mu" src="https://private.codecogs.com/gif.latex?%5Cmu">&nbsp;是目标节点，<img alt="N(u)" class="mathcode" src="https://private.codecogs.com/gif.latex?N%28u%29">&nbsp;表示&nbsp;<img alt="\mu" src="https://private.codecogs.com/gif.latex?%5Cmu">&nbsp;的邻域，包括&nbsp;<img alt="\mu" src="https://private.codecogs.com/gif.latex?%5Cmu">&nbsp;本身； <img alt="h_{v}^{l}" class="mathcode" src="https://private.codecogs.com/gif.latex?h_%7Bv%7D%5E%7Bl%7D">&nbsp;表示节点&nbsp;<img alt="v" class="mathcode" src="https://private.codecogs.com/gif.latex?v"> 在第&nbsp;<img alt="l" class="mathcode" src="https://private.codecogs.com/gif.latex?l">&nbsp;层的隐藏特征； &nbsp;<img alt="W" class="mathcode" src="https://private.codecogs.com/gif.latex?W">&nbsp;和&nbsp;<img alt="b" class="mathcode" src="https://private.codecogs.com/gif.latex?b">&nbsp;是可学习的权重，将节点的特征映射到图中的相邻节点；<img alt="h\in \mathbb{R}^{f}, W\in \mathbb{R}^{f\times f}, b\in \mathbb{R}^{f}" class="mathcode" src="https://private.codecogs.com/gif.latex?h%5Cin%20%5Cmathbb%7BR%7D%5E%7Bf%7D%2C%20W%5Cin%20%5Cmathbb%7BR%7D%5E%7Bf%5Ctimes%20f%7D%2C%20b%5Cin%20%5Cmathbb%7BR%7D%5E%7Bf%7D"> ,其中&nbsp;<img alt="f" class="mathcode" src="https://private.codecogs.com/gif.latex?f">&nbsp;是特征尺寸。</p><h1 id="4%20Methodology"><a name="t5"></a><a name="t5"></a>4 Methodology</h1><p style="text-indent:33px;">所提出的 GraphRel 包含2个阶段的预测的总体架构图下图所示。<strong>第1阶段</strong>，采用bi-RNN和GCN来提取顺序和位置依赖词特征。 给定单词特征，预测每个单词对与所有单词的实体之间的关系。<strong>第2阶段</strong>，为第一阶段预测的每个关系建立完整的关系图，并在每个图上应用GCN以整合每个关系的信息，并进一步考虑实体与关系之间的相互作用。</p><p><img alt="" class="has" height="535" src="https://img-blog.csdnimg.cn/20191023092533484.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwOTMxMTgx,size_16,color_FFFFFF,t_70" width="1200"></p><h2 id="4.1%C2%A0%201st-phase%20Prediction"><a name="t6"></a><a name="t6"></a>4.1&nbsp; 1st-phase Prediction</h2><p style="text-indent:33px;">为充分考虑文本特征的顺序和位置依赖性，首先应用双向RNN提取顺序特征，然后使用双向GCN进一步提取位置依赖性特征。 然后，基于提取的单词特征，预测每个单词对与单词实体之间的关系。</p><h3 id="4.1.1%20Bi-LSTM"><a name="t7"></a><a name="t7"></a>4.1.1 Bi-LSTM</h3><p style="text-indent:33px;">使用LSTM作为双RNN单元，&nbsp;对于每个单词，将单词嵌入和词性（POS）嵌入组合为初始特征，其中&nbsp;<img alt="h_{u}^{0}" class="mathcode" src="https://private.codecogs.com/gif.latex?h_%7Bu%7D%5E%7B0%7D">&nbsp;代表单词&nbsp;<img alt="u" class="mathcode" src="https://private.codecogs.com/gif.latex?u">&nbsp;的初始特征，而&nbsp;<img alt="Word(u)" class="mathcode" src="https://private.codecogs.com/gif.latex?Word%28u%29">&nbsp;和&nbsp;<img alt="Pos(u)" class="mathcode" src="https://private.codecogs.com/gif.latex?Pos%28u%29">&nbsp;分别是单词&nbsp;<img alt="u" src="https://private.codecogs.com/gif.latex?u">&nbsp;的单词嵌入和POS嵌入。 使用GloVe的预训练词嵌入，并随机初始化POS嵌入以使用整个GraphRel 进行训练。</p><p style="text-align:center;"><img alt="" class="has" height="87" src="https://img-blog.csdnimg.cn/20191023094653902.png" width="500"></p><h3 id="4.1.2%20Bi-GCN"><a name="t8"></a><a name="t8"></a>4.1.2 Bi-GCN</h3><p style="text-indent:33px;">由于原始输入语句是一个序列并且没有固有的图结构可言，因此使用denpendency parse为输入语句创建一个依赖树。 将依赖树用作输入句子的邻接矩阵，并使用GCN提取位置依赖特征。原始GCN是为无向图设计的。，为了同时考虑单词的输入输出特征，将Bi-GCN规定为如下表达式：</p><p style="text-align:center;"><img alt="" class="has" height="321" src="https://img-blog.csdnimg.cn/20191023110123301.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwOTMxMTgx,size_16,color_FFFFFF,t_70" width="500"></p><p>其中&nbsp;<img alt="h_{u}^{l}" class="mathcode" src="https://private.codecogs.com/gif.latex?h_%7Bu%7D%5E%7Bl%7D">&nbsp;代表单词&nbsp;<img alt="u" class="mathcode" src="https://private.codecogs.com/gif.latex?u">&nbsp;在&nbsp;<img alt="l" src="https://private.codecogs.com/gif.latex?l">&nbsp;层中&nbsp;的隐藏特征，<img alt="\overrightarrow{N}(u)" class="mathcode" src="https://private.codecogs.com/gif.latex?%5Coverrightarrow%7BN%7D%28u%29">&nbsp;包括从单词&nbsp;<img alt="u" src="https://private.codecogs.com/gif.latex?u">&nbsp;输出的所有单词，<img alt="\overleftarrow{N}(u)" class="mathcode" src="https://private.codecogs.com/gif.latex?%5Coverleftarrow%7BN%7D%28u%29">&nbsp;包含从单词&nbsp;<img alt="u" src="https://private.codecogs.com/gif.latex?u">&nbsp;输入的所有单词，都包括单词&nbsp;<img alt="u" src="https://private.codecogs.com/gif.latex?u">&nbsp;本身。 <img alt="W" class="mathcode" src="https://private.codecogs.com/gif.latex?W">&nbsp;和 <img alt="b" class="mathcode" src="https://private.codecogs.com/gif.latex?b">&nbsp;都是可学习的卷积权重。<img alt="\overrightarrow{W}, \overrightarrow{b}, \overleftarrow{W},\overleftarrow{b}" class="mathcode" src="https://private.codecogs.com/gif.latex?%5Coverrightarrow%7BW%7D%2C%20%5Coverrightarrow%7Bb%7D%2C%20%5Coverleftarrow%7BW%7D%2C%5Coverleftarrow%7Bb%7D">&nbsp;分别代表输出权重和输入权重，输出和输入单词特征连接为最终单词特征。</p><h3 id="4.1.3%20Extraction%20of%20Entities%20and%20Relations"><a name="t9"></a><a name="t9"></a>4.1.3 Extraction of Entities and Relations</h3><p style="text-indent:33px;">利用从 Bi-RNN 和 Bi-GCN 提取的单词特征，预测单词实体并提取每个单词对的关系。 对于单词实体，根据1-layer LSTM上的单词特征预测所有单词，并应用分类损失（称为&nbsp;<img alt="eloss_{1p}" class="mathcode" src="https://private.codecogs.com/gif.latex?eloss_%7B1p%7D">）来训练它们。 <strong>对于关系提取，删除依赖边并对所有单词对进行预测</strong>。 对于每个关系r，学习权重矩阵&nbsp;<img alt="W_{r}^{1}, W_{r}^{2}, W_{r}^{3}" class="mathcode" src="https://private.codecogs.com/gif.latex?W_%7Br%7D%5E%7B1%7D%2C%20W_%7Br%7D%5E%7B2%7D%2C%20W_%7Br%7D%5E%7B3%7D">&nbsp;并计算关系趋势得分S为</p><p style="text-align:center;"><img alt="" class="has" height="79" src="https://img-blog.csdnimg.cn/20191023112951951.png" width="500"></p><p>其中&nbsp;<img alt="S_{(\omega 1, r, \omega 2)}" class="mathcode" src="https://private.codecogs.com/gif.latex?S_%7B%28%5Comega%201%2C%20r%2C%20%5Comega%202%29%7D">&nbsp;表示关系&nbsp;<img alt="r" class="mathcode" src="https://private.codecogs.com/gif.latex?r">&nbsp;下的&nbsp;<img alt="(\omega 1, \omega 2)" src="https://private.codecogs.com/gif.latex?%28%5Comega%201%2C%20%5Comega%202%29">&nbsp;的关系趋势得分，而&nbsp;<img alt="(\omega 1, \omega 2)" src="https://private.codecogs.com/gif.latex?%28%5Comega%201%2C%20%5Comega%202%29"> 表示单词对。 请注意，<img alt="S_{(\omega 1, r, \omega 2))}" src="https://private.codecogs.com/gif.latex?S_%7B%28%5Comega%201%2C%20r%2C%20%5Comega%202%29%29%7D">&nbsp;应该不同于&nbsp;<img alt="S_{(\omega 2, r, \omega 1))}" class="mathcode" src="https://private.codecogs.com/gif.latex?S_%7B%28%5Comega%202%2C%20r%2C%20%5Comega%201%29%29%7D">。 对于单词对&nbsp;<img alt="(\omega 1, \omega 2)" src="https://private.codecogs.com/gif.latex?%28%5Comega%201%2C%20%5Comega%202%29">&nbsp;，计算该单词对的所有相关趋势得分，包括非相关性，并将其表示为&nbsp;<img alt="S_{(\omega 1, null, \omega 2))}" class="mathcode" src="https://private.codecogs.com/gif.latex?S_%7B%28%5Comega%201%2C%20null%2C%20%5Comega%202%29%29%7D">&nbsp;。 将 softmax 函数应用于&nbsp;<img alt="S_{(\omega 1, r, \omega 2))}" src="https://private.codecogs.com/gif.latex?S_%7B%28%5Comega%201%2C%20r%2C%20%5Comega%202%29%29%7D">&nbsp;，得出<img alt="P_{r}_{(\omega 1, \omega 2))}" src="https://private.codecogs.com/gif.latex?P_%7Br%7D_%7B%28%5Comega%201%2C%20%5Comega%202%29%29%7D">&nbsp;，它表示每个关系&nbsp;<img alt="r" src="https://private.codecogs.com/gif.latex?r">&nbsp;对于&nbsp;<img alt="(\omega 1, \omega 2)" src="https://private.codecogs.com/gif.latex?%28%5Comega%201%2C%20%5Comega%202%29"> 的概率。</p><p style="text-indent:33px;">由于提取每个单词对的关系，因此设计中不包含三元组数量限制。 通过调查每个单词对的关系，GraphRel识别出尽可能多的关系。 利用<img alt="P_{r}_{(\omega 1, \omega 2))}" src="https://private.codecogs.com/gif.latex?P_%7Br%7D_%7B%28%5Comega%201%2C%20%5Comega%202%29%29%7D">，计算出分类损失的关系，记为&nbsp;<img alt="rloss_{1p}" class="mathcode" src="https://private.codecogs.com/gif.latex?rloss_%7B1p%7D">。 请注意，尽管&nbsp;<img alt="eloss_{1p}" src="https://private.codecogs.com/gif.latex?eloss_%7B1p%7D">&nbsp;和&nbsp;<img alt="rloss_{1p}" src="https://private.codecogs.com/gif.latex?rloss_%7B1p%7D">都不会被用作最终预测，但它们对于训练第一阶段GraphRel的有益辅助损失。</p><h2 id="4.2%C2%A0%202nd-phase%20Prediction"><a name="t10"></a><a name="t10"></a>4.2&nbsp; 2nd-phase Prediction</h2><p style="text-indent:33px;">第一阶段中没有考虑提取的实体和关系之间的联系，<strong>为了考虑命名实体和关系之间的影响，并考虑文本所有词对之间的隐含特征</strong>，本文在第二阶段提出了一种新的<strong>关系加权GCN</strong>进行进一步提取。</p><h3 id="4.2.1%C2%A0%20Relation-weighted%20Graph"><a name="t11"></a><a name="t11"></a>4.2.1&nbsp; Relation-weighted Graph</h3><p style="text-indent:33px;">经过第一阶段的预测，为每个关系&nbsp;<img alt="r" src="https://private.codecogs.com/gif.latex?r">&nbsp;构造了完整的关系加权图，其中&nbsp;<img alt="(\omega 1, \omega 2)" src="https://private.codecogs.com/gif.latex?%28%5Comega%201%2C%20%5Comega%202%29">&nbsp;的边权为&nbsp;<img alt="P_{r}_{(\omega 1, \omega 2)}" class="mathcode" src="https://private.codecogs.com/gif.latex?P_%7Br%7D_%7B%28%5Comega%201%2C%20%5Comega%202%29%7D">，如下图所示。</p><p style="text-align:center;"><img alt="" class="has" height="343" src="https://img-blog.csdnimg.cn/20191023193638111.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwOTMxMTgx,size_16,color_FFFFFF,t_70" width="500"></p><p>然后，第二阶段在每个关系图上采用Bi-GCN，考虑不同关系的不同影响程度并聚合作为<strong>综合词特征</strong>。 该过程可以表示为</p><p style="text-align:center;"><img alt="" class="has" height="113" src="https://img-blog.csdnimg.cn/20191023194320558.png" width="450"></p><p>其中<img alt="P_{r}_{(u, v)}" class="mathcode" src="https://private.codecogs.com/gif.latex?P_%7Br%7D_%7B%28u%2C%20v%29%7D">&nbsp;表示边权重（关系 <img alt="r" class="mathcode" src="https://private.codecogs.com/gif.latex?r">&nbsp;下单词&nbsp;<img alt="u" class="mathcode" src="https://private.codecogs.com/gif.latex?u">&nbsp;到单词 <img alt="v" class="mathcode" src="https://private.codecogs.com/gif.latex?v">&nbsp;的概率）。 <img alt="W_{r}" class="mathcode" src="https://private.codecogs.com/gif.latex?W_%7Br%7D">&nbsp;和 <img alt="b_{r}" class="mathcode" src="https://private.codecogs.com/gif.latex?b_%7Br%7D">&nbsp;表示关系 <img alt="r" class="mathcode" src="https://private.codecogs.com/gif.latex?r">&nbsp;下的GCN权重。 <img alt="V" class="mathcode" src="https://private.codecogs.com/gif.latex?V">&nbsp;包含所有单词，<img alt="R" class="mathcode" src="https://private.codecogs.com/gif.latex?R">&nbsp;包含所有关系。 请注意，完整的Bi-GCN还将输入和输出情况都考虑在内。 第二阶段的Bi-GCN进一步考虑了关系加权传播，并为每个单词提取了更充分的特征。用来自二阶段新提取的单词特征，再次执行命名实体和关系分类以实现更可靠的关系预测，损失为&nbsp;<img alt="eloss_{2p}" class="mathcode" src="https://private.codecogs.com/gif.latex?eloss_%7B2p%7D">&nbsp;和&nbsp;<img alt="rloss_{2p}" class="mathcode" src="https://private.codecogs.com/gif.latex?rloss_%7B2p%7D">。</p><h2 id="4.3%C2%A0%20Training%20Detail"><a name="t12"></a><a name="t12"></a>4.3&nbsp; Training Detail</h2><p style="text-indent:33px;">在GraphRel中使用两种损失：<strong>实体损失和关系损失</strong>，两者都属于类别损失，在训练过程中使用<strong>交叉熵</strong>作为分类损失函数。 实体损失使用BIESO方法来标注真实标签，&nbsp;<img alt="eloss_{1p}" class="mathcode" src="https://private.codecogs.com/gif.latex?eloss_%7B1p%7D">&nbsp;和&nbsp;<img alt="eloss_{2p}" class="mathcode" src="https://private.codecogs.com/gif.latex?eloss_%7B2p%7D">&nbsp;的真实实体标签相同。关系损失为每个单词对&nbsp;<img alt="(\omega 1, \omega 2)" src="https://private.codecogs.com/gif.latex?%28%5Comega%201%2C%20%5Comega%202%29">&nbsp;输入一个one-hot编码的关系向量作为&nbsp;<img alt="P_{r}_{(\omega 1, \omega 2)}" class="mathcode" src="https://private.codecogs.com/gif.latex?P_%7Br%7D_%7B%28%5Comega%201%2C%20%5Comega%202%29%7D">&nbsp;的真实数据。 由于基于单词对预测关系，真实数据同样应基于单词对。 也就是说，单词United对于单词Barack和单词Obama都有HasPresident的关系，单词States也是如此。基于单词对的关系表示为GraphRel提供了学习提取关系所需的信息，<img alt="rloss_{1p}" class="mathcode" src="https://private.codecogs.com/gif.latex?rloss_%7B1p%7D">&nbsp;和&nbsp;<img alt="rloss_{2p}" class="mathcode" src="https://private.codecogs.com/gif.latex?rloss_%7B2p%7D">&nbsp;的真实数据关系向量相同。</p><p style="text-indent:33px;">对于eloss和rloss，为类内实体或关系项添加额外的double-weight。总损失为所有实体损失和关系损失的总和：其中 <img alt="\alpha" class="mathcode" src="https://private.codecogs.com/gif.latex?%5Calpha">&nbsp;是第一阶段和第二阶段的损失之间的权重。将 <img alt="loss_{all}" class="mathcode" src="https://private.codecogs.com/gif.latex?loss_%7Ball%7D">&nbsp;降到最低，并以端到端的方式训练整个GraphRel。</p><p style="text-align:center;"><img alt="" class="has" height="70" src="https://img-blog.csdnimg.cn/20191023202501130.png" width="550"></p><h2 id="4.4%C2%A0%20Inference"><a name="t13"></a><a name="t13"></a>4.4&nbsp; Inference</h2><p style="text-indent:33px;">beseline预测方法是head prediction（没明白怎么理解），当且仅当 BarackObama, UnitedStates 都被识别为实体且 PresidentOf 为<img alt="P_{(Obama, States)}" class="mathcode" src="https://private.codecogs.com/gif.latex?P_%7B%28Obama%2C%20States%29%7D">&nbsp;的最大概率时，才会提取诸如（BarackObama，PresidentOf，United States）的关系三元组。另一种可能更稳定的提取方法是平均预测，其中考虑实体提及对之间的所有单词对，选择具有最大平均概率的关系。第三种是阈值预测方法，其中实体对的所有单词对仍会考虑在内，但以独立的方式进行。例如，如果4个分布中的2个具有 PresidentOf 作为最可能的类别，则仅当2/4 = 50％&gt;θ 时才提取三元组（BarackObama，PresidentOf，UnitedStates），其中 θ 是自由阈值参数。这样，用户可以选择自己喜欢的精度并通过调整 θ 进行权衡，如果未指定 θ=0 。</p><h1 id="5%C2%A0%20Experiments" style="text-indent:0px;"><a name="t14"></a><a name="t14"></a>5&nbsp; Experiments</h1><p style="text-indent:33px;">这一部分给出了GraphRel的实验结果。首先是实施细节、数据集和比较的baseline，然后展示两个数据集的定量结果，进行了详细的分析，并对不同类别的命名实体进行了分类，最后通过一个案例说明第二阶段的改进效果。</p><h2 id="5.1%20Experimental%20Settings" style="text-indent:0px;"><a name="t15"></a><a name="t15"></a>5.1 Experimental Settings</h2><p style="text-indent:33px;">实现中，选择预训练的GloVe（300d）作为固定的单词嵌入，然后将单词嵌入与可训练的POS嵌入（15d）连接起来，作为每个单词的最终输入嵌入，从spaCy检索每个单词的POS标签和整个句子的依存关系树。在第一阶段使用具有256个单位的Bi-LSTM和具有256个特征的2层Bi-GCN。 对于第二阶段，关系加权的Bi-GCN为1层，特征尺寸为256。在训练期间，将LSTM辍学率设置为0.5，学习率设置为0.0008，损失权重α设置为 3，使用Adam优化器训练GraphRel，并在PyTorch下实现它。</p><h3 id="5.1.1%C2%A0%20Datasets" style="text-indent:0px;"><a name="t16"></a><a name="t16"></a>5.1.1&nbsp; Datasets</h3><p style="text-indent:33px;">使用NYT和WebNLG数据集来评估所提出的方法。 正如 NovelTagging 和 MultiDecoder 一样，对于NYT过滤的句子超过100个单词；对于WebNLG，在实验中的每个实例中仅使用第一个句子。 表2中描述了NYT和WebNLG的统计信息，将关系三元组分为三类：正常，EntityPairOverlap（EPO）和SingleEntityOverlap（SEO），&nbsp;表2还显示了每个类别的总数。由于一个实体属于几个不同的关系，因此EntityPairOverlap和SingleEntityOverlap难度更大。 会在详细分析中讨论不同类别的结果。</p><h2 id="5.2%C2%A0%20Baseline%20and%20Evaluation%20Metrics" style="text-indent:0px;"><a name="t17"></a><a name="t17"></a>5.2&nbsp; Baseline and Evaluation Metrics</h2><p style="text-indent:33px;">将GraphRel与两个baseline进行了比较：NovelTagging和MultiDecoder。 NovelTagging是一种序列标记器，可预测每个句子单词的实体和关系；MultiDecoder是一种最新的方法，将关系提取视为seq-seq问题，并使用动态解码器提取关系三元组。 这两个baseline的结果直接来自原始论文。 作为两个baselien，采用了标准的F1分数来评估结果。 当且仅当两个对应实体的关系和首部与真实数据相同时，才能将预测的三元组视为正确的。</p><p style="text-align:center;"><img alt="" class="has" height="307" src="https://img-blog.csdnimg.cn/20191023215945553.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwOTMxMTgx,size_16,color_FFFFFF,t_70" width="500"></p><h2 id="5.3%C2%A0%20Quantitative%20Results" style="text-indent:0px;"><a name="t18"></a><a name="t18"></a>5.3&nbsp; Quantitative Results</h2><p style="text-indent:33px;">表1列出了NYT和WebNLG数据集的NovelTagging，MultiDecoder和GraphRel的精确率，召回率和F1得分。 MultiDecoder的原始论文中提出的OneDecoder仅使用单个解码器来提取关系三元组。 <img alt="GraphRel_{1p}" class="mathcode" src="https://private.codecogs.com/gif.latex?GraphRel_%7B1p%7D">&nbsp;是提出的方法，但仅在第一阶段，而GraphRel2p是完整版本，可预测第二阶段之后的关系和实体。 对于NYT数据集，<img alt="GraphRel_{1-hop}" class="mathcode" src="https://private.codecogs.com/gif.latex?GraphRel_%7B1-hop%7D">&nbsp;在F1方面的表现优于NovelTagging 18.0％，OneDecoder 4.0％和MultiDecoder 1.3％。 由于&nbsp;<img alt="GraphRel_{1-hop}" class="mathcode" src="https://private.codecogs.com/gif.latex?GraphRel_%7B1-hop%7D">&nbsp;同时具有顺序和位置依赖词特征，因此它在精确率和查全率方面均表现更好，因此F1得分更高。 与第二阶段的关系加权GCN相比，考虑名称实体与关系之间相互作用的&nbsp;<img alt="GraphRel_{2p}" class="mathcode" src="https://private.codecogs.com/gif.latex?GraphRel_%7B2p%7D">&nbsp;与&nbsp;<img alt="GraphRel_{1p}" class="mathcode" src="https://private.codecogs.com/gif.latex?GraphRel_%7B1p%7D">&nbsp;相比，进一步超过了MultiDecoder 3.2％，并且改进了1.9％。</p><p style="text-align:center;"><img alt="" class="has" height="338" src="https://img-blog.csdnimg.cn/20191023215923980.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwOTMxMTgx,size_16,color_FFFFFF,t_70" width="1000"></p><p style="text-indent:33px;">在WebNLG数据集上可以找到类似的结果：GraphRel1p的基准F1 Score优于MultiDecoder 3.6％，而GraphRel2p 比 GraphRel1p进一步提高了2.2％。 根据NYT和WebNLG的结果，GCN的位置依赖关系特征和第二阶段预测都可以在精确率，召回率和F1 Score方面帮助进行关系预测。 NovelTagging和MultiDecoder都使用顺序体系结构。 由于NovelTagging假定实体属于单一关系，因此精度很高，但召回率很低。 MultiDecoder使用动态解码器生成关系三元组。 由于RNN结构的先天限制（后信息需要前信息发生），它可以生成的三元组的数量受到限制。 但是GraphRel在预测每个单词对的关系时不受此限制。GraphRel是最平衡的方法，同时保持了高精确率和高召回率，并有更高的F1 Score。</p><h2 id="5.4%C2%A0%20Detailed%20Analysis"><a name="t19"></a><a name="t19"></a>5.4&nbsp; Detailed Analysis</h2><p style="text-indent:33px;">为了进一步分析提出的GraphRel，给出在不同类型的三元组、不同的推理方法、改进的命名实体识别以及使用不同GCN层数的情况下的分析结果。</p><h3 id="5.4.1%C2%A0%20Different%20Types%20of%20Triplets" style="text-indent:0px;"><a name="t20"></a><a name="t20"></a>5.4.1&nbsp; Different Types of Triplets</h3><p style="text-indent:33px;">首先调查不同实体类别下的结果。图4给出了NYT和WebNLG数据集的结果。对于GraphRel，当预测所有单词对的关系时，所有单词都可以与其他单词具有关系：因此<strong>实体重叠</strong>不是问题。尽管MultiDecoder尝试使用动态解码器，但结果表明GraphRel在所有实体类别中都超过了它们。例如，在WebNLG数据集上，GraphRel1p在MultiClass上的表现优于Normal3.5％，EPO类2.9％和SEO类3.4％。 GraphRel2p进一步改进了每个类的GraphRel1p。还比较了给定的句子中不同数量的三元组的结果，如图5所示。x轴表示句子中的1、2、3、4或5个以上的三元组。由于使用了单个解码器，所以OneDecoder对于单个三元组的性能很好，但是对于句子中更多的三元组，性能会急剧下降。与针对不同实体类别的实验一样，GraphRel1p和GraphRel2p在句子中的所有三元组数目下均优于baseline。 GraphRel1p在一个句子中多于5个三元组得分超过MultiDecoder7.5％，而GraphRel2p在NYT上进一步超过MultiDecoder 11.1％。</p><p><img alt="" class="has" height="512" src="https://img-blog.csdnimg.cn/2019102322341628.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwOTMxMTgx,size_16,color_FFFFFF,t_70" width="1200"></p><p><img alt="" class="has" height="503" src="https://img-blog.csdnimg.cn/20191023223442193.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwOTMxMTgx,size_16,color_FFFFFF,t_70" width="1200"></p><h3 id="5.4.2%C2%A0%20Inference%20Methods"><a name="t21"></a><a name="t21"></a>5.4.2&nbsp; Inference Methods</h3><p style="text-indent:33px;">比较两种baseline方法，即 head 和 average，以及在不同θ下的阈值法。 图6显示了将其应用于NYT和WebNLG上的GraphRel2p的结果。 可以看出，阈值法有效地调整了不同θ选择下精确率和召回率之间的平衡。 通过将阈值从 θ=0.8 降低到 θ=0，NYT和WebNLG的召回率分别显着提高了1.8％和1.4％，而精确率仅略微降低了0.6％。 阈值法的有效性致使在两个数据集上均达到最佳性能，超过了其他两种方法。</p><p style="text-align:center;"><img alt="" class="has" height="474" src="https://img-blog.csdnimg.cn/2019102409370847.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwOTMxMTgx,size_16,color_FFFFFF,t_70" width="700"></p><h3 style="text-indent:0px;"><a name="t22"></a><a name="t22"></a>5.4.3&nbsp; Improvement over Entity Recognition and Different Numbers of GCN Layer</h3><p style="text-indent:33px;">根据表4，对于NYT和WebNLG上的实体识别，GraphRel2p可以超过第一阶段0.4％和2.8％。 第二阶段的关系加权的GCN不仅对关系提取有效，而且对命名实体识别也有效。为了确认2层加1层的最佳设置，在第一阶段和第二阶段中使用不同数量的GCN层进行测试。表5给出了将3个GCN层用于第一阶段并将2个关系加权GCN层用于第二阶段的实验结果，表明更多的GCN层不能带来更好的预测，而（2,1）层应该是最适合关系提取任务的设置。&nbsp;</p><h1 style="text-indent:0px;"><a name="t23"></a><a name="t23"></a>5.5&nbsp; Case Study</h1><p style="text-indent:33px;">表3显示了GraphRel的案例研究。第一个句子很简单，并且GraphRel1p和GraphRel2p都可以准确提取。对于第二种情况，尽管不属于命名实体，但它应包含Italy的隐藏语义。 因此第二阶段可以进一步预测 <em>A.S. Gubbio 1910 grounds in Italy</em>。第三种情况是SEO类，其中GraphRel1p发现Asam pedas与Asam padeh相同，因此后者也应位于马来半岛并来自马来西亚。</p><p style="text-align:center;"><img alt="" class="has" height="427" src="https://img-blog.csdnimg.cn/20191024100130760.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwOTMxMTgx,size_16,color_FFFFFF,t_70" width="1200"></p><h1 style="text-indent:0px;"><a name="t24"></a><a name="t24"></a>6&nbsp; Conclusion</h1><p style="text-indent:33px;">本文中提出了GraphRel，这是一种基于图卷积网络（GCN）共同学习命名实体和关系的端到端关系提取模型。将RNN和GCN结合起来提取每个单词的顺序特征和位置依赖特征，也考虑了文本中所有单词对之间的隐含特征。 通过预测每个单词对的关系，解决实体重叠的问题。 此外，提出一种新颖的关系加权GCN，考虑了命名实体与关系之间的相互作用。在NYT和WebNLG数据集上评估提出的方法。 结果表明，此方法比以前的工作分别提高了3.2％和5.8％，并实现了关系提取的SOTA。</p><p>&nbsp;</p>]]></content>
      
      
      <categories>
          
          <category> 笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文记录 </tag>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
