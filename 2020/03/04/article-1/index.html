<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5"><title>《GraphRel：Modeling Text as Relational Graphs for Joint Entity and Relation Extraction》阅读记录 | 想吃煎饼果子</title><meta name="description" content="《GraphRel：Modeling Text as Relational Graphs for Joint Entity and Relation Extraction》阅读记录"><meta name="keywords" content="论文记录,NLP"><meta name="author" content="R"><meta name="copyright" content="R"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/img/favicon.ico"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="https://fonts.googleapis.com" crossorigin><link rel="preconnect" href="//busuanzi.ibruce.info"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="《GraphRel：Modeling Text as Relational Graphs for Joint Entity and Relation Extraction》阅读记录"><meta name="twitter:description" content="《GraphRel：Modeling Text as Relational Graphs for Joint Entity and Relation Extraction》阅读记录"><meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/jerryc127/butterfly_cdn@2.1.0/top_img/default.png"><meta property="og:type" content="article"><meta property="og:title" content="《GraphRel：Modeling Text as Relational Graphs for Joint Entity and Relation Extraction》阅读记录"><meta property="og:url" content="http://yoursite.com/2020/03/04/article-1/"><meta property="og:site_name" content="想吃煎饼果子"><meta property="og:description" content="《GraphRel：Modeling Text as Relational Graphs for Joint Entity and Relation Extraction》阅读记录"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/jerryc127/butterfly_cdn@2.1.0/top_img/default.png"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script src="https://cdn.jsdelivr.net/npm/js-cookie/dist/js.cookie.min.js"></script><script>const autoChangeMode = 'false'
var t = Cookies.get("theme");
if (autoChangeMode == '1'){
const isDarkMode = window.matchMedia("(prefers-color-scheme: dark)").matches
const isLightMode = window.matchMedia("(prefers-color-scheme: light)").matches
const isNotSpecified = window.matchMedia("(prefers-color-scheme: no-preference)").matches
const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

if (t === undefined){
  if (isLightMode) activateLightMode()
  else if (isDarkMode) activateDarkMode()
  else if (isNotSpecified || hasNoSupport){
    console.log('You specified no preference for a color scheme or your browser does not support it. I Schedule dark mode during night time.')
    now = new Date();
    hour = now.getHours();
    isNight = hour < 6 || hour >= 18
    isNight ? activateDarkMode() : activateLightMode()
}
} else if (t == 'light') activateLightMode()
else activateDarkMode()


} else if (autoChangeMode == '2'){
  now = new Date();
  hour = now.getHours();
  isNight = hour < 6 || hour >= 18
  if(t === undefined) isNight? activateDarkMode() : activateLightMode()
  else if (t === 'light') activateLightMode()
  else activateDarkMode() 
} else {
  if ( t == 'dark' ) activateDarkMode()
  else if ( t == 'light') activateLightMode()
}

function activateDarkMode(){
  document.documentElement.setAttribute('data-theme', 'dark')
  if (document.querySelector('meta[name="theme-color"]') !== null){
    document.querySelector('meta[name="theme-color"]').setAttribute('content','#000')
  }
}
function activateLightMode(){
  document.documentElement.setAttribute('data-theme', 'light')
  if (document.querySelector('meta[name="theme-color"]') !== null){
  document.querySelector('meta[name="theme-color"]').setAttribute('content','#fff')
  }
}</script><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><link rel="canonical" href="http://yoursite.com/2020/03/04/article-1/"><link rel="prev" title="《Graph Neural Networks with Generated Parameters for Relation Extraction》阅读笔记" href="http://yoursite.com/2020/03/04/article-2/"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"cookieDomain":"https://xxx/","msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简"},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  bookmark: {
    title: 'Snackbar.bookmark.title',
    message_prev: '按',
    message_next: '键将本页加入书签'
  },
  runtime_unit: '天',
  runtime: true,
  copyright: undefined,
  ClickShowText: undefined,
  medium_zoom: false,
  fancybox: true,
  Snackbar: undefined,
  baiduPush: false,
  isHome: false,
  isPost: true
  
}</script><meta name="generator" content="Hexo 4.2.0"></head><body><header> <div id="page-header"><span class="pull_left" id="blog_name"><a class="blog_title" id="site-name" href="/">想吃煎饼果子</a></span><span class="toggle-menu pull_right close"><a class="site-page"><i class="fa fa-bars fa-fw" aria-hidden="true"></i></a></span><span class="pull_right menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> About</span></a></div></div></span></div></header><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="/img/avatar.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">文章</div><div class="length_num">6</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">标签</div><div class="length_num">2</div></a></div></div><div class="mobile_data_item is-center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">分类</div><div class="length_num">1</div></a></div></div></div><hr><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> About</span></a></div></div></div><div id="mobile-sidebar-toc"><div class="toc_mobile_headline">目录</div><div class="sidebar-toc__content"><ol class="toc_mobile_items"><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#Abstract"><span class="toc_mobile_items-number">1.</span> <span class="toc_mobile_items-text">Abstract</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#1%20Introduction"><span class="toc_mobile_items-number">2.</span> <span class="toc_mobile_items-text">1 Introduction</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#2%20Related%20Work"><span class="toc_mobile_items-number">3.</span> <span class="toc_mobile_items-text">2 Related Work</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#3%20Review%20of%20GCN"><span class="toc_mobile_items-number">4.</span> <span class="toc_mobile_items-text">3 Review of GCN</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#4%20Methodology"><span class="toc_mobile_items-number">5.</span> <span class="toc_mobile_items-text">4 Methodology</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#4.1%C2%A0%201st-phase%20Prediction"><span class="toc_mobile_items-number">5.1.</span> <span class="toc_mobile_items-text">4.1&amp;nbsp; 1st-phase Prediction</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#4.1.1%20Bi-LSTM"><span class="toc_mobile_items-number">5.1.1.</span> <span class="toc_mobile_items-text">4.1.1 Bi-LSTM</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#4.1.2%20Bi-GCN"><span class="toc_mobile_items-number">5.1.2.</span> <span class="toc_mobile_items-text">4.1.2 Bi-GCN</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#4.1.3%20Extraction%20of%20Entities%20and%20Relations"><span class="toc_mobile_items-number">5.1.3.</span> <span class="toc_mobile_items-text">4.1.3 Extraction of Entities and Relations</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#4.2%C2%A0%202nd-phase%20Prediction"><span class="toc_mobile_items-number">5.2.</span> <span class="toc_mobile_items-text">4.2&amp;nbsp; 2nd-phase Prediction</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#4.2.1%C2%A0%20Relation-weighted%20Graph"><span class="toc_mobile_items-number">5.2.1.</span> <span class="toc_mobile_items-text">4.2.1&amp;nbsp; Relation-weighted Graph</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#4.3%C2%A0%20Training%20Detail"><span class="toc_mobile_items-number">5.3.</span> <span class="toc_mobile_items-text">4.3&amp;nbsp; Training Detail</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#4.4%C2%A0%20Inference"><span class="toc_mobile_items-number">5.4.</span> <span class="toc_mobile_items-text">4.4&amp;nbsp; Inference</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#5%C2%A0%20Experiments"><span class="toc_mobile_items-number">6.</span> <span class="toc_mobile_items-text">5&amp;nbsp; Experiments</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#5.1%20Experimental%20Settings"><span class="toc_mobile_items-number">6.1.</span> <span class="toc_mobile_items-text">5.1 Experimental Settings</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#5.1.1%C2%A0%20Datasets"><span class="toc_mobile_items-number">6.1.1.</span> <span class="toc_mobile_items-text">5.1.1&amp;nbsp; Datasets</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#5.2%C2%A0%20Baseline%20and%20Evaluation%20Metrics"><span class="toc_mobile_items-number">6.2.</span> <span class="toc_mobile_items-text">5.2&amp;nbsp; Baseline and Evaluation Metrics</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#5.3%C2%A0%20Quantitative%20Results"><span class="toc_mobile_items-number">6.3.</span> <span class="toc_mobile_items-text">5.3&amp;nbsp; Quantitative Results</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#5.4%C2%A0%20Detailed%20Analysis"><span class="toc_mobile_items-number">6.4.</span> <span class="toc_mobile_items-text">5.4&amp;nbsp; Detailed Analysis</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#5.4.1%C2%A0%20Different%20Types%20of%20Triplets"><span class="toc_mobile_items-number">6.4.1.</span> <span class="toc_mobile_items-text">5.4.1&amp;nbsp; Different Types of Triplets</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#5.4.2%C2%A0%20Inference%20Methods"><span class="toc_mobile_items-number">6.4.2.</span> <span class="toc_mobile_items-text">5.4.2&amp;nbsp; Inference Methods</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#null"><span class="toc_mobile_items-number">6.4.3.</span> <span class="toc_mobile_items-text">5.4.3&amp;nbsp; Improvement over Entity Recognition and Different Numbers of GCN Layer</span></a></li></ol></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#null"><span class="toc_mobile_items-number">7.</span> <span class="toc_mobile_items-text">5.5&amp;nbsp; Case Study</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#null"><span class="toc_mobile_items-number">8.</span> <span class="toc_mobile_items-text">6&amp;nbsp; Conclusion</span></a></li></ol></div></div></div><div id="body-wrap"><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true">     </i><div class="auto_open" id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Abstract"><span class="toc-number">1.</span> <span class="toc-text">Abstract</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#1%20Introduction"><span class="toc-number">2.</span> <span class="toc-text">1 Introduction</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2%20Related%20Work"><span class="toc-number">3.</span> <span class="toc-text">2 Related Work</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#3%20Review%20of%20GCN"><span class="toc-number">4.</span> <span class="toc-text">3 Review of GCN</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#4%20Methodology"><span class="toc-number">5.</span> <span class="toc-text">4 Methodology</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#4.1%C2%A0%201st-phase%20Prediction"><span class="toc-number">5.1.</span> <span class="toc-text">4.1&amp;nbsp; 1st-phase Prediction</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4.1.1%20Bi-LSTM"><span class="toc-number">5.1.1.</span> <span class="toc-text">4.1.1 Bi-LSTM</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4.1.2%20Bi-GCN"><span class="toc-number">5.1.2.</span> <span class="toc-text">4.1.2 Bi-GCN</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4.1.3%20Extraction%20of%20Entities%20and%20Relations"><span class="toc-number">5.1.3.</span> <span class="toc-text">4.1.3 Extraction of Entities and Relations</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4.2%C2%A0%202nd-phase%20Prediction"><span class="toc-number">5.2.</span> <span class="toc-text">4.2&amp;nbsp; 2nd-phase Prediction</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4.2.1%C2%A0%20Relation-weighted%20Graph"><span class="toc-number">5.2.1.</span> <span class="toc-text">4.2.1&amp;nbsp; Relation-weighted Graph</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4.3%C2%A0%20Training%20Detail"><span class="toc-number">5.3.</span> <span class="toc-text">4.3&amp;nbsp; Training Detail</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4.4%C2%A0%20Inference"><span class="toc-number">5.4.</span> <span class="toc-text">4.4&amp;nbsp; Inference</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#5%C2%A0%20Experiments"><span class="toc-number">6.</span> <span class="toc-text">5&amp;nbsp; Experiments</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#5.1%20Experimental%20Settings"><span class="toc-number">6.1.</span> <span class="toc-text">5.1 Experimental Settings</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5.1.1%C2%A0%20Datasets"><span class="toc-number">6.1.1.</span> <span class="toc-text">5.1.1&amp;nbsp; Datasets</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5.2%C2%A0%20Baseline%20and%20Evaluation%20Metrics"><span class="toc-number">6.2.</span> <span class="toc-text">5.2&amp;nbsp; Baseline and Evaluation Metrics</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5.3%C2%A0%20Quantitative%20Results"><span class="toc-number">6.3.</span> <span class="toc-text">5.3&amp;nbsp; Quantitative Results</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5.4%C2%A0%20Detailed%20Analysis"><span class="toc-number">6.4.</span> <span class="toc-text">5.4&amp;nbsp; Detailed Analysis</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5.4.1%C2%A0%20Different%20Types%20of%20Triplets"><span class="toc-number">6.4.1.</span> <span class="toc-text">5.4.1&amp;nbsp; Different Types of Triplets</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5.4.2%C2%A0%20Inference%20Methods"><span class="toc-number">6.4.2.</span> <span class="toc-text">5.4.2&amp;nbsp; Inference Methods</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#null"><span class="toc-number">6.4.3.</span> <span class="toc-text">5.4.3&amp;nbsp; Improvement over Entity Recognition and Different Numbers of GCN Layer</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#null"><span class="toc-number">7.</span> <span class="toc-text">5.5&amp;nbsp; Case Study</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#null"><span class="toc-number">8.</span> <span class="toc-text">6&amp;nbsp; Conclusion</span></a></li></ol></div></div></div><main id="content-outer"><div id="top-container" style="background-image: url(https://cdn.jsdelivr.net/gh/jerryc127/butterfly_cdn@2.1.0/top_img/default.png)"><div id="post-info"><div id="post-title"><div class="posttitle">《GraphRel：Modeling Text as Relational Graphs for Joint Entity and Relation Extraction》阅读记录</div></div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 发表于 2020-03-04<span class="post-meta__separator">|</span><i class="fa fa-history fa-fw" aria-hidden="true"></i> 更新于 2020-03-04</time><span class="post-meta__separator">|</span><span><i class="fa fa-inbox post-meta__icon fa-fw" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E7%AC%94%E8%AE%B0/">笔记</a></span><div class="post-meta-wordcount"><div class="post-meta-pv-cv"><span><i class="fa fa-eye post-meta__icon fa-fw" aria-hidden="true"> </i>阅读量:</span><span id="busuanzi_value_page_pv"></span></div></div></div></div></div><div class="layout layout_post" id="content-inner">   <article id="post"><div class="article-container" id="post-content"><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>本文提出了一种端到端的关系抽取模型GraphRel，它使用GCN来共同学习命名实体和关系。与之前的基线相比，我们通过关系加权GCN来考虑命名实体和关系之间的交互，以更好地提取关系。线性结构和依赖结构都用于提取文本的序列和区域特征，并且完整的单词图还用于提取文本中所有单词对的隐含特征。使用基于图的方法，重叠关系的预测比先前的序列方法有了很大的改进。我们在两个公共数据集上评估GraphRel。结果表明，GraphRel在保持高精度的同时，显著提高了查全率。另外，GraphRel的性能比以前的工作高出3.2%和5.8%(F1 Score)，实现了关系抽取的新SOTA。</p>
<a id="more"></a>

<h1 id="1%20Introduction"><a name="t2"></a><a name="t2"></a>1 Introduction</h1>
<p style="text-indent:33px;">提取具有语义关系的实体对，即三元组（如BarackObama，PresidentOf，UnitedStates）是信息提取的中心任务，需要从非结构化文本自动构建知识。 三个关键方面仍需要在统一框架中进行全面处理：</p>
<ul><li>实体识别和关系提取的端到端联合建模；</li>
    <li>对重叠关系的预测，即共同提及的关系；</li>
    <li>考虑关系之间的相互作用，尤其是重叠关系。</li>
</ul><p style="text-indent:33px;">传统上，①pipeline使用命名实体识别器提取实体，然后预测每对提取的实体之间的关系；②建立了联合实体识别和关系抽取模型，以利用这两个任务之间的密切交互。联合建模这些复杂的方法是基于特征的结构化学习系统，严重依赖于特征工程。</p>

<p style="text-indent:33px;">随着深度神经网络的成功，基于神经网络的自动特征学习方法已被应用到关系提取中。 这些方法在两个提及实体之间的单词序列上 ①使用CNN，LSTM或Tree-LSTM，②两个实体之间的最短依赖路径或 ③跨越两个实体的minimal constituency sub-tree 来编码每对实体的相关信息 。 但这些方法不是实体和关系的端到端联合建模。假设给出了实体，并且在实际使用中需要命名实体识别器时，预期其性能会显着下降。</p>
<p style="text-indent:33px;">关系提取的另一个挑战是如何考虑关系之间的相互作用，这对于重叠关系——即共享共同实体的关系特别重要。例如，①EntityPairOverlap，从（BarackObama, Governance, UnitedStates）推断出（BarackObama, PresidentOf, UnitedStates）。另一个案例是，从（BarackObama, LiveIn, WhiteHouse）和（WhiteHouse, PresidentialPalace, UnitedStates）推断出刚才的三元组，其中后两个三元组被认为具有②SingleEntityOverlap。尽管这种交互作用在知识库完成中很常见，但无论是通过直接推理还是通过间接证明进行这种交互，对于输入中不存在实体的联合实体识别和关系提取模型而言尤其困难。尽管Zheng等人提出了一种基于LSTM序列标签器的强大的实体与关系的神经端到端联合模型，但他们必须完全放弃重叠关系。</p>
<p style="text-indent:33px;">在本文中，我们提出了GraphRel，这是一种用于实体识别和关系提取的神经端到端联合模型，它是第一个处理关系提取中所有三个关键部分的模型。 GraphRel通过堆叠Bi-LSTM句子编码器和GCN依赖树编码器来学习自动提取每个单词的隐藏特征。 然后GraphRel标记实体提及单词并预测连接提及的关系三元组，这是第一阶段预测。</p>
<p style="text-indent:33px;">为了在考虑到三元组之间的相互作用的情况下进行预测，我们在GraphRel第二阶段添加了一个新颖的关系加权GCN。 第一阶段GraphRel接收到实体损失和关系损失，沿着依赖关系链接提取节点隐藏特征，同时建立具有关系加权边的新全连接图。 然后，通过对<strong>中间图</strong>进行操作，第二阶段GCN在最终分类每个边之前有效地考虑实体之间的相互作用以及（可能重叠的）关系。</p>
<p><strong>GraphRel：</strong></p>
<ul><li>考虑了线性和依赖结构，以及文本中所有单词对之间的隐含特征；</li>
    <li>对实体和关系进行端到端的联合建模，同时考虑所有单词对进行预测；</li>
    <li>仔细考虑实体与关系之间的相互作用。</li>
</ul><p style="text-indent:33px;">在两个公共关系提取数据集上评估该方法：NYT和WebNLG。 实验结果表明，GraphRel与以前的工作相比大大改善了重叠关系，并且在两个数据集上都达到了SOTA。</p>

<h1 id="2%20Related%20Work"><a name="t3"></a><a name="t3"></a>2 Related Work</h1>
<p style="text-indent:33px;">模型的BiLSTM-GCN编码器部分类似于Miwa和Bansal（2016）提出的BiLSTM-TreeLSTM模型，因为它们还在序列顶部堆叠了依赖树，以共同对实体和关系进行建模。BiLSTM-TreeLSTM模型在每个句子上使用Bi-LSTM进行自动特征学习，并且所提取的隐藏特征由顺序实体tagger和最短依赖路径关系classifier共享。 但是，在引入共享参数以进行联合实体识别和关系提取时，它们仍必须通过pipeline传递tagger预测的实体提及，以形成关系分类的实体对。</p>
<p style="text-indent:33px;">Zheng等人（2018）没有像以前的工作那样尝试对每个提及对进行分类，而是将关系提取构造为与实体识别一样的序列标记问题。 这可以在Bi-LSTM编码器的顶部通过LSTM解码器对关系提取进行建模。 但是，尽管在NYT数据集上显示出令人鼓舞的结果，但它们的优势却来自于关注孤立的关系并完全放弃了重叠关系。 相比之下，<strong>GraphRel在端到端并共同建模实体识别的同时，可以处理所有类型的关系</strong>。</p>
<p style="text-indent:33px;">Zeng等人随后提出了一种用于关系提取的端对端序列到序列模型。 它们通过Bi-LSTM编码每个句子，并使用最后的编码器隐藏状态初始化一个（OneDecoder）或多个（MultiDecoder）LSTM，以动态解码关系三元组。 解码时，通过选择一个关系并从句子中复制两个单词来生成三元组。seq2seq设置部分处理三元组之间的交互。然而，在生成新三元组时，通过考虑先前生成的具有强制线性顺序的三元组，只能单向捕获关系之间的相互作用。相反，在本文中，<strong>通过在LSTM-GCN编码器上应用第二阶段GCN，提出在具有自动学习链接的字图上传播实体和关系信息。</strong></p>
<p style="text-indent:33px;">最近，在许多自然语言处理（NLP）任务中都使用了GCN的依赖关系结构。 Marcheggiani 和 Titov 将GCN应用于单词序列以进行语义角色标记。 Liu等人通过GCN对长文档进行编码以执行文本匹配。 Cetoli等人结合RNN和GCN来识别命名实体。 关于考虑单词序列的依存关系进行关系提取的工作也有一些工作。 在提出的GrpahRel中，<strong>不仅堆叠Bi-LSTM和GCN来考虑线性结构和依赖结构，而且采用第二阶段关系加权GCN来进一步建模实体与关系之间的相互作用。</strong></p>
<h1 id="3%20Review%20of%20GCN"><a name="t4"></a><a name="t4"></a>3 Review of GCN</h1>
<p style="text-indent:33px;">作为卷积神经网络（CNN），图卷积网络（GCN）卷积相邻节点的特征，并将节点的信息传播到其最近的邻居。如图1所示，通过堆叠GCN层，GCN可以提取每个节点的区域特征。</p>
<p style="text-align:center;"><a href="https://img-blog.csdnimg.cn/20191022211619599.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwOTMxMTgx,size_16,color_FFFFFF,t_70" target="_blank" rel="noopener" data-fancybox="group" data-caption class="fancybox"><img alt class="has lazyload" height="226" width="400" title data-src="https://img-blog.csdnimg.cn/20191022211619599.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwOTMxMTgx,size_16,color_FFFFFF,t_70"></a></p>
<p>GCN层通过使用以下等式考虑相邻节点的特征来检索新的节点特征：</p>
<p style="text-align:center;"><a href="https://img-blog.csdnimg.cn/2019102221192439.png" target="_blank" rel="noopener" data-fancybox="group" data-caption class="fancybox"><img alt class="has lazyload" height="120" width="450" title data-src="https://img-blog.csdnimg.cn/2019102221192439.png"></a></p>
<p>其中&nbsp;<a href="https://private.codecogs.com/gif.latex?%5Cmu" target="_blank" rel="noopener" data-fancybox="group" data-caption="\mu" class="fancybox"><img alt="\mu" title="\mu" data-src="https://private.codecogs.com/gif.latex?%5Cmu" class="lazyload"></a>&nbsp;是目标节点，<a href="https://private.codecogs.com/gif.latex?N%28u%29" target="_blank" rel="noopener" data-fancybox="group" data-caption="N(u)" class="fancybox"><img alt="N(u)" class="mathcode lazyload" title="N(u)" data-src="https://private.codecogs.com/gif.latex?N%28u%29"></a>&nbsp;表示&nbsp;<a href="https://private.codecogs.com/gif.latex?%5Cmu" target="_blank" rel="noopener" data-fancybox="group" data-caption="\mu" class="fancybox"><img alt="\mu" title="\mu" data-src="https://private.codecogs.com/gif.latex?%5Cmu" class="lazyload"></a>&nbsp;的邻域，包括&nbsp;<a href="https://private.codecogs.com/gif.latex?%5Cmu" target="_blank" rel="noopener" data-fancybox="group" data-caption="\mu" class="fancybox"><img alt="\mu" title="\mu" data-src="https://private.codecogs.com/gif.latex?%5Cmu" class="lazyload"></a>&nbsp;本身； <a href="https://private.codecogs.com/gif.latex?h_%7Bv%7D%5E%7Bl%7D" target="_blank" rel="noopener" data-fancybox="group" data-caption="h_{v}^{l}" class="fancybox"><img alt="h_{v}^{l}" class="mathcode lazyload" title="h_{v}^{l}" data-src="https://private.codecogs.com/gif.latex?h_%7Bv%7D%5E%7Bl%7D"></a>&nbsp;表示节点&nbsp;<a href="https://private.codecogs.com/gif.latex?v" target="_blank" rel="noopener" data-fancybox="group" data-caption="v" class="fancybox"><img alt="v" class="mathcode lazyload" title="v" data-src="https://private.codecogs.com/gif.latex?v"></a> 在第&nbsp;<a href="https://private.codecogs.com/gif.latex?l" target="_blank" rel="noopener" data-fancybox="group" data-caption="l" class="fancybox"><img alt="l" class="mathcode lazyload" title="l" data-src="https://private.codecogs.com/gif.latex?l"></a>&nbsp;层的隐藏特征； &nbsp;<a href="https://private.codecogs.com/gif.latex?W" target="_blank" rel="noopener" data-fancybox="group" data-caption="W" class="fancybox"><img alt="W" class="mathcode lazyload" title="W" data-src="https://private.codecogs.com/gif.latex?W"></a>&nbsp;和&nbsp;<a href="https://private.codecogs.com/gif.latex?b" target="_blank" rel="noopener" data-fancybox="group" data-caption="b" class="fancybox"><img alt="b" class="mathcode lazyload" title="b" data-src="https://private.codecogs.com/gif.latex?b"></a>&nbsp;是可学习的权重，将节点的特征映射到图中的相邻节点；<a href="https://private.codecogs.com/gif.latex?h%5Cin%20%5Cmathbb%7BR%7D%5E%7Bf%7D%2C%20W%5Cin%20%5Cmathbb%7BR%7D%5E%7Bf%5Ctimes%20f%7D%2C%20b%5Cin%20%5Cmathbb%7BR%7D%5E%7Bf%7D" target="_blank" rel="noopener" data-fancybox="group" data-caption="h\in \mathbb{R}^{f}, W\in \mathbb{R}^{f\times f}, b\in \mathbb{R}^{f}" class="fancybox"><img alt="h\in \mathbb{R}^{f}, W\in \mathbb{R}^{f\times f}, b\in \mathbb{R}^{f}" class="mathcode lazyload" title="h\in \mathbb{R}^{f}, W\in \mathbb{R}^{f\times f}, b\in \mathbb{R}^{f}" data-src="https://private.codecogs.com/gif.latex?h%5Cin%20%5Cmathbb%7BR%7D%5E%7Bf%7D%2C%20W%5Cin%20%5Cmathbb%7BR%7D%5E%7Bf%5Ctimes%20f%7D%2C%20b%5Cin%20%5Cmathbb%7BR%7D%5E%7Bf%7D"></a> ,其中&nbsp;<a href="https://private.codecogs.com/gif.latex?f" target="_blank" rel="noopener" data-fancybox="group" data-caption="f" class="fancybox"><img alt="f" class="mathcode lazyload" title="f" data-src="https://private.codecogs.com/gif.latex?f"></a>&nbsp;是特征尺寸。</p>
<h1 id="4%20Methodology"><a name="t5"></a><a name="t5"></a>4 Methodology</h1>
<p style="text-indent:33px;">所提出的 GraphRel 包含2个阶段的预测的总体架构图下图所示。<strong>第1阶段</strong>，采用bi-RNN和GCN来提取顺序和位置依赖词特征。 给定单词特征，预测每个单词对与所有单词的实体之间的关系。<strong>第2阶段</strong>，为第一阶段预测的每个关系建立完整的关系图，并在每个图上应用GCN以整合每个关系的信息，并进一步考虑实体与关系之间的相互作用。</p>
<p><a href="https://img-blog.csdnimg.cn/20191023092533484.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwOTMxMTgx,size_16,color_FFFFFF,t_70" target="_blank" rel="noopener" data-fancybox="group" data-caption class="fancybox"><img alt class="has lazyload" height="535" width="1200" title data-src="https://img-blog.csdnimg.cn/20191023092533484.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwOTMxMTgx,size_16,color_FFFFFF,t_70"></a></p>
<h2 id="4.1%C2%A0%201st-phase%20Prediction"><a name="t6"></a><a name="t6"></a>4.1&nbsp; 1st-phase Prediction</h2>
<p style="text-indent:33px;">为充分考虑文本特征的顺序和位置依赖性，首先应用双向RNN提取顺序特征，然后使用双向GCN进一步提取位置依赖性特征。 然后，基于提取的单词特征，预测每个单词对与单词实体之间的关系。</p>
<h3 id="4.1.1%20Bi-LSTM"><a name="t7"></a><a name="t7"></a>4.1.1 Bi-LSTM</h3>
<p style="text-indent:33px;">使用LSTM作为双RNN单元，&nbsp;对于每个单词，将单词嵌入和词性（POS）嵌入组合为初始特征，其中&nbsp;<a href="https://private.codecogs.com/gif.latex?h_%7Bu%7D%5E%7B0%7D" target="_blank" rel="noopener" data-fancybox="group" data-caption="h_{u}^{0}" class="fancybox"><img alt="h_{u}^{0}" class="mathcode lazyload" title="h_{u}^{0}" data-src="https://private.codecogs.com/gif.latex?h_%7Bu%7D%5E%7B0%7D"></a>&nbsp;代表单词&nbsp;<a href="https://private.codecogs.com/gif.latex?u" target="_blank" rel="noopener" data-fancybox="group" data-caption="u" class="fancybox"><img alt="u" class="mathcode lazyload" title="u" data-src="https://private.codecogs.com/gif.latex?u"></a>&nbsp;的初始特征，而&nbsp;<a href="https://private.codecogs.com/gif.latex?Word%28u%29" target="_blank" rel="noopener" data-fancybox="group" data-caption="Word(u)" class="fancybox"><img alt="Word(u)" class="mathcode lazyload" title="Word(u)" data-src="https://private.codecogs.com/gif.latex?Word%28u%29"></a>&nbsp;和&nbsp;<a href="https://private.codecogs.com/gif.latex?Pos%28u%29" target="_blank" rel="noopener" data-fancybox="group" data-caption="Pos(u)" class="fancybox"><img alt="Pos(u)" class="mathcode lazyload" title="Pos(u)" data-src="https://private.codecogs.com/gif.latex?Pos%28u%29"></a>&nbsp;分别是单词&nbsp;<a href="https://private.codecogs.com/gif.latex?u" target="_blank" rel="noopener" data-fancybox="group" data-caption="u" class="fancybox"><img alt="u" title="u" data-src="https://private.codecogs.com/gif.latex?u" class="lazyload"></a>&nbsp;的单词嵌入和POS嵌入。 使用GloVe的预训练词嵌入，并随机初始化POS嵌入以使用整个GraphRel 进行训练。</p>
<p style="text-align:center;"><a href="https://img-blog.csdnimg.cn/20191023094653902.png" target="_blank" rel="noopener" data-fancybox="group" data-caption class="fancybox"><img alt class="has lazyload" height="87" width="500" title data-src="https://img-blog.csdnimg.cn/20191023094653902.png"></a></p>
<h3 id="4.1.2%20Bi-GCN"><a name="t8"></a><a name="t8"></a>4.1.2 Bi-GCN</h3>
<p style="text-indent:33px;">由于原始输入语句是一个序列并且没有固有的图结构可言，因此使用denpendency parse为输入语句创建一个依赖树。 将依赖树用作输入句子的邻接矩阵，并使用GCN提取位置依赖特征。原始GCN是为无向图设计的。，为了同时考虑单词的输入输出特征，将Bi-GCN规定为如下表达式：</p>
<p style="text-align:center;"><a href="https://img-blog.csdnimg.cn/20191023110123301.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwOTMxMTgx,size_16,color_FFFFFF,t_70" target="_blank" rel="noopener" data-fancybox="group" data-caption class="fancybox"><img alt class="has lazyload" height="321" width="500" title data-src="https://img-blog.csdnimg.cn/20191023110123301.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwOTMxMTgx,size_16,color_FFFFFF,t_70"></a></p>
<p>其中&nbsp;<a href="https://private.codecogs.com/gif.latex?h_%7Bu%7D%5E%7Bl%7D" target="_blank" rel="noopener" data-fancybox="group" data-caption="h_{u}^{l}" class="fancybox"><img alt="h_{u}^{l}" class="mathcode lazyload" title="h_{u}^{l}" data-src="https://private.codecogs.com/gif.latex?h_%7Bu%7D%5E%7Bl%7D"></a>&nbsp;代表单词&nbsp;<a href="https://private.codecogs.com/gif.latex?u" target="_blank" rel="noopener" data-fancybox="group" data-caption="u" class="fancybox"><img alt="u" class="mathcode lazyload" title="u" data-src="https://private.codecogs.com/gif.latex?u"></a>&nbsp;在&nbsp;<a href="https://private.codecogs.com/gif.latex?l" target="_blank" rel="noopener" data-fancybox="group" data-caption="l" class="fancybox"><img alt="l" title="l" data-src="https://private.codecogs.com/gif.latex?l" class="lazyload"></a>&nbsp;层中&nbsp;的隐藏特征，<a href="https://private.codecogs.com/gif.latex?%5Coverrightarrow%7BN%7D%28u%29" target="_blank" rel="noopener" data-fancybox="group" data-caption="\overrightarrow{N}(u)" class="fancybox"><img alt="\overrightarrow{N}(u)" class="mathcode lazyload" title="\overrightarrow{N}(u)" data-src="https://private.codecogs.com/gif.latex?%5Coverrightarrow%7BN%7D%28u%29"></a>&nbsp;包括从单词&nbsp;<a href="https://private.codecogs.com/gif.latex?u" target="_blank" rel="noopener" data-fancybox="group" data-caption="u" class="fancybox"><img alt="u" title="u" data-src="https://private.codecogs.com/gif.latex?u" class="lazyload"></a>&nbsp;输出的所有单词，<a href="https://private.codecogs.com/gif.latex?%5Coverleftarrow%7BN%7D%28u%29" target="_blank" rel="noopener" data-fancybox="group" data-caption="\overleftarrow{N}(u)" class="fancybox"><img alt="\overleftarrow{N}(u)" class="mathcode lazyload" title="\overleftarrow{N}(u)" data-src="https://private.codecogs.com/gif.latex?%5Coverleftarrow%7BN%7D%28u%29"></a>&nbsp;包含从单词&nbsp;<a href="https://private.codecogs.com/gif.latex?u" target="_blank" rel="noopener" data-fancybox="group" data-caption="u" class="fancybox"><img alt="u" title="u" data-src="https://private.codecogs.com/gif.latex?u" class="lazyload"></a>&nbsp;输入的所有单词，都包括单词&nbsp;<a href="https://private.codecogs.com/gif.latex?u" target="_blank" rel="noopener" data-fancybox="group" data-caption="u" class="fancybox"><img alt="u" title="u" data-src="https://private.codecogs.com/gif.latex?u" class="lazyload"></a>&nbsp;本身。 <a href="https://private.codecogs.com/gif.latex?W" target="_blank" rel="noopener" data-fancybox="group" data-caption="W" class="fancybox"><img alt="W" class="mathcode lazyload" title="W" data-src="https://private.codecogs.com/gif.latex?W"></a>&nbsp;和 <a href="https://private.codecogs.com/gif.latex?b" target="_blank" rel="noopener" data-fancybox="group" data-caption="b" class="fancybox"><img alt="b" class="mathcode lazyload" title="b" data-src="https://private.codecogs.com/gif.latex?b"></a>&nbsp;都是可学习的卷积权重。<a href="https://private.codecogs.com/gif.latex?%5Coverrightarrow%7BW%7D%2C%20%5Coverrightarrow%7Bb%7D%2C%20%5Coverleftarrow%7BW%7D%2C%5Coverleftarrow%7Bb%7D" target="_blank" rel="noopener" data-fancybox="group" data-caption="\overrightarrow{W}, \overrightarrow{b}, \overleftarrow{W},\overleftarrow{b}" class="fancybox"><img alt="\overrightarrow{W}, \overrightarrow{b}, \overleftarrow{W},\overleftarrow{b}" class="mathcode lazyload" title="\overrightarrow{W}, \overrightarrow{b}, \overleftarrow{W},\overleftarrow{b}" data-src="https://private.codecogs.com/gif.latex?%5Coverrightarrow%7BW%7D%2C%20%5Coverrightarrow%7Bb%7D%2C%20%5Coverleftarrow%7BW%7D%2C%5Coverleftarrow%7Bb%7D"></a>&nbsp;分别代表输出权重和输入权重，输出和输入单词特征连接为最终单词特征。</p>
<h3 id="4.1.3%20Extraction%20of%20Entities%20and%20Relations"><a name="t9"></a><a name="t9"></a>4.1.3 Extraction of Entities and Relations</h3>
<p style="text-indent:33px;">利用从 Bi-RNN 和 Bi-GCN 提取的单词特征，预测单词实体并提取每个单词对的关系。 对于单词实体，根据1-layer LSTM上的单词特征预测所有单词，并应用分类损失（称为&nbsp;<a href="https://private.codecogs.com/gif.latex?eloss_%7B1p%7D" target="_blank" rel="noopener" data-fancybox="group" data-caption="eloss_{1p}" class="fancybox"><img alt="eloss_{1p}" class="mathcode lazyload" title="eloss_{1p}" data-src="https://private.codecogs.com/gif.latex?eloss_%7B1p%7D"></a>）来训练它们。 <strong>对于关系提取，删除依赖边并对所有单词对进行预测</strong>。 对于每个关系r，学习权重矩阵&nbsp;<a href="https://private.codecogs.com/gif.latex?W_%7Br%7D%5E%7B1%7D%2C%20W_%7Br%7D%5E%7B2%7D%2C%20W_%7Br%7D%5E%7B3%7D" target="_blank" rel="noopener" data-fancybox="group" data-caption="W_{r}^{1}, W_{r}^{2}, W_{r}^{3}" class="fancybox"><img alt="W_{r}^{1}, W_{r}^{2}, W_{r}^{3}" class="mathcode lazyload" title="W_{r}^{1}, W_{r}^{2}, W_{r}^{3}" data-src="https://private.codecogs.com/gif.latex?W_%7Br%7D%5E%7B1%7D%2C%20W_%7Br%7D%5E%7B2%7D%2C%20W_%7Br%7D%5E%7B3%7D"></a>&nbsp;并计算关系趋势得分S为</p>
<p style="text-align:center;"><a href="https://img-blog.csdnimg.cn/20191023112951951.png" target="_blank" rel="noopener" data-fancybox="group" data-caption class="fancybox"><img alt class="has lazyload" height="79" width="500" title data-src="https://img-blog.csdnimg.cn/20191023112951951.png"></a></p>
<p>其中&nbsp;<a href="https://private.codecogs.com/gif.latex?S_%7B%28%5Comega%201%2C%20r%2C%20%5Comega%202%29%7D" target="_blank" rel="noopener" data-fancybox="group" data-caption="S_{(\omega 1, r, \omega 2)}" class="fancybox"><img alt="S_{(\omega 1, r, \omega 2)}" class="mathcode lazyload" title="S_{(\omega 1, r, \omega 2)}" data-src="https://private.codecogs.com/gif.latex?S_%7B%28%5Comega%201%2C%20r%2C%20%5Comega%202%29%7D"></a>&nbsp;表示关系&nbsp;<a href="https://private.codecogs.com/gif.latex?r" target="_blank" rel="noopener" data-fancybox="group" data-caption="r" class="fancybox"><img alt="r" class="mathcode lazyload" title="r" data-src="https://private.codecogs.com/gif.latex?r"></a>&nbsp;下的&nbsp;<a href="https://private.codecogs.com/gif.latex?%28%5Comega%201%2C%20%5Comega%202%29" target="_blank" rel="noopener" data-fancybox="group" data-caption="(\omega 1, \omega 2)" class="fancybox"><img alt="(\omega 1, \omega 2)" title="(\omega 1, \omega 2)" data-src="https://private.codecogs.com/gif.latex?%28%5Comega%201%2C%20%5Comega%202%29" class="lazyload"></a>&nbsp;的关系趋势得分，而&nbsp;<a href="https://private.codecogs.com/gif.latex?%28%5Comega%201%2C%20%5Comega%202%29" target="_blank" rel="noopener" data-fancybox="group" data-caption="(\omega 1, \omega 2)" class="fancybox"><img alt="(\omega 1, \omega 2)" title="(\omega 1, \omega 2)" data-src="https://private.codecogs.com/gif.latex?%28%5Comega%201%2C%20%5Comega%202%29" class="lazyload"></a> 表示单词对。 请注意，<a href="https://private.codecogs.com/gif.latex?S_%7B%28%5Comega%201%2C%20r%2C%20%5Comega%202%29%29%7D" target="_blank" rel="noopener" data-fancybox="group" data-caption="S_{(\omega 1, r, \omega 2))}" class="fancybox"><img alt="S_{(\omega 1, r, \omega 2))}" title="S_{(\omega 1, r, \omega 2))}" data-src="https://private.codecogs.com/gif.latex?S_%7B%28%5Comega%201%2C%20r%2C%20%5Comega%202%29%29%7D" class="lazyload"></a>&nbsp;应该不同于&nbsp;<a href="https://private.codecogs.com/gif.latex?S_%7B%28%5Comega%202%2C%20r%2C%20%5Comega%201%29%29%7D" target="_blank" rel="noopener" data-fancybox="group" data-caption="S_{(\omega 2, r, \omega 1))}" class="fancybox"><img alt="S_{(\omega 2, r, \omega 1))}" class="mathcode lazyload" title="S_{(\omega 2, r, \omega 1))}" data-src="https://private.codecogs.com/gif.latex?S_%7B%28%5Comega%202%2C%20r%2C%20%5Comega%201%29%29%7D"></a>。 对于单词对&nbsp;<a href="https://private.codecogs.com/gif.latex?%28%5Comega%201%2C%20%5Comega%202%29" target="_blank" rel="noopener" data-fancybox="group" data-caption="(\omega 1, \omega 2)" class="fancybox"><img alt="(\omega 1, \omega 2)" title="(\omega 1, \omega 2)" data-src="https://private.codecogs.com/gif.latex?%28%5Comega%201%2C%20%5Comega%202%29" class="lazyload"></a>&nbsp;，计算该单词对的所有相关趋势得分，包括非相关性，并将其表示为&nbsp;<a href="https://private.codecogs.com/gif.latex?S_%7B%28%5Comega%201%2C%20null%2C%20%5Comega%202%29%29%7D" target="_blank" rel="noopener" data-fancybox="group" data-caption="S_{(\omega 1, null, \omega 2))}" class="fancybox"><img alt="S_{(\omega 1, null, \omega 2))}" class="mathcode lazyload" title="S_{(\omega 1, null, \omega 2))}" data-src="https://private.codecogs.com/gif.latex?S_%7B%28%5Comega%201%2C%20null%2C%20%5Comega%202%29%29%7D"></a>&nbsp;。 将 softmax 函数应用于&nbsp;<a href="https://private.codecogs.com/gif.latex?S_%7B%28%5Comega%201%2C%20r%2C%20%5Comega%202%29%29%7D" target="_blank" rel="noopener" data-fancybox="group" data-caption="S_{(\omega 1, r, \omega 2))}" class="fancybox"><img alt="S_{(\omega 1, r, \omega 2))}" title="S_{(\omega 1, r, \omega 2))}" data-src="https://private.codecogs.com/gif.latex?S_%7B%28%5Comega%201%2C%20r%2C%20%5Comega%202%29%29%7D" class="lazyload"></a>&nbsp;，得出<a href="https://private.codecogs.com/gif.latex?P_%7Br%7D_%7B%28%5Comega%201%2C%20%5Comega%202%29%29%7D" target="_blank" rel="noopener" data-fancybox="group" data-caption="P_{r}_{(\omega 1, \omega 2))}" class="fancybox"><img alt="P_{r}_{(\omega 1, \omega 2))}" title="P_{r}_{(\omega 1, \omega 2))}" data-src="https://private.codecogs.com/gif.latex?P_%7Br%7D_%7B%28%5Comega%201%2C%20%5Comega%202%29%29%7D" class="lazyload"></a>&nbsp;，它表示每个关系&nbsp;<a href="https://private.codecogs.com/gif.latex?r" target="_blank" rel="noopener" data-fancybox="group" data-caption="r" class="fancybox"><img alt="r" title="r" data-src="https://private.codecogs.com/gif.latex?r" class="lazyload"></a>&nbsp;对于&nbsp;<a href="https://private.codecogs.com/gif.latex?%28%5Comega%201%2C%20%5Comega%202%29" target="_blank" rel="noopener" data-fancybox="group" data-caption="(\omega 1, \omega 2)" class="fancybox"><img alt="(\omega 1, \omega 2)" title="(\omega 1, \omega 2)" data-src="https://private.codecogs.com/gif.latex?%28%5Comega%201%2C%20%5Comega%202%29" class="lazyload"></a> 的概率。</p>
<p style="text-indent:33px;">由于提取每个单词对的关系，因此设计中不包含三元组数量限制。 通过调查每个单词对的关系，GraphRel识别出尽可能多的关系。 利用<a href="https://private.codecogs.com/gif.latex?P_%7Br%7D_%7B%28%5Comega%201%2C%20%5Comega%202%29%29%7D" target="_blank" rel="noopener" data-fancybox="group" data-caption="P_{r}_{(\omega 1, \omega 2))}" class="fancybox"><img alt="P_{r}_{(\omega 1, \omega 2))}" title="P_{r}_{(\omega 1, \omega 2))}" data-src="https://private.codecogs.com/gif.latex?P_%7Br%7D_%7B%28%5Comega%201%2C%20%5Comega%202%29%29%7D" class="lazyload"></a>，计算出分类损失的关系，记为&nbsp;<a href="https://private.codecogs.com/gif.latex?rloss_%7B1p%7D" target="_blank" rel="noopener" data-fancybox="group" data-caption="rloss_{1p}" class="fancybox"><img alt="rloss_{1p}" class="mathcode lazyload" title="rloss_{1p}" data-src="https://private.codecogs.com/gif.latex?rloss_%7B1p%7D"></a>。 请注意，尽管&nbsp;<a href="https://private.codecogs.com/gif.latex?eloss_%7B1p%7D" target="_blank" rel="noopener" data-fancybox="group" data-caption="eloss_{1p}" class="fancybox"><img alt="eloss_{1p}" title="eloss_{1p}" data-src="https://private.codecogs.com/gif.latex?eloss_%7B1p%7D" class="lazyload"></a>&nbsp;和&nbsp;<a href="https://private.codecogs.com/gif.latex?rloss_%7B1p%7D" target="_blank" rel="noopener" data-fancybox="group" data-caption="rloss_{1p}" class="fancybox"><img alt="rloss_{1p}" title="rloss_{1p}" data-src="https://private.codecogs.com/gif.latex?rloss_%7B1p%7D" class="lazyload"></a>都不会被用作最终预测，但它们对于训练第一阶段GraphRel的有益辅助损失。</p>
<h2 id="4.2%C2%A0%202nd-phase%20Prediction"><a name="t10"></a><a name="t10"></a>4.2&nbsp; 2nd-phase Prediction</h2>
<p style="text-indent:33px;">第一阶段中没有考虑提取的实体和关系之间的联系，<strong>为了考虑命名实体和关系之间的影响，并考虑文本所有词对之间的隐含特征</strong>，本文在第二阶段提出了一种新的<strong>关系加权GCN</strong>进行进一步提取。</p>
<h3 id="4.2.1%C2%A0%20Relation-weighted%20Graph"><a name="t11"></a><a name="t11"></a>4.2.1&nbsp; Relation-weighted Graph</h3>
<p style="text-indent:33px;">经过第一阶段的预测，为每个关系&nbsp;<a href="https://private.codecogs.com/gif.latex?r" target="_blank" rel="noopener" data-fancybox="group" data-caption="r" class="fancybox"><img alt="r" title="r" data-src="https://private.codecogs.com/gif.latex?r" class="lazyload"></a>&nbsp;构造了完整的关系加权图，其中&nbsp;<a href="https://private.codecogs.com/gif.latex?%28%5Comega%201%2C%20%5Comega%202%29" target="_blank" rel="noopener" data-fancybox="group" data-caption="(\omega 1, \omega 2)" class="fancybox"><img alt="(\omega 1, \omega 2)" title="(\omega 1, \omega 2)" data-src="https://private.codecogs.com/gif.latex?%28%5Comega%201%2C%20%5Comega%202%29" class="lazyload"></a>&nbsp;的边权为&nbsp;<a href="https://private.codecogs.com/gif.latex?P_%7Br%7D_%7B%28%5Comega%201%2C%20%5Comega%202%29%7D" target="_blank" rel="noopener" data-fancybox="group" data-caption="P_{r}_{(\omega 1, \omega 2)}" class="fancybox"><img alt="P_{r}_{(\omega 1, \omega 2)}" class="mathcode lazyload" title="P_{r}_{(\omega 1, \omega 2)}" data-src="https://private.codecogs.com/gif.latex?P_%7Br%7D_%7B%28%5Comega%201%2C%20%5Comega%202%29%7D"></a>，如下图所示。</p>
<p style="text-align:center;"><a href="https://img-blog.csdnimg.cn/20191023193638111.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwOTMxMTgx,size_16,color_FFFFFF,t_70" target="_blank" rel="noopener" data-fancybox="group" data-caption class="fancybox"><img alt class="has lazyload" height="343" width="500" title data-src="https://img-blog.csdnimg.cn/20191023193638111.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwOTMxMTgx,size_16,color_FFFFFF,t_70"></a></p>
<p>然后，第二阶段在每个关系图上采用Bi-GCN，考虑不同关系的不同影响程度并聚合作为<strong>综合词特征</strong>。 该过程可以表示为</p>
<p style="text-align:center;"><a href="https://img-blog.csdnimg.cn/20191023194320558.png" target="_blank" rel="noopener" data-fancybox="group" data-caption class="fancybox"><img alt class="has lazyload" height="113" width="450" title data-src="https://img-blog.csdnimg.cn/20191023194320558.png"></a></p>
<p>其中<a href="https://private.codecogs.com/gif.latex?P_%7Br%7D_%7B%28u%2C%20v%29%7D" target="_blank" rel="noopener" data-fancybox="group" data-caption="P_{r}_{(u, v)}" class="fancybox"><img alt="P_{r}_{(u, v)}" class="mathcode lazyload" title="P_{r}_{(u, v)}" data-src="https://private.codecogs.com/gif.latex?P_%7Br%7D_%7B%28u%2C%20v%29%7D"></a>&nbsp;表示边权重（关系 <a href="https://private.codecogs.com/gif.latex?r" target="_blank" rel="noopener" data-fancybox="group" data-caption="r" class="fancybox"><img alt="r" class="mathcode lazyload" title="r" data-src="https://private.codecogs.com/gif.latex?r"></a>&nbsp;下单词&nbsp;<a href="https://private.codecogs.com/gif.latex?u" target="_blank" rel="noopener" data-fancybox="group" data-caption="u" class="fancybox"><img alt="u" class="mathcode lazyload" title="u" data-src="https://private.codecogs.com/gif.latex?u"></a>&nbsp;到单词 <a href="https://private.codecogs.com/gif.latex?v" target="_blank" rel="noopener" data-fancybox="group" data-caption="v" class="fancybox"><img alt="v" class="mathcode lazyload" title="v" data-src="https://private.codecogs.com/gif.latex?v"></a>&nbsp;的概率）。 <a href="https://private.codecogs.com/gif.latex?W_%7Br%7D" target="_blank" rel="noopener" data-fancybox="group" data-caption="W_{r}" class="fancybox"><img alt="W_{r}" class="mathcode lazyload" title="W_{r}" data-src="https://private.codecogs.com/gif.latex?W_%7Br%7D"></a>&nbsp;和 <a href="https://private.codecogs.com/gif.latex?b_%7Br%7D" target="_blank" rel="noopener" data-fancybox="group" data-caption="b_{r}" class="fancybox"><img alt="b_{r}" class="mathcode lazyload" title="b_{r}" data-src="https://private.codecogs.com/gif.latex?b_%7Br%7D"></a>&nbsp;表示关系 <a href="https://private.codecogs.com/gif.latex?r" target="_blank" rel="noopener" data-fancybox="group" data-caption="r" class="fancybox"><img alt="r" class="mathcode lazyload" title="r" data-src="https://private.codecogs.com/gif.latex?r"></a>&nbsp;下的GCN权重。 <a href="https://private.codecogs.com/gif.latex?V" target="_blank" rel="noopener" data-fancybox="group" data-caption="V" class="fancybox"><img alt="V" class="mathcode lazyload" title="V" data-src="https://private.codecogs.com/gif.latex?V"></a>&nbsp;包含所有单词，<a href="https://private.codecogs.com/gif.latex?R" target="_blank" rel="noopener" data-fancybox="group" data-caption="R" class="fancybox"><img alt="R" class="mathcode lazyload" title="R" data-src="https://private.codecogs.com/gif.latex?R"></a>&nbsp;包含所有关系。 请注意，完整的Bi-GCN还将输入和输出情况都考虑在内。 第二阶段的Bi-GCN进一步考虑了关系加权传播，并为每个单词提取了更充分的特征。用来自二阶段新提取的单词特征，再次执行命名实体和关系分类以实现更可靠的关系预测，损失为&nbsp;<a href="https://private.codecogs.com/gif.latex?eloss_%7B2p%7D" target="_blank" rel="noopener" data-fancybox="group" data-caption="eloss_{2p}" class="fancybox"><img alt="eloss_{2p}" class="mathcode lazyload" title="eloss_{2p}" data-src="https://private.codecogs.com/gif.latex?eloss_%7B2p%7D"></a>&nbsp;和&nbsp;<a href="https://private.codecogs.com/gif.latex?rloss_%7B2p%7D" target="_blank" rel="noopener" data-fancybox="group" data-caption="rloss_{2p}" class="fancybox"><img alt="rloss_{2p}" class="mathcode lazyload" title="rloss_{2p}" data-src="https://private.codecogs.com/gif.latex?rloss_%7B2p%7D"></a>。</p>
<h2 id="4.3%C2%A0%20Training%20Detail"><a name="t12"></a><a name="t12"></a>4.3&nbsp; Training Detail</h2>
<p style="text-indent:33px;">在GraphRel中使用两种损失：<strong>实体损失和关系损失</strong>，两者都属于类别损失，在训练过程中使用<strong>交叉熵</strong>作为分类损失函数。 实体损失使用BIESO方法来标注真实标签，&nbsp;<a href="https://private.codecogs.com/gif.latex?eloss_%7B1p%7D" target="_blank" rel="noopener" data-fancybox="group" data-caption="eloss_{1p}" class="fancybox"><img alt="eloss_{1p}" class="mathcode lazyload" title="eloss_{1p}" data-src="https://private.codecogs.com/gif.latex?eloss_%7B1p%7D"></a>&nbsp;和&nbsp;<a href="https://private.codecogs.com/gif.latex?eloss_%7B2p%7D" target="_blank" rel="noopener" data-fancybox="group" data-caption="eloss_{2p}" class="fancybox"><img alt="eloss_{2p}" class="mathcode lazyload" title="eloss_{2p}" data-src="https://private.codecogs.com/gif.latex?eloss_%7B2p%7D"></a>&nbsp;的真实实体标签相同。关系损失为每个单词对&nbsp;<a href="https://private.codecogs.com/gif.latex?%28%5Comega%201%2C%20%5Comega%202%29" target="_blank" rel="noopener" data-fancybox="group" data-caption="(\omega 1, \omega 2)" class="fancybox"><img alt="(\omega 1, \omega 2)" title="(\omega 1, \omega 2)" data-src="https://private.codecogs.com/gif.latex?%28%5Comega%201%2C%20%5Comega%202%29" class="lazyload"></a>&nbsp;输入一个one-hot编码的关系向量作为&nbsp;<a href="https://private.codecogs.com/gif.latex?P_%7Br%7D_%7B%28%5Comega%201%2C%20%5Comega%202%29%7D" target="_blank" rel="noopener" data-fancybox="group" data-caption="P_{r}_{(\omega 1, \omega 2)}" class="fancybox"><img alt="P_{r}_{(\omega 1, \omega 2)}" class="mathcode lazyload" title="P_{r}_{(\omega 1, \omega 2)}" data-src="https://private.codecogs.com/gif.latex?P_%7Br%7D_%7B%28%5Comega%201%2C%20%5Comega%202%29%7D"></a>&nbsp;的真实数据。 由于基于单词对预测关系，真实数据同样应基于单词对。 也就是说，单词United对于单词Barack和单词Obama都有HasPresident的关系，单词States也是如此。基于单词对的关系表示为GraphRel提供了学习提取关系所需的信息，<a href="https://private.codecogs.com/gif.latex?rloss_%7B1p%7D" target="_blank" rel="noopener" data-fancybox="group" data-caption="rloss_{1p}" class="fancybox"><img alt="rloss_{1p}" class="mathcode lazyload" title="rloss_{1p}" data-src="https://private.codecogs.com/gif.latex?rloss_%7B1p%7D"></a>&nbsp;和&nbsp;<a href="https://private.codecogs.com/gif.latex?rloss_%7B2p%7D" target="_blank" rel="noopener" data-fancybox="group" data-caption="rloss_{2p}" class="fancybox"><img alt="rloss_{2p}" class="mathcode lazyload" title="rloss_{2p}" data-src="https://private.codecogs.com/gif.latex?rloss_%7B2p%7D"></a>&nbsp;的真实数据关系向量相同。</p>
<p style="text-indent:33px;">对于eloss和rloss，为类内实体或关系项添加额外的double-weight。总损失为所有实体损失和关系损失的总和：其中 <a href="https://private.codecogs.com/gif.latex?%5Calpha" target="_blank" rel="noopener" data-fancybox="group" data-caption="\alpha" class="fancybox"><img alt="\alpha" class="mathcode lazyload" title="\alpha" data-src="https://private.codecogs.com/gif.latex?%5Calpha"></a>&nbsp;是第一阶段和第二阶段的损失之间的权重。将 <a href="https://private.codecogs.com/gif.latex?loss_%7Ball%7D" target="_blank" rel="noopener" data-fancybox="group" data-caption="loss_{all}" class="fancybox"><img alt="loss_{all}" class="mathcode lazyload" title="loss_{all}" data-src="https://private.codecogs.com/gif.latex?loss_%7Ball%7D"></a>&nbsp;降到最低，并以端到端的方式训练整个GraphRel。</p>
<p style="text-align:center;"><a href="https://img-blog.csdnimg.cn/20191023202501130.png" target="_blank" rel="noopener" data-fancybox="group" data-caption class="fancybox"><img alt class="has lazyload" height="70" width="550" title data-src="https://img-blog.csdnimg.cn/20191023202501130.png"></a></p>
<h2 id="4.4%C2%A0%20Inference"><a name="t13"></a><a name="t13"></a>4.4&nbsp; Inference</h2>
<p style="text-indent:33px;">beseline预测方法是head prediction（没明白怎么理解），当且仅当 BarackObama, UnitedStates 都被识别为实体且 PresidentOf 为<a href="https://private.codecogs.com/gif.latex?P_%7B%28Obama%2C%20States%29%7D" target="_blank" rel="noopener" data-fancybox="group" data-caption="P_{(Obama, States)}" class="fancybox"><img alt="P_{(Obama, States)}" class="mathcode lazyload" title="P_{(Obama, States)}" data-src="https://private.codecogs.com/gif.latex?P_%7B%28Obama%2C%20States%29%7D"></a>&nbsp;的最大概率时，才会提取诸如（BarackObama，PresidentOf，United States）的关系三元组。另一种可能更稳定的提取方法是平均预测，其中考虑实体提及对之间的所有单词对，选择具有最大平均概率的关系。第三种是阈值预测方法，其中实体对的所有单词对仍会考虑在内，但以独立的方式进行。例如，如果4个分布中的2个具有 PresidentOf 作为最可能的类别，则仅当2/4 = 50％&gt;θ 时才提取三元组（BarackObama，PresidentOf，UnitedStates），其中 θ 是自由阈值参数。这样，用户可以选择自己喜欢的精度并通过调整 θ 进行权衡，如果未指定 θ=0 。</p>
<h1 id="5%C2%A0%20Experiments" style="text-indent:0px;"><a name="t14"></a><a name="t14"></a>5&nbsp; Experiments</h1>
<p style="text-indent:33px;">这一部分给出了GraphRel的实验结果。首先是实施细节、数据集和比较的baseline，然后展示两个数据集的定量结果，进行了详细的分析，并对不同类别的命名实体进行了分类，最后通过一个案例说明第二阶段的改进效果。</p>
<h2 id="5.1%20Experimental%20Settings" style="text-indent:0px;"><a name="t15"></a><a name="t15"></a>5.1 Experimental Settings</h2>
<p style="text-indent:33px;">实现中，选择预训练的GloVe（300d）作为固定的单词嵌入，然后将单词嵌入与可训练的POS嵌入（15d）连接起来，作为每个单词的最终输入嵌入，从spaCy检索每个单词的POS标签和整个句子的依存关系树。在第一阶段使用具有256个单位的Bi-LSTM和具有256个特征的2层Bi-GCN。 对于第二阶段，关系加权的Bi-GCN为1层，特征尺寸为256。在训练期间，将LSTM辍学率设置为0.5，学习率设置为0.0008，损失权重α设置为 3，使用Adam优化器训练GraphRel，并在PyTorch下实现它。</p>
<h3 id="5.1.1%C2%A0%20Datasets" style="text-indent:0px;"><a name="t16"></a><a name="t16"></a>5.1.1&nbsp; Datasets</h3>
<p style="text-indent:33px;">使用NYT和WebNLG数据集来评估所提出的方法。 正如 NovelTagging 和 MultiDecoder 一样，对于NYT过滤的句子超过100个单词；对于WebNLG，在实验中的每个实例中仅使用第一个句子。 表2中描述了NYT和WebNLG的统计信息，将关系三元组分为三类：正常，EntityPairOverlap（EPO）和SingleEntityOverlap（SEO），&nbsp;表2还显示了每个类别的总数。由于一个实体属于几个不同的关系，因此EntityPairOverlap和SingleEntityOverlap难度更大。 会在详细分析中讨论不同类别的结果。</p>
<h2 id="5.2%C2%A0%20Baseline%20and%20Evaluation%20Metrics" style="text-indent:0px;"><a name="t17"></a><a name="t17"></a>5.2&nbsp; Baseline and Evaluation Metrics</h2>
<p style="text-indent:33px;">将GraphRel与两个baseline进行了比较：NovelTagging和MultiDecoder。 NovelTagging是一种序列标记器，可预测每个句子单词的实体和关系；MultiDecoder是一种最新的方法，将关系提取视为seq-seq问题，并使用动态解码器提取关系三元组。 这两个baseline的结果直接来自原始论文。 作为两个baselien，采用了标准的F1分数来评估结果。 当且仅当两个对应实体的关系和首部与真实数据相同时，才能将预测的三元组视为正确的。</p>
<p style="text-align:center;"><a href="https://img-blog.csdnimg.cn/20191023215945553.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwOTMxMTgx,size_16,color_FFFFFF,t_70" target="_blank" rel="noopener" data-fancybox="group" data-caption class="fancybox"><img alt class="has lazyload" height="307" width="500" title data-src="https://img-blog.csdnimg.cn/20191023215945553.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwOTMxMTgx,size_16,color_FFFFFF,t_70"></a></p>
<h2 id="5.3%C2%A0%20Quantitative%20Results" style="text-indent:0px;"><a name="t18"></a><a name="t18"></a>5.3&nbsp; Quantitative Results</h2>
<p style="text-indent:33px;">表1列出了NYT和WebNLG数据集的NovelTagging，MultiDecoder和GraphRel的精确率，召回率和F1得分。 MultiDecoder的原始论文中提出的OneDecoder仅使用单个解码器来提取关系三元组。 <a href="https://private.codecogs.com/gif.latex?GraphRel_%7B1p%7D" target="_blank" rel="noopener" data-fancybox="group" data-caption="GraphRel_{1p}" class="fancybox"><img alt="GraphRel_{1p}" class="mathcode lazyload" title="GraphRel_{1p}" data-src="https://private.codecogs.com/gif.latex?GraphRel_%7B1p%7D"></a>&nbsp;是提出的方法，但仅在第一阶段，而GraphRel2p是完整版本，可预测第二阶段之后的关系和实体。 对于NYT数据集，<a href="https://private.codecogs.com/gif.latex?GraphRel_%7B1-hop%7D" target="_blank" rel="noopener" data-fancybox="group" data-caption="GraphRel_{1-hop}" class="fancybox"><img alt="GraphRel_{1-hop}" class="mathcode lazyload" title="GraphRel_{1-hop}" data-src="https://private.codecogs.com/gif.latex?GraphRel_%7B1-hop%7D"></a>&nbsp;在F1方面的表现优于NovelTagging 18.0％，OneDecoder 4.0％和MultiDecoder 1.3％。 由于&nbsp;<a href="https://private.codecogs.com/gif.latex?GraphRel_%7B1-hop%7D" target="_blank" rel="noopener" data-fancybox="group" data-caption="GraphRel_{1-hop}" class="fancybox"><img alt="GraphRel_{1-hop}" class="mathcode lazyload" title="GraphRel_{1-hop}" data-src="https://private.codecogs.com/gif.latex?GraphRel_%7B1-hop%7D"></a>&nbsp;同时具有顺序和位置依赖词特征，因此它在精确率和查全率方面均表现更好，因此F1得分更高。 与第二阶段的关系加权GCN相比，考虑名称实体与关系之间相互作用的&nbsp;<a href="https://private.codecogs.com/gif.latex?GraphRel_%7B2p%7D" target="_blank" rel="noopener" data-fancybox="group" data-caption="GraphRel_{2p}" class="fancybox"><img alt="GraphRel_{2p}" class="mathcode lazyload" title="GraphRel_{2p}" data-src="https://private.codecogs.com/gif.latex?GraphRel_%7B2p%7D"></a>&nbsp;与&nbsp;<a href="https://private.codecogs.com/gif.latex?GraphRel_%7B1p%7D" target="_blank" rel="noopener" data-fancybox="group" data-caption="GraphRel_{1p}" class="fancybox"><img alt="GraphRel_{1p}" class="mathcode lazyload" title="GraphRel_{1p}" data-src="https://private.codecogs.com/gif.latex?GraphRel_%7B1p%7D"></a>&nbsp;相比，进一步超过了MultiDecoder 3.2％，并且改进了1.9％。</p>
<p style="text-align:center;"><a href="https://img-blog.csdnimg.cn/20191023215923980.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwOTMxMTgx,size_16,color_FFFFFF,t_70" target="_blank" rel="noopener" data-fancybox="group" data-caption class="fancybox"><img alt class="has lazyload" height="338" width="1000" title data-src="https://img-blog.csdnimg.cn/20191023215923980.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwOTMxMTgx,size_16,color_FFFFFF,t_70"></a></p>
<p style="text-indent:33px;">在WebNLG数据集上可以找到类似的结果：GraphRel1p的基准F1 Score优于MultiDecoder 3.6％，而GraphRel2p 比 GraphRel1p进一步提高了2.2％。 根据NYT和WebNLG的结果，GCN的位置依赖关系特征和第二阶段预测都可以在精确率，召回率和F1 Score方面帮助进行关系预测。 NovelTagging和MultiDecoder都使用顺序体系结构。 由于NovelTagging假定实体属于单一关系，因此精度很高，但召回率很低。 MultiDecoder使用动态解码器生成关系三元组。 由于RNN结构的先天限制（后信息需要前信息发生），它可以生成的三元组的数量受到限制。 但是GraphRel在预测每个单词对的关系时不受此限制。GraphRel是最平衡的方法，同时保持了高精确率和高召回率，并有更高的F1 Score。</p>
<h2 id="5.4%C2%A0%20Detailed%20Analysis"><a name="t19"></a><a name="t19"></a>5.4&nbsp; Detailed Analysis</h2>
<p style="text-indent:33px;">为了进一步分析提出的GraphRel，给出在不同类型的三元组、不同的推理方法、改进的命名实体识别以及使用不同GCN层数的情况下的分析结果。</p>
<h3 id="5.4.1%C2%A0%20Different%20Types%20of%20Triplets" style="text-indent:0px;"><a name="t20"></a><a name="t20"></a>5.4.1&nbsp; Different Types of Triplets</h3>
<p style="text-indent:33px;">首先调查不同实体类别下的结果。图4给出了NYT和WebNLG数据集的结果。对于GraphRel，当预测所有单词对的关系时，所有单词都可以与其他单词具有关系：因此<strong>实体重叠</strong>不是问题。尽管MultiDecoder尝试使用动态解码器，但结果表明GraphRel在所有实体类别中都超过了它们。例如，在WebNLG数据集上，GraphRel1p在MultiClass上的表现优于Normal3.5％，EPO类2.9％和SEO类3.4％。 GraphRel2p进一步改进了每个类的GraphRel1p。还比较了给定的句子中不同数量的三元组的结果，如图5所示。x轴表示句子中的1、2、3、4或5个以上的三元组。由于使用了单个解码器，所以OneDecoder对于单个三元组的性能很好，但是对于句子中更多的三元组，性能会急剧下降。与针对不同实体类别的实验一样，GraphRel1p和GraphRel2p在句子中的所有三元组数目下均优于baseline。 GraphRel1p在一个句子中多于5个三元组得分超过MultiDecoder7.5％，而GraphRel2p在NYT上进一步超过MultiDecoder 11.1％。</p>
<p><a href="https://img-blog.csdnimg.cn/2019102322341628.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwOTMxMTgx,size_16,color_FFFFFF,t_70" target="_blank" rel="noopener" data-fancybox="group" data-caption class="fancybox"><img alt class="has lazyload" height="512" width="1200" title data-src="https://img-blog.csdnimg.cn/2019102322341628.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwOTMxMTgx,size_16,color_FFFFFF,t_70"></a></p>
<p><a href="https://img-blog.csdnimg.cn/20191023223442193.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwOTMxMTgx,size_16,color_FFFFFF,t_70" target="_blank" rel="noopener" data-fancybox="group" data-caption class="fancybox"><img alt class="has lazyload" height="503" width="1200" title data-src="https://img-blog.csdnimg.cn/20191023223442193.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwOTMxMTgx,size_16,color_FFFFFF,t_70"></a></p>
<h3 id="5.4.2%C2%A0%20Inference%20Methods"><a name="t21"></a><a name="t21"></a>5.4.2&nbsp; Inference Methods</h3>
<p style="text-indent:33px;">比较两种baseline方法，即 head 和 average，以及在不同θ下的阈值法。 图6显示了将其应用于NYT和WebNLG上的GraphRel2p的结果。 可以看出，阈值法有效地调整了不同θ选择下精确率和召回率之间的平衡。 通过将阈值从 θ=0.8 降低到 θ=0，NYT和WebNLG的召回率分别显着提高了1.8％和1.4％，而精确率仅略微降低了0.6％。 阈值法的有效性致使在两个数据集上均达到最佳性能，超过了其他两种方法。</p>
<p style="text-align:center;"><a href="https://img-blog.csdnimg.cn/2019102409370847.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwOTMxMTgx,size_16,color_FFFFFF,t_70" target="_blank" rel="noopener" data-fancybox="group" data-caption class="fancybox"><img alt class="has lazyload" height="474" width="700" title data-src="https://img-blog.csdnimg.cn/2019102409370847.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwOTMxMTgx,size_16,color_FFFFFF,t_70"></a></p>
<h3 style="text-indent:0px;"><a name="t22"></a><a name="t22"></a>5.4.3&nbsp; Improvement over Entity Recognition and Different Numbers of GCN Layer</h3>
<p style="text-indent:33px;">根据表4，对于NYT和WebNLG上的实体识别，GraphRel2p可以超过第一阶段0.4％和2.8％。 第二阶段的关系加权的GCN不仅对关系提取有效，而且对命名实体识别也有效。为了确认2层加1层的最佳设置，在第一阶段和第二阶段中使用不同数量的GCN层进行测试。表5给出了将3个GCN层用于第一阶段并将2个关系加权GCN层用于第二阶段的实验结果，表明更多的GCN层不能带来更好的预测，而（2,1）层应该是最适合关系提取任务的设置。&nbsp;</p>
<h1 style="text-indent:0px;"><a name="t23"></a><a name="t23"></a>5.5&nbsp; Case Study</h1>
<p style="text-indent:33px;">表3显示了GraphRel的案例研究。第一个句子很简单，并且GraphRel1p和GraphRel2p都可以准确提取。对于第二种情况，尽管不属于命名实体，但它应包含Italy的隐藏语义。 因此第二阶段可以进一步预测 <em>A.S. Gubbio 1910 grounds in Italy</em>。第三种情况是SEO类，其中GraphRel1p发现Asam pedas与Asam padeh相同，因此后者也应位于马来半岛并来自马来西亚。</p>
<p style="text-align:center;"><a href="https://img-blog.csdnimg.cn/20191024100130760.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwOTMxMTgx,size_16,color_FFFFFF,t_70" target="_blank" rel="noopener" data-fancybox="group" data-caption class="fancybox"><img alt class="has lazyload" height="427" width="1200" title data-src="https://img-blog.csdnimg.cn/20191024100130760.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwOTMxMTgx,size_16,color_FFFFFF,t_70"></a></p>
<h1 style="text-indent:0px;"><a name="t24"></a><a name="t24"></a>6&nbsp; Conclusion</h1>
<p style="text-indent:33px;">本文中提出了GraphRel，这是一种基于图卷积网络（GCN）共同学习命名实体和关系的端到端关系提取模型。将RNN和GCN结合起来提取每个单词的顺序特征和位置依赖特征，也考虑了文本中所有单词对之间的隐含特征。 通过预测每个单词对的关系，解决实体重叠的问题。 此外，提出一种新颖的关系加权GCN，考虑了命名实体与关系之间的相互作用。在NYT和WebNLG数据集上评估提出的方法。 结果表明，此方法比以前的工作分别提高了3.2％和5.8％，并实现了关系提取的SOTA。</p>
<p>&nbsp;</p></div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">R</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://yoursite.com/2020/03/04/article-1/">http://yoursite.com/2020/03/04/article-1/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://yoursite.com">想吃煎饼果子</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E8%AE%BA%E6%96%87%E8%AE%B0%E5%BD%95/">论文记录    </a><a class="post-meta__tags" href="/tags/NLP/">NLP    </a></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/gh/jerryc127/butterfly_cdn@2.1.0/top_img/default.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js"></script></div></div><div class="post-reward"><a class="reward-button button--primary button--animated"> <i class="fa fa-qrcode"></i> 打赏<div class="reward-main"><ul class="reward-all"><li class="reward-item"><img class="lazyload post-qr-code__img" src="/img/wechat.jpg" alt="微信"><div class="post-qr-code__desc">微信</div></li><li class="reward-item"><img class="lazyload post-qr-code__img" src="/img/alipay.jpg" alt="支付寶"><div class="post-qr-code__desc">支付寶</div></li></ul></div></a></div><nav class="pagination_post" id="pagination"><div class="prev-post pull-full"><a href="/2020/03/04/article-2/"><img class="prev_cover lazyload" data-src="https://cdn.jsdelivr.net/gh/jerryc127/butterfly_cdn@2.1.0/top_img/default.png" onerror="onerror=null;src='/img/404.jpg'"><div class="label">上一篇</div><div class="prev_info"><span>《Graph Neural Networks with Generated Parameters for Relation Extraction》阅读笔记</span></div></a></div></nav><div class="relatedPosts"><div class="relatedPosts_headline"><i class="fa fa-fw fa-thumbs-up" aria-hidden="true"></i><span> 相关推荐</span></div><div class="relatedPosts_list"><div class="relatedPosts_item"><a href="/2020/03/04/article-2/" title="《Graph Neural Networks with Generated Parameters for Relation Extraction》阅读笔记"><img class="relatedPosts_cover lazyload"data-src="https://cdn.jsdelivr.net/gh/jerryc127/butterfly_cdn@2.1.0/top_img/default.png"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-03-04</div><div class="relatedPosts_title">《Graph Neural Networks with Generated Parameters for Relation Extraction》阅读笔记</div></div></a></div><div class="relatedPosts_item"><a href="/2020/03/04/article-3/" title="《Effective Modeling of Encoder-Decoder Architecturefor Joint Entity and Relation Extraction》"><img class="relatedPosts_cover lazyload"data-src="https://cdn.jsdelivr.net/gh/jerryc127/butterfly_cdn@2.1.0/top_img/default.png"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-03-04</div><div class="relatedPosts_title">《Effective Modeling of Encoder-Decoder Architecturefor Joint Entity and Relation Extraction》</div></div></a></div><div class="relatedPosts_item"><a href="/2020/03/04/article-4/" title="《Extracting Relational Facts by an End-to-End Neural Model with Copy Mechanism》记录"><img class="relatedPosts_cover lazyload"data-src="https://cdn.jsdelivr.net/gh/jerryc127/butterfly_cdn@2.1.0/top_img/default.png"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-03-04</div><div class="relatedPosts_title">《Extracting Relational Facts by an End-to-End Neural Model with Copy Mechanism》记录</div></div></a></div><div class="relatedPosts_item"><a href="/2020/03/04/article-5/" title="《Chinese Open Relation Extraction and Knowledge Base Establishment》阅读记录"><img class="relatedPosts_cover lazyload"data-src="https://cdn.jsdelivr.net/gh/jerryc127/butterfly_cdn@2.1.0/top_img/default.png"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-03-04</div><div class="relatedPosts_title">《Chinese Open Relation Extraction and Knowledge Base Establishment》阅读记录</div></div></a></div><div class="relatedPosts_item"><a href="/2020/03/04/article/" title="《Attention Guided Graph Convolutional Networks for Relation Extraction》阅读记录"><img class="relatedPosts_cover lazyload"data-src="https://cdn.jsdelivr.net/gh/jerryc127/butterfly_cdn@2.1.0/top_img/default.png"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-03-04</div><div class="relatedPosts_title">《Attention Guided Graph Convolutional Networks for Relation Extraction》阅读记录</div></div></a></div></div><div class="clear_both"></div></div></div></main><footer id="footer" data-type="color"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2020 By R</div><div class="framework-info"><span>驱动 </span><a href="http://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 </span><a href="https://github.com/jerryc127/hexo-theme-butterfly" target="_blank" rel="noopener"><span>Butterfly</span></a></div></div></footer></div><section class="rightside" id="rightside"><div id="rightside-config-hide"><i class="fa fa-book" id="readmode" title="阅读模式"></i><i class="fa fa-plus" id="font_plus" title="放大字体"></i><i class="fa fa-minus" id="font_minus" title="缩小字体"></i><a class="translate_chn_to_cht" id="translateLink" href="javascript:translatePage();" title="简繁转换" target="_self">繁</a><i class="darkmode fa fa-moon-o" id="darkmode" title="夜间模式"></i></div><div id="rightside-config-show"><div id="rightside_config" title="设置"><i class="fa fa-cog" aria-hidden="true"></i></div><i class="fa fa-list-ul close" id="mobile-toc-button" title="目录" aria-hidden="true"></i><i class="fa fa-arrow-up" id="go-up" title="回到顶部" aria-hidden="true"></i></div></section><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@latest/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/lazysizes@latest/lazysizes.min.js" async=""></script></body></html>